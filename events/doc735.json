{
  "start": 1577876400000,
  "duration": 25,
  "room": "UB5.132",
  "title": "Snorkel Beambell - Real-time Weak Supervision on Apache Flink",
  "subtitle": "",
  "track": "HPC, Big Data, and Data Science",
  "abstract": "The advent of Deep Learning models has led to a massive growth of real-world machine learning. Deep Learning allows Machine Learning Practitioners to get the state-of-the-art score on benchmarks without any hand-engineered features. These Deep Learning models rely on massive hand-labeled training datasets which is a bottleneck in developing and modifying machine learning models.\n\nMost large scale Machine Learning systems today like Google\u2019s DryBell use some form of Weak Supervision to construct lower quality, large scale training datasets that can be used to continuously retrain and deploy models in a real-world scenario.\n\nThe challenge with continuous retraining is that one needs to maintain prior state (e.g., the learning functions in case of Weak Supervision or a pre-trained model like BERT or Word2Vec for Transfer Learning) that is shared across multiple streams, while continuously updating the model. Apache Beam\u2019s Stateful Stream processing capabilities are a perfect match here including support for scalable Weak Supervision.\n\nPrior work on using Beam\u2019s State coupled with Flink\u2019s dynamic processing capabilities to store and update word embeddings for real-time Online Topic Modeling of text has been presented at Flink Forward Berlin 2018.  Similar streaming pipelines would also work for real-time model updates using Weak Supervision and Transfer Learning. In this talk, we\u2019ll be looking at a framework - Snorkel BeamBell - a framework leveraging Stanford\u2019s Snorkel library for Weak Supervision and Apache Beam for large scale Weak Supervision Learning for online labeling of large amounts of data that can continuously learn new classification models based on Stateful Learning Functions and user feedback.",
  "description": "The advent of Deep Learning models has led to a massive growth of real-world machine learning. Deep Learning allows Machine Learning Practitioners to get the state-of-the-art score on benchmarks without any hand-engineered features. These Deep Learning models rely on massive hand-labeled training datasets which is a bottleneck in developing and modifying machine learning models.\n\nMost large scale Machine Learning systems today like Google\u2019s DryBell use some form of Weak Supervision to construct lower quality, large scale training datasets that can be used to continuously retrain and deploy models in a real-world scenario.\n\nThe challenge with continuous retraining is that one needs to maintain prior state (e.g., the learning functions in case of Weak Supervision or a pre-trained model like BERT or Word2Vec for Transfer Learning) that is shared across multiple streams, while continuously updating the model. Apache Beam\u2019s Stateful Stream processing capabilities are a perfect match here including support for scalable Weak Supervision.\n\nPrior work on using Beam\u2019s State coupled with Flink\u2019s dynamic processing capabilities to store and update word embeddings for real-time Online Topic Modeling of text has been presented at Flink Forward Berlin 2018.  Similar streaming pipelines would also work for real-time model updates using Weak Supervision and Transfer Learning. In this talk, we\u2019ll be looking at a framework - Snorkel BeamBell - a framework leveraging Stanford\u2019s Snorkel library for Weak Supervision and Apache Beam for large scale Weak Supervision Learning for online labeling of large amounts of data that can continuously learn new classification models based on Stateful Learning Functions and user feedback.\n\nThe audience would come away with a better understanding of how Weak Supervision with Apache Beam\u2019s stateful stream processing can be used to accelerate the labeling of training data, and real-time training and update of machine learning models.",
  "persons": [
    "Suneel Marthi"
  ]
}
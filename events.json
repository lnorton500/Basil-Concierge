[
  {
    "start": 1577871000000,
    "duration": 25,
    "room": "Janson",
    "title": "Welcome to FOSDEM 2020",
    "subtitle": "",
    "track": "Keynotes",
    "abstract": "<p>FOSDEM welcome and opening talk.</p>",
    "description": "<p>Welcome to FOSDEM 2020!</p>",
    "persons": [
      "FOSDEM Staff"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 50,
    "room": "Janson",
    "title": "LibreOffice turns ten and what's next",
    "subtitle": "Lots to learn, and get excited about",
    "track": "History",
    "abstract": "<p>From ten years of LibreOffice, how can you apply what we\nlearned to your project ? What is going on in LibreOffice today, and\nwhere is it going ? and How can you re-use or contribute to the story.</p>",
    "description": "<p>Come hear about the story of LibreOffice, the reasons we\nstarted - and some of the highlights: successes, failures and other\nlessons learned from our first ten years. Hear how our initial\ndecisions and vision of open-ness and vendor neutrality panned\nout. See what has been learned about building an effective commercial\necosystem, with certification.</p>\n\n<pre><code>Hear about the trajectory of technical updates and how we\n</code></pre>\n\n<p>re-juvenated an open-source code-base through massive re-factoring, as\nwell as re-targetting for web and mobile.</p>\n\n<pre><code>Catch up with the latest in Online innovation, optimization\n</code></pre>\n\n<p>and scalability work as well as our growing integration with lots of\nother Open Source projects.</p>\n\n<pre><code>Finally catch up with the latest and greatest feature/function\n</code></pre>\n\n<p>improvements as we move towards LibreOffice 7, and find out how you\ncan best get involved with the next decade of the LibreOffice story.</p>",
    "persons": [
      "Michael Meeks"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 50,
    "room": "Janson",
    "title": "Over Twenty Years Of Automation",
    "subtitle": "",
    "track": "History",
    "abstract": "<p>Over the past twenty years, the automation landscape has changed dramatically.\nAs our hunger for complex technical infrastructure increased, and our inability to keep up with these demands faltered, we've outsourced a lot of the work to third-parties and cloud providers.\nWe'll step backwards and show where we came from, and where we're going.\nIf we don't understand this future, and step up to the challenge, then we eventually won't control our own computers anymore.\nWe'll discuss this timeline from a tools perspective and showcase many live demos of the past, present, and what will be possible in the future.\nThis presentation will contain many demos and interactive examples. I will showcase some modern ideas I have with my Free Software project called mgmtconfig.</p>",
    "description": "",
    "persons": [
      "James Shubin"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 50,
    "room": "Janson",
    "title": "Blender, Coming of Age",
    "subtitle": "18 years of Blender open source projects",
    "track": "History",
    "abstract": "<p>The presentation is going to be audiovisual and entertaining; based on a number of short videos I want to tell the story of Blender. Starting in late 90s, how Blender became open source, going over the big milestones for Blender, end ending with the fast growth of our project and the interest of the film and game industry. Blender now is a more mature project now, which involves a different dynamics than it used to be. How are we going to tackle the challenges of the industry, while not losing the community that brought us this far?</p>",
    "description": "",
    "persons": [
      "Ton Roosendaal"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 50,
    "room": "Janson",
    "title": "Generation gaps",
    "subtitle": "",
    "track": "History",
    "abstract": "<p>For as long as computers have been around, roughly every 10-15 years, the whole industry underwent a reset and reinvented itself anew… until the early 1990s, when somehow, the industry skipped a generation. Instead, it looked backwards, and adopted an older model of computing. The cost has been very high and is holding back the development of the entire field.</p>",
    "description": "<p>This talk looks at how we turned to the past instead of the future, what we missed out on as a result, and how to move forward. It follows on from the above proposal, but takes a different tack and should stand alone. It looks at where personal computers might have – but didn’t – go in the 1980s and 1990s. At a sampling of advanced OS technologies that never caught on – Plan 9, Inferno, Taos, Oberon – and the cool stuff we lost out on as a result.</p>\n\n<p>It will end with trying to identify the next such generation gap, and why we should consider starting afresh rather than adapting current tech for the next gen.</p>",
    "persons": [
      "Liam Proven"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 50,
    "room": "Janson",
    "title": "HTTP/3 for everyone",
    "subtitle": "The next generation HTTP is coming",
    "track": "Internet",
    "abstract": "<p>HTTP/3 is designed to improve in areas where HTTP/2 still has some shortcomings, primarily by changing the transport layer. HTTP/3 is the first major protocol to step away from TCP and instead it uses QUIC.</p>",
    "description": "<p>HTTP/3 is the designated name for the coming next version of the protocol that is currently under development within the QUIC working group in the IETF.</p>\n\n<p>HTTP/3 is designed to improve in areas where HTTP/2 still has some shortcomings, primarily by changing the transport layer. HTTP/3 is the first major protocol to step away from TCP and instead it uses QUIC.</p>\n\n<p>Daniel Stenberg does a presentation about HTTP/3 and QUIC. Why the new protocols are deemed necessary, how they work, how they change how things are sent over the network and what some of the coming deployment challenges will be.</p>",
    "persons": [
      "Daniel Stenberg"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 50,
    "room": "Janson",
    "title": "State of the Onion",
    "subtitle": "The Road to Mainstream Adoption and Improved Censorship Circumvention",
    "track": "Internet",
    "abstract": "<p>The Tor Project is building usable free software to fight surveillance and censorship across the globe. In this talk we'll give an update on what we got up to during 2019, what happened in the wider Tor ecosystem, and what lies ahead of us.</p>",
    "description": "<p>During the past year the Tor Project has been working hard on improving the software, building and training communities around the world as well as creating an anti-censorship team and roadmap that can push forward technologies to circumvent censorship.</p>\n\n<p>This talk will cover major milestones we achieved and will give an outline about what is lying ahead. In particular, we'll talk about our work to scale the network so it can cope with increased demand as we move forward with our plans for mainstream adoption of Tor Browser and the Tor network.</p>\n\n<p>We will also share updates about our anti-censorship efforts, a year on from the formation of a dedicated Anti-Censorship team, and their work on next generation pluggable transports. Moreover, we'll explain our defense against website traffic fingerprinting attacks and plans for improving onion services and making them more usable (DDoS resistance, better user interfaces for authentication and dealing with errors).</p>\n\n<p>Finally, we'll shed some light onefforts to get Tor support directly embedded into other browsers, like Firefox and Brave, and educating users both by reorganizing the content on our website, creating dedicated community and developer portals and extensive trainings throughout the world.</p>",
    "persons": [
      "Pili"
    ]
  },
  {
    "start": 1577901600000,
    "duration": 50,
    "room": "Janson",
    "title": "SCION",
    "subtitle": "Future internet that you can use today",
    "track": "Internet",
    "abstract": "<p>Do you know where your internet traffic flows? Does it go through China even if you don't want it to? SCION is a new internet architecture aimed at solving this problem. We will show how you can easily join the already existing worldwide network.</p>",
    "description": "<p>The current Internet was not designed with control and security considerations in mind: incidents such as the hijacking of all traffic for YouTube by a Pakistani ISP in February 2008, the Cloudflare DNS service hijacked by AnchNet in May 2018, or a large chunk of European mobile traffic being rerouted through China in June 2019 show that we cannot quite trust the current Internet. SCION is a proposed future Internet architecture aiming to offer high availability and security, even in the presence of actively malicious network operators and devices.</p>\n\n<p>Designing a new Internet from scratch gives us the opportunity to make it work a lot better: we are aiming to notably improve security, availability, and performance. At the same time, just replacing the Internet would not be feasible, and thus we also emphasise practical concerns, such as incremental deployment and backwards compatibility. Thanks to that, SCION is currently the only clean-slate Internet architecture with a world-wide research network and production deployments in several large institutions in Switzerland; and you can start using it today.</p>\n\n<p>In the first part of this talk, we will drive you through the current state of SCION design and implementation, showing how it provides its important features:</p>\n\n<ul>\n<li>path awareness and path control by end hosts</li>\n<li>geofencing and isolation from untrusted actors</li>\n<li>scalability</li>\n<li>backward compatibility with existing infrastructure and protocols</li>\n<li>increased performance by active usage of multiple links</li>\n<li>fast rerouting in case of outages in any segment of the network</li>\n</ul>\n\n\n<p>The world-wide test deployment, SCIONLab, consists of around 50 different points-of-presence around the globe, many of them connected via direct, BGP-free, links. Having many independent organizations belonging to a continually evolving network introduces some non-trivial challenges of managing what you don’t own, which we will also talk about.</p>\n\n<p>We will show a live demo presenting how easy it is today for the end user to join the network and start using the available services. We will also present how taking down a part of the network can look from the user perspective and how SCION prevents a scenario of China or Pakistan hijacking traffic.</p>\n\n<p>To close the talk, we will very briefly present the future plans and the direction in which we want the project to evolve.</p>",
    "persons": [
      "Mateusz Kowalski",
      "Kamila Součková"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "The Selfish Contributor Explained",
    "subtitle": "",
    "track": "Community and Ethics",
    "abstract": "<p>It has become very popular in the last several years to think of free and open source as a community forward activity, indeed the modern approach is to try and form a community or foundation first and do code second.  There is also much talk about maintainer burn out and community exploitation.  However, the same people who talk about this still paraphrase the most famous quote from the Cathedral and the Bazaar \"Scratching your own itch\".  They forget this is your own itch not everyone else's because Open Source begins as a selfish activity.  The fact that communities do form around a load of selfish competing individuals is actually a testament to the unacknowledged power of open source to co-opt the selfish instinct and make it synergistic to some communal good.</p>\n\n<p>This talk will explore the selfish origins of free and open source and dissect the co-option power it has to form broad and deep communities from what are apparently simple transactional engagements.  We'll also explain how some of the more recent community failures have been engendered by the concentration on long term community to the detriment of enabling purely transactional mechanics by which initially selfish contributors come to the project.</p>",
    "description": "<p>The origins of selfish contributions, while called out in the founding canons of the open source and free software movements, were initially not very well recognized until the first open source projects (and here the author will use the example he's familiar with: the Linux Kernel) tried to engage successfully with companies trying to utilize the projects for their own ends.  We became adept at explaining why forking is bad and how your cost structure will rise exponentially if you do it and how the cheapest method of achieving your business goals is in fact to work with the existing community on whatever feature the company was trying to add as part of its business plan.  In fact, those of use who did company engagement because transactional sales people offering a corporation the achievements of business goals in exchange for some unexpected community engagement.</p>\n\n<p>Nowadays, all companies have drunk the open source coolaid and all pledge their loyalty to further the interests of the community and this previous learning is all but forgotten in the third and fourth waves of open source.  However, at their hearts, corporations are still the same business goal focussed transactional entities we had to deal with in the early days of open source and an understanding of how to co-opt their transactional nature would go a long way to assisting new open source developers in their quest to form communities.</p>\n\n<p>This talk will begin with some history of the Linux kernel's corporate engagement, explore and explain some of the processes within open source development that lead to the conversion of transactionalism into long term community synergy (the truly astounding ability of open source to co-opt selfishness) and then give a few examples of how serving the community rather than enabling transactionalism can give rise to burn out and apparent exploitation.  In conclusion we'll give some simple rules to enable this co-opting and suggest how some of the competing interests of the ethical source and social justice movements might likewise be co-opted.</p>",
    "persons": [
      "James Bottomley"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "The Ethics Behind Your IoT",
    "subtitle": "",
    "track": "Community and Ethics",
    "abstract": "<p>Internet of Things (IoT) devices are part of the future we were promised. Armed with our mobile devices, we can control everything from our cars to our toasters to the doors of our homes. Along with convenience, IoT devices bring us ethical quandaries, as designers and users. We need to consider the ethical implicates of the technologies we are building and ask ourselves not just about the ways they are being used, for both good and evil, but the potential use cases we might encounter in the future.</p>",
    "description": "<p>IoT devices are becoming increasingly prevalent in our lives -- even my water filter is wifi enabled. In this session, we'll be looking at two case studies of how one might interact with IoT devices, and then consider the ethical implications of these devices, focused on the social impacts they can have on an individual or a society. While we will focus on smart doorbells and smart locks and situations in which installing these could significantly impact quality of life, we will touch on other common IoT devices.</p>",
    "persons": [
      "Molly de Blanc"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "Freedom and AI: Can Free Software include ethical AI systems?",
    "subtitle": "Exploring the intersection of Free software and AI",
    "track": "Community and Ethics",
    "abstract": "<p>Despite the number of working groups, advisory committees, and coordination roundtables, there is little progress towards creating more ethical and safe AI systems. AI systems are deployed in increasingly fragile contexts. From law enforcement to humanitarian aid, several organizations use AI powered systems to make or inform critical decisions with increasingly outsized side effects.</p>\n\n<p>What is a rights-based approach for designing minimally safe and transparent guidelines for AI systems? In this talk, we explore what a Free AI system might look like. Then, taking research and guidelines from organizations such as Google and the UN Office for the Coordination of Humanitarian Affairs, we propose practical policies and tools to ensure those building an AI system respect user freedom. Lastly, we propose the outlines of a new kind of framework where all derivative works also respect those freedoms.</p>",
    "description": "<p>Rights based approaches are commonly used within humanitarian contexts to approach problems that the sector faces. In this talk, we use the same approach to solving the issue of “unethical” AI systems. We do so by first defining a set of rights that we feel must be respected, proposing a number of methods that we feel helps ensure a system respects those rights, and lastly propose an organizational and regulatory framework that we feel could help encourage and enforce those methods be used by developers of AI systems.</p>",
    "persons": [
      "Justin W. Flory",
      "Michael Nolan"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "How Containers and Kubernetes re-defined the GNU/Linux Operating System",
    "subtitle": "A Greybeard's Worst Nightmare",
    "track": "Containers and Security",
    "abstract": "<p>Free Software (as in Freedom) had won. The vertically integrated Cloud now is the predominant operational paradigm and is threatening to undermine software freedom. To many all seems lost, but the world keeps changing and decentralized compute is making a comeback. Containers and Kubernetes are already having a deep impact on the Linux operating system (OS) that goes well beyond DevOps and cloud-native applications. The concepts of application-centric packaging, process isolation through Linux containers, and immutable infrastructure are shaking up the core traditions of today's GNU/Linux operating systems. These concepts are also challenging the assumptions and approaches derived from the past 40+ years of work that originated with UNIX. The Linux distribution as we know it is coming to an end, and is being replaced by a new concept of containerized, multi-instance, multi-user applications, which can be deployed in scale-out environments as well as for widely distributed compute scenarios. In this session, we'll assess this new OS environment in the times of '5G' de-centralized cloud and take a deep look at the consequences this new OS model has for both developers and admins.</p>",
    "description": "<p>This talk will draw on the history of compute in general and Free and Open Source in specific to explain an evolution of paradigms from the GNU/Linux Distribution to modern Free Software application plattforms build on Kubernetes and how they can shape the future of compute in the face of major technological changes.</p>",
    "persons": [
      "Daniel Riek"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "Fixing the Kubernetes clusterfuck",
    "subtitle": "Understanding security from the kernel up",
    "track": "Containers and Security",
    "abstract": "<p>Kubernetes is complex, and extremely vulnerable. In 2019 we explored the complexity of the Kubernetes codebase, and the antipatterns therein. This year we want to look at understanding how we observe our cluster at runtime. Let's live code some C and C++ and explore the libraries that bring Wireshark, Falco, and Sysdig to life. We concretely demonstrate how we are able to audit a Kubernetes system, by taking advantage of auditing the kernel's syscall information while enriching this data with meta information from Kubernetes.</p>",
    "description": "<p>We start off by presenting the problem of Kubernetes security at runtime. We discuss concerns with namespace and privilege escalation in a Kubernetes environment. We discover how auditing the kernel gives us visibility into both the container layer, as well as the underlying system layer.</p>\n\n<p>We look at building an eBPF probe, or kernel module to begin auditing syscall metrics. We discover how we are able to pull those out of the kernel into userspace, and start exploring powerful patterns for using these metrics to secure a Kubernetes cluster.</p>\n\n<p>The audience walks away understanding how the kernel treats containers, and how we are able to easily make sense of them. The audience also walks away equipped with an OSS toolkit for understanding, observing, and securing a Kubernetes environment.</p>",
    "persons": [
      "Kris Nova"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "Address Space Isolation in the Linux Kernel",
    "subtitle": "",
    "track": "Containers and Security",
    "abstract": "<p>Security is a big problem especially in the cloud of container workloads. This presentation investigates improving security in the Linux kernel itself. The first target is securing sensitive application data, for instance, private keys.</p>",
    "description": "<p>Address space isolation has been used to protect the kernel and userspace programs from each other since the invention of the virtual memory.</p>\n\n<p>Assuming that kernel bugs and therefore exploits are inevitable it might be worth isolating parts of the kernel to minimize damage that these exploits can cause. Moreover, restricted mappings in the kernel mode may improve mitigation of hardware speculation vulnerabilities.</p>\n\n<p>There are several ongoing efforts to use restricted address spaces in Linux kernel for various use cases:\n* speculation vulnerabilities mitigation in KVM\n* support for memory areas visible only in a single owning context\n* hardening of the Linux containers</p>\n\n<p>We are going to present the approach for the implementation of restricted mappings in the Linux kernel and how this implementation would be used with various use-cases.</p>\n\n<p>We are also going to take a closer look at possibility to assign an address space to the Linux namespaces, so that tasks running in namespace A have different view of kernel memory mappings than the tasks running in namespace B. For instance, by keeping all the objects in a network namespace private, we can achieve levels of isolation equivalent to running a separated network stack.</p>",
    "persons": [
      "James Bottomley",
      "Mike Rapoport"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "Guix: Unifying provisioning, deployment, and package management in the age of containers",
    "subtitle": "",
    "track": "Containers and Security",
    "abstract": "<p>This talk will reflect on what GNU Guix has to offer to users and how it compares to other approaches—from CONDA and pip to Flatpak and Docker.  Guix is not only a transactional package manager and declarative GNU/Linux distribution: it’s also an environment manager, a container provisioning tool, and more.  We will describe these tools and our journey to 1.0, emphasizing key properties that set Guix apart:\nreproducibility, transparency, and hackability.</p>",
    "description": "<p>When it comes to software deployment, we are getting used to a new distribution of roles among many actors: traditional distros take care\nof core software pieces, “application bundles” à la Docker/Flatpak provide complex applications, Cabal, Gem, npm, pip, and friends take care of language-specific software, and Puppet/Ansible orchestrate the whole thing.  Each of these tools has its justification, but the end result is a maze that’s hard to deal with.</p>\n\n<p>In this talk I will present GNU Guix, a software deployment toolbox and its associated distro that feature transactional upgrades and rollbacks, declarative OS deployment inspired by Nix, and reproducible builds.  I will show how Guix approaches a variety of use cases: “package management” à la apt-get, environment management à la VirtualEnv, Ansible-style declarative OS deployment, and container provisioning à la Docker.</p>\n\n<p>Guix emphasizes programmability and one of its salient features is that it provides a unified Scheme programming environment to deal with with all the aspects of configuration, deployment, and system management—including service management by PID 1.  I will illustrate how\nthis works out on standalone Guix systems, and show the benefits of the approach by discussing applications that take advantage of Guix as a library to support reproducible software deployment.</p>\n\n<p>Last, I will reflect on the road that led to Guix 1.0 six months ago and present some of the challenges ahead.</p>",
    "persons": [
      "Ludovic Courtès"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Civil society needs Free Software hackers",
    "subtitle": "",
    "track": "Lightning Talks",
    "abstract": "<p>More and more traditionally processes in our society now incorporate, and are influenced by software.</p>",
    "description": "<p>Processes that decide for example: Who will be able to go to which university? Who will be invited for a job interview? How long does someone have to go to jail?</p>\n\n<p>Therefore many organisation which work for people's rights are now confronted with the problems proprietary software creates for society.  The pupils associations, the unions, human right organisations, or environmental organisations -- all of them need to understand how software works to do their work in our society.</p>\n\n<p>To continue to fulfil their role, civil society needs to understand how processes are implemented in software, they need to be able to challenge the assumptions, the values, and the way programmers designed them, and have a better understanding how you could change them.</p>\n\n<p>In short: in a world in which more and more of our live is controlled by software, civil society organisations need us as Free Software hackers to support them doing their job.</p>",
    "persons": [
      "Matthias Kirschner"
    ]
  },
  {
    "start": 1577881200000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "A tool for Community Supported Agriculture (CSA) management, OpenOlitor",
    "subtitle": "",
    "track": "Lightning Talks",
    "abstract": "<p>OpenOlitor is a SaaS open-source tool facilitating the organization and management of CSAs (Community Supported Agriculture) communities. This\ntool covers a large spectrum of functionalities needed for CSAs such as member management, emailing, invoicing, share planning and delivery, absence\nscheduling, etc. This software is organized and monitored by an international community that promotes the tool, helps operate it and support the\ninterested communities. In order to promote the sustainability of the tool and this international community an organization based on CSS\n(Community Supported Software) has been proposed.</p>",
    "description": "<h1>1 - Introduction</h1>\n\n<p>The Community Supported Agriculture movement has grown considerably the last few years proposing a new model of food production and distribution. CSA initiatives connect producers and consumers directly. The consumer receives a basket of locally produced, fresh products on a subscription basis. Risk is shared among all participants of the community, independently of being a consumer or producer. The growing popularity of the CSA model has lead to larger sized communities. Consequently, the management effort for this organizations is becoming unaffordable on a volunteer basis.\nIn such conditions a software helping with the redundant tasks, and connecting all participants of these communities, can be particularly supportive.\nThe main motivations for the creation of OpenOlitor (OO) are to attend to the growing popularity of the CSA model with the aim of keeping the model\neconomically viable and sustainable by reducing management task time investment through building software which facilitates easy organization.</p>\n\n<h1>2 - OpenOlitor</h1>\n\n<p>OpenOlitor is a web-based, open source tool facilitating the organization and management of CSAs (Community Supported Agriculture)\ncommunities. This tool is composed of two different access points:</p>\n\n<ul>\n<li>The participant console: any member holding one or more subscriptions, has an access to this console. This displays basic information about next subscription, products that will be delivered, absences, etc.</li>\n<li>The admin console: only defined administrators can access this console. All the community management tools are available from this portal. Please, refer to the next subsection for a detailed list of functionalities.</li>\n</ul>\n\n\n<h2>2.1 - Functionalities  OpenOlitor (OO) covers the main functionalities CSAs need:</h2>\n\n<ul>\n<li>Members and people management: OO allows the management of all the current members, historical members or interested individuals. A member can be composed of multiple people. Basic information is required for every member, such us contact information, details of payment, etc.</li>\n<li>Subscription types: A member receives one or several baskets (also called shares) of products periodically. Different types of baskets are allowed. Types are defined by the community and may include delivery type, pickup or frequency, for example;</li>\n<li>Subscription management: A member can be subscribed to one or several subscriptions;</li>\n<li>Delivery management: Deliveries are prepared by the administrators. As a result shares will be created where subscriptions, delivery types, prices are calculated automatically;</li>\n<li>Billing: Automatic invoices are created for every member;</li>\n<li>Participatory task management: Some CSAs require a certain community work hours in order to be part of the community. OO can track this information and publish available tasks to the members;</li>\n<li>Absence management: Members can feed the system their pickup absences that would be taken into account automatically;</li>\n<li>Product/producer management: A CSA can have one or multiple producers and each producer can have one or more products. This information can be used for billing purposes or basket shaping purposes;</li>\n<li>Emailing: an integrated emailing functionality is provided for some of the modules already explained. This allows administrators to easily mail information to the participants, such as invoices, general announcements, etc.</li>\n<li>Payment management: SEPA and ESR payment work-flows are integrated;</li>\n</ul>\n\n\n<h2>2.2 - Basic architecture</h2>\n\n<p>The OO platform is divided in back-end and front-end:</p>\n\n<ul>\n<li>Front-end: The front-end is a Web-Application (AngularJS) accessible from any browser. A front-end application is dedicated to every CSA. The code is accessible publicly in Github:</li>\n<li>Administration portal (https://github.com/OpenOlitor/openolitor-client-admin)</li>\n<li><p>Members portal (https://github.com/OpenOlitor/openolitor-client-kundenportal)</p></li>\n<li><p>Back-end: Programmed in Scala using the Akka library for concurrency and distributed systems. A single server can be shared among multiple initiatives. The code is published on Github. Even if the server can be shared among multiple organizations, a MariaDB database is created per initiative. This model guaranties data ownership and privacy. The code is accessible publicly in Github: https://github.com/OpenOlitor/openolitor-server</p></li>\n</ul>\n\n\n<h1>3 - Current numbers</h1>\n\n<p>Currently, seven CSAs are using OpenOlitor to support their organizations in a production environment. Three more CSAs are in the process of transitioning to this digital tool. A few numbers extracted from the last two years and a half to put into perspective the work volume facilitated by OO:</p>\n\n<ul>\n<li>+/- 100'000 baskets already delivered</li>\n<li>+/- 3'000 managed deliveries, average 33 baskets per delivery</li>\n<li>+/- 3'400 subscriptions</li>\n<li>+/- 2'100 subscribers</li>\n</ul>\n\n\n<h1>4 - Hosting and Operations</h1>\n\n<p>As shown by the architecture, the database and the front-end are CSA specific but the server can be used by multiple organizations. We promote the idea of hosting communities where a group of CSAs share the costs for the hosting and the effort for the operational work. This model is currently in use for all CSAs using the software.</p>\n\n<h1>5 - Sustainability model</h1>\n\n<p>As a financial model we promote the idea of Community Supported Software (CSS). This model is directly inspired by the CSA model where all participants share the risk and responsibility of the food production. Transferring this idea to software, an organization working with OO is invited to participate on the maintenance, operation, support and further development of the software. A fixed financial contribution is defined per basket delivered. All contributions are  shared with all projects using the tool. Using this model, the effort and risk is equally shared, independently of the size of the CSA.</p>\n\n<h1>6 - Legal organizations supporting OO</h1>\n\n<p>This software is organized and monitored by an international community that promotes this tool:</p>\n\n<p>OpenOlitor International: Non legally formed organization composed by around 10 members from different European countries. This organization is in charge of the vision and principles of all interactions with the software. This group meets periodically to decide OO main features and how to evolve the platform. Everyone interested in OO is invited to join;\nOpenOlitor association: non-profit organization based in Switzerland. Legal body managing the founds received by initiatives and public sources;\nOpenOlitor operations association: Organization in charge for the operation and support of the Swiss based CSAs;\nSunu: a German based organization that promotes digitalization for CSAs. They are promoting the ue of OO as well as the operational and support for German CSAs.</p>",
    "persons": [
      "Mikel Cordovilla"
    ]
  },
  {
    "start": 1577882400000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "What's in my food ? Open Food Facts, the Wikipedia of Food",
    "subtitle": "Mixing mobile crowdsourcing, ai, opensource and opendata to improve food transparency",
    "track": "Lightning Talks",
    "abstract": "<p>Open Food Facts is a collaborative and crowdsourced database of food products from the whole planet, licensed under the Open Database License (ODBL). It was launched in 2012, and today it is powered by 27000 contributors who have collected data and images for over 1 million products in 178 countries (and growing strong…)\nThis is the opportunity to learn more about Open Food Facts, and the latest developments of the project.</p>",
    "description": "<p>Scan a product using your phone, take a picture, and you're already part of the Open Food Facts revolution !</p>\n\n<p>In this talk we'll show how Open Food Facts leverages open source technologies such as Perl, Python, TensorFlow, MongoDB, Java, Swift, React and Flutter as well as the great power of communities to open data of public interest for health &amp; science, as well as unforeseen applications in your daily life.</p>\n\n<p>We will also introduce you to Open Beauty Facts, for freeing your cosmetic cupboard: shampoos, toothpastes, lipsticks, etc.</p>\n\n<p>How does it work?\nUsing our Android or iPhone app, you can easily scan the barcode of products from your home or local store.\nYou can either check them out (thanks to the decoding and comparison tools) or contribute pictures of their labels, assisted by our label-reading AI.\nThe same can also be done from the website, where additional tools are available to fill in the product details from the labels, navigate or vizualise the database based in various ways, or access the APIs and raw data to make your own tools and analysis.</p>\n\n<p>Open Food Facts is developed and managed by a community of open source, open data and food enthusiasts and is organised as a non-profit association. All its creations are open:\n- the collected data is published as Open Data,\n- the software running the server(s) is open source and reusable (it was also used to create the Open Beauty Facts database),\n- the mobile applications are open source as well.</p>",
    "persons": [
      "Pierre Slamich"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Web3 - the Internet of Freedom, Value, and Trust",
    "subtitle": "On exiting the system and reclaiming control of our digital and physical lives",
    "track": "Lightning Talks",
    "abstract": "<p>For as long as human society has existed, humans have been unable to trust each other. For millennia, we relied on middlemen to establish business or legal relationships. With the advent of Web2.0, we also relayed the establishment of personal connections, and the system has turned against us. The middlemen abuse our needs and their power and we find ourselves chained to convenience at the expense of our own thoughts, our own privacy. Web3 is a radical new frontier ready to turn the status quo on its head, and these are the technologies we're using to make it happen.</p>",
    "description": "",
    "persons": [
      "Bruno Škvorc"
    ]
  },
  {
    "start": 1577884800000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Next, the programmable web browser",
    "subtitle": "How the architectural choices and the Lisp language make for an infinitely extensible web browser",
    "track": "Lightning Talks",
    "abstract": "<p>While actual browsers expose their internals through an API and limit access to the host system, Next doesn't, allowing for infinite extensibility and inviting the users to program their web browser. On top of that, it doesn't tie itself to a particular platform (we currently provide bindings to WebKit and WebEngine) and allows for live code reloads, thanks to the Common Lisp language, about which we'll share our experience too.</p>",
    "description": "<p>Next is a keyboard-oriented, extensible web browser designed for power users. While most (all?) current browsers expose their internals through an API, Next exposes its entire code to the user. Modern browsers limit access to the host system, and Next doesn't, allowing for a broad range of new features. Similar projects have failed due to being tied to a particular platform (Vimperator, Conkeror…), but Next's choice is to have its core written as an independent library, and to provide bindings to web engines (currently WebKit and WebEngine are supported). Next's magic touch is its live coding capability: we can develop a command from a REPL, compile the function and try the changes immediately, without restarting anything. Or just edit our init file and reload it into the current session. This flexbility comes for free thanks to the Common Lisp language, the experience with which we'd like to share too.</p>\n\n<p>Next is usable as of today. It features ad-blocking (only basic for now, contributions are welcome), multi-selection, bookmarks, session persistence, Emacs and Vim-like shortcuts, history seen as a tree, fuzzy completion everywhere, basic git-forking and file management interfaces…</p>\n\n<p>We are entering a new phase of development, with hopefully announcements that we can make public at the time of the conference :)</p>",
    "persons": [
      "Atlas Engineer"
    ]
  },
  {
    "start": 1577886000000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Weblate: open-source continuous localization platform",
    "subtitle": "How to bring your project closer to its users with localization platform that doesn’t bother anyone with manual work.",
    "track": "Lightning Talks",
    "abstract": "<p>You will learn how to localize your project easily with little effort, open-source way. No repetitive work, no manual work with translation files anymore. Weblate is unique for its tight integration to VCS. Set it up once and start engaging the community of translators. More languages translated means more happy users of your software. Be like openSUSE, Fedora, and many more, and speak your users' language now thanks to Weblate!</p>",
    "description": "<p>I will show you the main perks of Weblate and the setup of the project. If you have a project with open repo and you want to start translating it, take your git:// link, and we will set it up right on the spot. FOSDEM is a great time and place to found your translating community.</p>",
    "persons": [
      "Václav Zbránek"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Kapow! A Web Framework for the Shell",
    "subtitle": "",
    "track": "Lightning Talks",
    "abstract": "<p>This talk is about \"Kapow!\" an open source webframework for the shell developed by BBVA Innovation Labs. We will talk about the current development of the project including an overview of Kapow!'s technology stack and the recent release of the first stable version.</p>",
    "description": "<p>The motivation behind the project was to create an adapter between the shell and the web. Allowing users to expose command line programs as HTTP services with a high degree of freedom and flexibility, not imposing any predefined behavior.\nThe project is based on an open specification.</p>\n\n<p>Kapow! supports an increasing list of HTTP features; including forms, websockets and streaming. The architecture is based on exposing a private REST API through which the shell can interact with the incoming user HTTP requests.</p>\n\n<p>Leveraging this architecture an ecosystem of specialized tools is planned, to help with common high level tasks in Kapow! services.\nFor example:\n- Shell commands that can interact with incoming web browser requests and render specialized web pages.\n- Automatic mapping of shell command parameters to REST API through bash completion scripts.</p>\n\n<p>Roberto will do some demos about creating Kapow! services from scratch (expose nmap as a service, tcpdump, pandoc and ffmpeg).</p>",
    "persons": [
      "Roberto Abdelkader Martínez Pérez"
    ]
  },
  {
    "start": 1577888400000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Yjs: A CRDT framework for shared editing",
    "subtitle": "Enable shared editing in every application",
    "track": "Lightning Talks",
    "abstract": "<p>Shared editing is the ability to collaboratively edit the same text in real-time. The market for shared editing solutions is fragmented. Once you choose a solution you will be locked into using a specific editor and a specific backend. Yjs is a data synchronization framework that aims to enable shared editing in all open-source editors using any networking stack.</p>",
    "description": "<p>Yjs is a data synchronization framework that is specifically designed for creating shared editing applications like Google Docs. The number of editors, that Yjs supports, is steadily growing. At this time we implemented shared-editing support for six of the most prominent open-source rich-text editors - including <a href=\"https://quilljs.com/\">Quill</a>, <a href=\"http://prosemirror.net/\">ProseMirror</a>, and <a href=\"https://codemirror.net/\">CodeMirror</a>. We are currently working on integrating Yjs in Gutenberg, the new WordPress editor.</p>\n\n<p>Since Yjs is network agnostic and does not require a central authority to resolve sync conflicts, it is possible to use any networking stack to share document updates of the collaborative document. We created an ecosystem of modules that handle data synchronization over, for example, WebRTC, <a href=\"https://www.datprotocol.com/\">Dat Protocol</a>, <a href=\"https://ipfs.io/\">IPFS</a>, and traditional client-server connections via WebSockets.</p>\n\n<p>In this lightning talk, we want to show the huge potential of Yjs that hopefully will make it the go-to solution for creating shared editing applications on the web.</p>",
    "persons": [
      "Kevin Jahns"
    ]
  },
  {
    "start": 1577889600000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Encrypt your collaboration with CryptPad",
    "subtitle": "Real demo !",
    "track": "Lightning Talks",
    "abstract": "<p>We'll show a real demonstration of how you can encrypt your data and collaborate with others in real-time using the CryptPad Open Source project.\nThis demonstration will include real-time Wysiwyg, Text, Kanban, Spreadsheet, File storage and Teams features allowing to share your documents securely with your friends and co-workers.</p>",
    "description": "<p>The Internet business model is about \"surveillance capitalism\" and every day our data is being used to target us with more and more invading advertisements, and every day data is being leaked by Internet business and cloud providers.</p>\n\n<p>CryptPad is using end-to-end encryption to protect your data, while including innovative algorithms to allow collaboration between users. With CryptPad, the cloud hoster cannot read your data and if data leaks, hackers only get encrypted data which they cannot read without the keys.</p>\n\n<p>Start ditching proprietary privacy invading cloud services for an free software privacy protecting alternative.</p>\n\n<p>Come join the 20000 weekly users of the cryptpad.fr main instance and the 300 other instances available, including the German Pirate Party and C3 Wien instances.</p>\n\n<p>Come join us restoring our privacy.</p>",
    "persons": [
      "Ludovic Dubost"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Protect your data objects, not your network connections",
    "subtitle": "Good news for a paradigm shift",
    "track": "Lightning Talks",
    "abstract": "<p>Agenda</p>\n\n<p>1) Current situation: complicated &amp; incomplete threat models\n2) Concepts worth looking into\n3) data sovereignty\n4) named data networks\n5) zero trust\n6) Our hands-on experience with the above</p>",
    "description": "<p>The current state of play to protect data is a tedious task that involves many stakeholders and blocks resources.\nThe shift from on-premise to private/public cloud systems requires a careful inspection of an assumed threat model.\nThe application of a zero-trust model is one radical shift to authenticate and authorize at any given point in your\nIT landscape, but most importantly it breaks with the assumption that we can design systems that are \"safe and secure\".\nThe talk will highlight the above mentioned concepts and will give a brief outline of a new approach called Named Data\nNetwork (NDN) and how this could improve the situation in terms of data sovereignty.</p>",
    "persons": [
      "Stephan Schwichtenberg"
    ]
  },
  {
    "start": 1577892000000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Optimizing sandbox creation with a FUSE file system",
    "subtitle": "Using sandboxfs to speed up Bazel builds",
    "track": "Lightning Talks",
    "abstract": "<p>The Bazel build system sandboxes each action (e.g. each compiler invocation) to ensure the action only has access to declared inputs and that the action only generates the promised outputs. This ensures that the execution of each build tool is deterministic and not subject to system-wide state. Unfortunately, creating these sandboxes is costly, and every millisecond added to the runtime of each action has a large impact on total build time. Just consider that Bazel focuses on large-ish builds with thousands of actions in them: each little inefficiency quickly multiplies and can result in significant slowdowns, and developers always want faster build times. In this talk, I'll explain how Bazel implements sandboxing and I will cover a FUSE file system I've been developing, sandboxfs, to optimize this process. I'll go into the details of the file system, explain how it started as a Go project and was then rewritten in Rust, and then show some performance metrics.</p>",
    "description": "<p>Possible outline:</p>\n\n<ul>\n<li>Brief introduction to Bazel: what are actions.</li>\n<li>Why is sandboxing of actions important and what it intends to achieve.</li>\n<li>How does sandboxing work in the default case and what problems it carries performance- and correctness-wise.</li>\n<li>The solution: how we can use FUSE to eliminate the major source of slowdowns.</li>\n<li>Brief explanation of what sandboxfs is and how Bazel takes advantage of it.</li>\n<li>Delve a little bit into why the project started in Go and was then rewritten in Rust.</li>\n<li>Present performance metrics.</li>\n<li>Ideas for other possible uses for sandboxfs (NixOS and reproducible package builds).</li>\n</ul>",
    "persons": [
      "Julio Merino"
    ]
  },
  {
    "start": 1577893200000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Indexing Encrypted Data Using Bloom Filters",
    "subtitle": "",
    "track": "Lightning Talks",
    "abstract": "<p>Bloom filters are a probabilistic data structure that tell us where things are not.  They also utilize one way hash functions to build a probabilistic representation of an object.  This talk will address how this structure can be used to provide an index into encrypted data that can be made publicly available with minimal risk.</p>",
    "description": "<p>Talk will cover how bloom filters are constructed, the Flat Bloofi indexing implementation and how to take the properties to be indexed and create Bloom filters, and then how to associate the bloom filter with the encrypted object in the index.</p>\n\n<p>The result is an extremely fast index that can retrieve data items containing partial keys.</p>\n\n<p>After this talk participants will be able to provide search capabilities across a collection of encrypted objects.</p>\n\n<p>Code examples will be provided.</p>",
    "persons": [
      "Claude Warren"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Verifpal",
    "subtitle": "Cryptographic Protocol Analysis for Students and Engineers",
    "track": "Lightning Talks",
    "abstract": "<p>Verifpal is new software for verifying the security of cryptographic protocols. Building upon contemporary research in symbolic formal verification, Verifpal’s main aim is to appeal more to real-world practitioners, students and engineers without sacrificing comprehensive formal verification features. Verifpal represents a serious attempt at making the formal analysis of advanced cryptographic systems such as Signal and TLS 1.3 easier to achieve.</p>",
    "description": "<p>Contemporary research in symbolic formal verification has led to confirming security guarantees (as well as finding attacks) in secure channel protocols such as TLS and Signal. However, formal verification in general has not managed to significantly exit the academic bubble. Verifpal is new software for verifying the security of cryptographic protocols that aims is to work better for real-world practitioners, students and engineers without sacrificing comprehensive formal verification features.</p>\n\n<p>In order to achieve this, Verifpal introduces a new, intuitive language for modeling protocols that is easier to write and understand than the languages employed by existing tools. Its formal verification paradigm is also designed explicitly to provide protocol modeling that avoids user error. By modeling principals explicitly and with discrete states, Verifpal models are able to be written in a way that reflects how protocols are described in the real world. At the same time, Verifpal is able to model protocols under an active attacker with unbounded sessions and fresh values, and supports queries for advanced security properties such as forward secrecy or key compromise impersonation.</p>\n\n<p>Verifpal has already been used to verify security properties for Signal, Scuttlebutt, TLS 1.3 and other protocols. It is a community-focused project, and available under a GPLv3 license.</p>\n\n<p>An Intuitive Protocol Modeling Language:\nThe Verifpal language is meant to illustrate protocols close to how one may describe them in an informal conversation, while still being precise and expressive enough for formal modeling. Verifpal reasons about the protocol model with explicit principals: Alice and Bob exist and have independent states.</p>\n\n<p>Modeling that Avoids User Error:\nVerifpal does not allow users to define their own cryptographic primitives. Instead, it comes with built-in cryptographic functions — this is meant to remove the potential for users to define fundamental cryptographic operations incorrectly.</p>\n\n<p>Easy to Understand Analysis Output:\nWhen a contradiction is found for a query, the result is related in a readable format that ties the attack to a real-world scenario. This is done by using terminology to indicate how the attack could have been possible, such as through a man-in-the-middle on ephemeral keys.</p>\n\n<p>Friendly and Integrated Software:\nVerifpal comes with a Visual Studio Code extension that offers syntax highlighting and, soon, live query verification within Visual Studio Code, allowing developers to obtain insights on their model as they are writing it.</p>",
    "persons": [
      "Nadim Kobeissi"
    ]
  },
  {
    "start": 1577895600000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Mandos",
    "subtitle": "Disk encryption without passwords",
    "track": "Lightning Talks",
    "abstract": "<p>Disk encryption is essential for physical computer security, but seldom used due to the trouble of remembering and typing a password at every restart. We describe Mandos, a program which solves this problem, its security model, and the underlying concepts of its design, as well as its evolution over the 10 years since its initial release.</p>",
    "description": "<p>Any security system must have a clear view of its intended threat model – i.e. what threats it is actually intended to protect against; the specific choices and tradeoffs made for Mandos will be explained. Another danger of security system design is the risk of its non-use; i.e. that the system will not be used for some real or perceived drawbacks, such as complexity. The deliberate design choices of Mandos, involving low-interaction, “invisible” and automatic features, will be covered.  Special emphasis will be made on the many necessary changes made since the last FOSDEM talk in 2015.</p>",
    "persons": [
      "Teddy Hogeborn"
    ]
  },
  {
    "start": 1577896800000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "RedWax - trust only yourself",
    "subtitle": "Easy Industry best practice authentication and security - federated or just for you.",
    "track": "Lightning Talks",
    "abstract": "<p>In this talk we will show you, practical, hands on, how you can secure your application, a small user community or environment using industry best of breed security, fully self-supporting and  <em>without</em> having to rely on a central certificate authority, big-tech or other central powerhouses. As devolved &amp; federated, or as central as you want - you set the rules. Working for you, with your standard-issue iPhone, your router or your laptop out of the box.</p>\n\n<p>Project redwax produces industry best practice crypto in a small package. Available today. If you know how to install the apache webserver - you are almost there.</p>\n\n<p>Project Redwax lets you download,a set of easy to deploy simple tools that capture and hard code a lot of industry best practice and specialist PKI knowledge. So that they can be put into the hands of a much wider community than currently served by a few specialist industries. It provides a non centralised, interoperable, open standard, open source, fully federated trust network where participants are not required to ask permission and can be self sufficient.</p>\n\n<p>This presentation presents what is available <em>today</em> and our plans on how to take this further, to engage with the wider open source community that together we can support individuals, organisations and (small) companies to get best of breed, distributed, peer to peer, security, confidentiality and privacy without having to rely on central infrastructures.</p>",
    "description": "<p>Wouldn’t it be nice to be able to trust your own environment without having to trust a corporation or government? Wouldn’t it be nice to take the sting out of certificate management?</p>\n\n<p>With some hands on examples we introduce the audience to the advantages of running your own certificate authority for security IOT in and around your home and establishing a trusted channel for exchanging information with your friends.</p>\n\n<p>This project (and code) helps you to decentralize trust management so that the values security, confidentiality and privacy can be upheld in public infrastructure and private interactions. We strengthen the existing technologies and infrastructure by providing a modular, very simple and foremost practical set of tools to manage public key based trust infrastructures as currently used.</p>\n\n<p>Project Redwax lets you download, a set of easy to deploy simple tools that capture and hard code a lot of industry best practice and specialist PKI knowledge. So that they can be put into the hands of a much wider community than currently served by a few specialist industries. It provides a non centralised, interoperable, open standard, open source, fully federated trust network where participants are not required to ask permission and can be self sufficient.</p>\n\n<p>With support from NLNET project Redwax has made the code available under the Apache license from a infrastructure that is firmly rooted in Europe. And the good news - it is simple - if you know how to install the Apache Webserver - you are almost there. We are working with the unix/linux community to get the code in standard distro’s and cloud init scripts so that these capabilities are made available to wider community of application developers.</p>\n\n<p>This presentation presents what is available <em>today</em> and our plans on how to take this further, to engage with the wider open source community that together we can support individuals, organisations and (small) companies to get best of breed, distributed, peer to peer, security, confidentiality and privacy without having to rely on central infrastructures, rare knowledge or big interests.</p>\n\n<p>Speaker:        Dirk-Willem van Gulik (Dirkx@apache.org)\nTalk:           50 minutes.\nTravel support:     not needed\nLanguage:               English</p>\n\n<p>BIO\nDirk-Willem van Gulik?</p>\n\n<p>During the startup-years - Dirk-Willem van Gulik helped shape the world-wide-web. He was one of the founders, and the first president, of the Apache Software Foundation; and worked on standards such as HTTP at the Internet Engineering Taskforce. He has worked for the Joint Research Centre of the European Commission, the United Nations, telecommunications firms, the BBC, several satellite&amp;space agencies and founded several startups. He participated in different international standards bodies, such as the IETF and W3C on metadata, GIS, PKI, Security, Architecture and Internet standards. Dirk build the initial engineering team at Covalent - the first open source company; and was one of the Founders of Asemantics, a leader in Enterprise Information Integration; which helped make the Semantic Web a reality. He then initiated Joost.com, a peer to peer based video and build and lead the team that created the worlds first instant play P2P viewer and a back office system with user profile driven advert targeting and payment settlements. He was the Chief Technical Architect at the BBC where has helped shape the audience facing delivery platform Forge in the time for the Olympics and where he made information security and compliance a core enabler for business processes. He currently works on several medical and privacy intensive security projects with a heavy emphasis on Architecture and Governance. When not at work, he loves to sail, hang out at the makerspaceleiden.nl or play with his lego.</p>",
    "persons": [
      "Dirk-Willem van Gulik"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "KDE Itinerary",
    "subtitle": "A privacy by design travel assistant",
    "track": "Lightning Talks",
    "abstract": "<p>Getting your itinerary presented in a unified, well structured and always up to date fashion rather than as advertisement overloaded HTML emails or via countless vendor apps has become a standard feature of digital assistants such as the Google platform. While very useful and convenient, it comes at a heavy privacy cost. Besides sensitive information such as passport or credit card numbers, the correlation of travel data from a large pool of users exposes a lot about people's work, interests and relationships. Just not using such services is one way to escape this, or we build a privacy-respecting alternative ourselves!</p>",
    "description": "<p>Standing on the shoulders of KDE, Wikidata, Navitia, OpenStreetMap and a few other FOSS communities we have been exploring what it would take to to build a free and privacy-respecting travel assistant during the past two years, resulting in a number of building blocks and the \"KDE Itinerary\" application. In this talk we will look at what has been built, and how, and what can be done with this now. In particular we will review the different types of data digital travel assistants rely on, where we can get those from, and at what impact for your privacy.</p>\n\n<p>The most obvious data source are your personal booking information. Extracting data from reservation documents is possible from a number of different input formats, such as emails, PDF files or Apple Wallet passes, considering structured annotations and barcodes, but also by using vendor-specific extractors for unstructured data. All of this is done locally on your own devices, without any online access.</p>\n\n<p>Reservation data is then augmented from open data sources such as Wikidata and OpenStreetMap to fill in often missing but crucial information such as timezones or geo coordinates of departure and arrival locations. And finally we need realtime traffic data as well, such as provided by Navitia as Open Data for ground-based transport.</p>\n\n<p>We will of course also look at how features based on that data can be integrated into applications. While there is the KDE Itinerary mobile application presenting these information in a timeline view with access to the corresponding boarding passes or ticket tokens, the underlying building blocks are explicitly meant to be reusable for wider integration. This might be of particular interest for people working on e.g. email application or digital assistants.</p>\n\n<p>Should the author fail to show up to this presentation it might be that last year's fixes for the barcode scanners at the Brussels airport station didn't work after all ;-)</p>",
    "persons": [
      "Volker Krause"
    ]
  },
  {
    "start": 1577899200000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Gate project",
    "subtitle": "Portable execution state",
    "track": "Lightning Talks",
    "abstract": "<p>This presentation is an introduction of an open source project I have been working on for five years: https://github.com/tsavola/gate.</p>\n\n<p>Building on WebAssembly, Gate makes it possible to snapshot running programs and resume them in diverse environments: unlike with other snapshot-and-restore solutions, the snapshots are portable across CPU architectures and operating systems.  Part of the solution is appropriate resource abstraction.  The presentation hopefully includes a quick demonstration of migration of a live program between x86-64 and ARM64 machines.</p>",
    "description": "<p>A subproject of Gate is a specialized WebAssembly compiler implemented in Go: https://github.com/tsavola/wag</p>\n\n<p>Gate is at the stage where a proof-of-concept has been implemented; it works.  Its future would greatly benefit from wider community engagement, so I wish to present the project and its ideas to a technical user and developer audience.</p>\n\n<ul>\n<li>Blog article: https://savo.la/introduction-to-gate.html</li>\n<li>Server running Gate: https://www.gate.computer</li>\n<li>Port of DOOM for Gate: https://github.com/tsavola/DOOM/tree/gate</li>\n</ul>",
    "persons": [
      "Timo Savola"
    ]
  },
  {
    "start": 1577900400000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "The pool next to the ocean: How to bring OpenSource skills to more people",
    "subtitle": "InnerSource as a way to teach open collaboration skills and facilitate the opensourcing process for enterprises",
    "track": "Lightning Talks",
    "abstract": "<p>The pool next to the ocean: How to bring OpenSource skills to more people</p>\n\n<p>OpenSource powers the world and is everywhere with more and more enterprises and large companies understanding the value of it and the need to be able to be a good OpenSource citizen.\nHowever, not everyone in those enterprises has the skills to participate in OpenSource communities, feels ready to contribute something or to create and run a vibrant OpenSource community. I observed that there are two distinct groups of people - one with OSS background, ability and willingness to operate in that domain and those that will likely only use OSS without any likeliness to contribute or participate.\nLet's change that and build a bridge between those two groups while generating value for the enterprise making it more likely to receive support for this activity.\nInnerSource, the application of OpenSource principles and practices within the enterprise, can be this bridge.\nYou'll learn about creating opportunities for people who haven't been exposed to OpenSource collaboration to learn about the OpenSource ways of collaboration in a safe environment within their organization by creating shared projects internally that follow OpenSource practices and principles.\nYou'll also learn about how organizations can profit from cross-team/silo collaboration and knowledge exchange. Also, the acquisition of very valuable skills by their employees that can facilitate the successful transition of those internal projects into OpenSource and creation of vibrant communities around them.\nThis approach is successfully used by many enterprises, and I'm part of a community who has built and is building OpenSource-d training material for this.\nAttend this talk if you want to learn about how to deal with silo issues within your company, how to facilitate your companies way to transition projects to OpenSource or how to build up skills to successfully interact with OpenSource projects. Also attend if you want to hear a bit about freely available training material explaining InnerSource  concepts for people who haven't been involved in it yet.</p>",
    "description": "",
    "persons": [
      "Johannes Tigges"
    ]
  },
  {
    "start": 1577901600000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Go REUSE to license your code",
    "subtitle": "Free Software licensing made simple for everyone",
    "track": "Lightning Talks",
    "abstract": "<p>Why is it so hard to detect the licensing and copyright information of source code? Because it is a tedious and often confusing task for developers to provide this information. The REUSE project changes that! With three simple steps, it makes adding and reading licensing and copyright information easy for both humans and machines. In this presentation, Max Mehl will guide through the REUSE principles and presents how to make clear licensing simple.</p>",
    "description": "<p>If you want to grant users the freedom to use, study, share, and improve your software, you have to grant those freedoms in the license of the software. To encourage people to develop Free Software, we help developers to understand and apply Free Software licensing. REUSE, started in 2017, contributes to this goal. Any project following the initiative's recommendations makes copyright and licensing information readable to both: humans and machines. This way, we want to ensure that individuals, organisations and companies who are re-using code are aware of the license terms chosen by the original author.</p>\n\n<p>REUSE does not reinvent the wheel. On the opposite, it integrates nicely into development processes and other best practices for Free Software licensing. Additionally, there are tools and documentation to help you get started. We will have a closer look at these during this talk.</p>",
    "persons": [
      "Max Mehl"
    ]
  },
  {
    "start": 1577902800000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Concept Programming, from ideas to code",
    "subtitle": "",
    "track": "Lightning Talks",
    "abstract": "<p>Programming is the art of turning ideas into code.\nIdeas and code do not live in the same space. Any translation is lossy.\nConcept programming is a cross-language approach that focuses on this translation process, and helps identify often overlooked classes of issues.</p>",
    "description": "<p>Programming is the art of turning ideas into code.\nIdeas and code do not live in the same space. Consequently, any translation is lossy. But this loss is not immediately visible. For example, how does your programming language coerce you into coding a concept as simple as \"maximum\" or list in a way that is generally full of warts?\nConcept programming is a cross-language approach that focuses on this translation process, and helps identify often overlooked classes of issues. It separates the \"concept space\" and the \"code space\", and focuses on how the mechanics in one space translate (or not) into the other.</p>\n\n<p>It introduces a few core ideas:\n- Syntactic noise is the difference in look between code and concept. For example, in Lisp, you write (+ 1 2)\n- Semantic noise is the difference in behavior between code and concept. For example, in C, text is null-terminated.\n- Bandwidth is the amount of the concept space covered by the code. For example, the \"+\" operator has higher bandwidth in C++ than in C\n- Signal/noise ratio is the amount of code that does not carry any useful concept. For example, curly braces and semi-colons in C.</p>",
    "persons": [
      "Christophe de Dinechin"
    ]
  },
  {
    "start": 1577904000000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "DeskConnD: Secure, cross-platform IPC on the network",
    "subtitle": "Zeroconf + WAMP = Cross platform IPC",
    "track": "Lightning Talks",
    "abstract": "<p>DeskConnD is a cross-platform, python based daemon that uses Crossbar and WAMP to enable end-to-end encrypted IPC over the network.</p>",
    "description": "<p>In this talk Omer Akram will talk about his new project that he has been working on for the past year to mainly make it easy for developers to add functionalities to their apps that were previously cumbersome to implement, having no platform-specific dependencies enables this project to be cross-platform.</p>\n\n<p>Combining proven technologies like WebSocket/WAMP and ZeroConf, DeskConnD allows different components of a distributed app on the local network to securely communicate and do messaging based on the RPCs and PubSub paradigms.</p>",
    "persons": [
      "Omer Akram"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 30,
    "room": "H.1301 (Cornil)",
    "title": "Getting started with quantum software development",
    "subtitle": "",
    "track": "Quantum Computing",
    "abstract": "",
    "description": "",
    "persons": [
      "Tomas Babej"
    ]
  },
  {
    "start": 1577876700000,
    "duration": 35,
    "room": "H.1301 (Cornil)",
    "title": "Quantum machine learning with PennyLane",
    "subtitle": "",
    "track": "Quantum Computing",
    "abstract": "",
    "description": "",
    "persons": [
      "Joshua Izaac"
    ]
  },
  {
    "start": 1577879400000,
    "duration": 35,
    "room": "H.1301 (Cornil)",
    "title": "Quantum computing hardware and control systems",
    "subtitle": "",
    "track": "Quantum Computing",
    "abstract": "",
    "description": "",
    "persons": [
      "Felix Tripier"
    ]
  },
  {
    "start": 1577882100000,
    "duration": 35,
    "room": "H.1301 (Cornil)",
    "title": "The role of open source in building quantum computing ecosystem from scratch",
    "subtitle": "Context of a developing country",
    "track": "Quantum Computing",
    "abstract": "",
    "description": "",
    "persons": [
      "Hakob  Avetisyan"
    ]
  },
  {
    "start": 1577884800000,
    "duration": 35,
    "room": "H.1301 (Cornil)",
    "title": "Quantum Advantage and Quantum Computing in the Real World",
    "subtitle": "",
    "track": "Quantum Computing",
    "abstract": "",
    "description": "",
    "persons": [
      "Mark Mattingley-Scott"
    ]
  },
  {
    "start": 1577887500000,
    "duration": 35,
    "room": "H.1301 (Cornil)",
    "title": "Quantum circuit optimisation, verification, and simulation with PyZX",
    "subtitle": "",
    "track": "Quantum Computing",
    "abstract": "",
    "description": "",
    "persons": [
      "John van de Wetering"
    ]
  },
  {
    "start": 1577890200000,
    "duration": 35,
    "room": "H.1301 (Cornil)",
    "title": "SimulaQron - a simulator for developing quantum internet software",
    "subtitle": "",
    "track": "Quantum Computing",
    "abstract": "",
    "description": "",
    "persons": [
      "Axel Dahlberg"
    ]
  },
  {
    "start": 1577892900000,
    "duration": 35,
    "room": "H.1301 (Cornil)",
    "title": "Tools for Quantum Machine Learning",
    "subtitle": "",
    "track": "Quantum Computing",
    "abstract": "",
    "description": "",
    "persons": [
      "Jack Hidary"
    ]
  },
  {
    "start": 1577895600000,
    "duration": 35,
    "room": "H.1301 (Cornil)",
    "title": "Computing with the TensorNetwork Library",
    "subtitle": "",
    "track": "Quantum Computing",
    "abstract": "",
    "description": "",
    "persons": [
      "Stefan Leichenauer"
    ]
  },
  {
    "start": 1577898300000,
    "duration": 35,
    "room": "H.1301 (Cornil)",
    "title": "Quantum classifiers, robust data encodings, and software to implement them",
    "subtitle": "",
    "track": "Quantum Computing",
    "abstract": "",
    "description": "",
    "persons": [
      "Ryan LaRose"
    ]
  },
  {
    "start": 1577901000000,
    "duration": 35,
    "room": "H.1301 (Cornil)",
    "title": "Quantum computer brands: connecting apples and oranges",
    "subtitle": "",
    "track": "Quantum Computing",
    "abstract": "",
    "description": "",
    "persons": [
      "Petar Korponaić"
    ]
  },
  {
    "start": 1577903400000,
    "duration": 30,
    "room": "H.1301 (Cornil)",
    "title": "Quantum Open Source Foundation",
    "subtitle": "",
    "track": "Quantum Computing",
    "abstract": "",
    "description": "",
    "persons": [
      "Mark Fingerhuth"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 45,
    "room": "H.1302 (Depage)",
    "title": "State of OpenJDK",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>A review of the past year in the life of the OpenJDK Community, and a look at what's ahead.</p>",
    "description": "",
    "persons": [
      "Mark Reinhold"
    ]
  },
  {
    "start": 1577877600000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Project Loom: Advanced concurrency for fun and profit",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>Project Loom, an OpenJDK project, is \"intended to explore, incubate and deliver Java VM features and APIs built on top of them for the purpose of supporting easy-to-use, high-throughput lightweight concurrency and new programming models on the Java platform.\"  These feature include Lightweight Threads, delimited continuations, and tail-call elimination.</p>\n\n<p>The speaker, a Project Loom team member, will describe the project in depth, in particular the gnarly details of how coroutine and continuation scheduling mechanism works, and a new feature, Scoped Locals.</p>",
    "description": "",
    "persons": [
      "Andrew Haley"
    ]
  },
  {
    "start": 1577879400000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "TornadoVM: A Virtual Machine for Exploiting ​High-Performance Heterogeneous ​Execution of Java Programs​",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>The proliferation of heterogeneous hardware in recent years has driven us to consider that every system we program, most likely includes a mix of computing elements; each of which with different hardware characteristics enabling programmers to improve performance while reducing energy consumption. These new heterogeneous devices include multi-core CPUs, GPUs and FPGAs. This trend has been accompanied by changes in software development norms that do not necessarily favor programmers. A prime example is the two most popular heterogeneous programming languages, CUDA and OpenCL, which expose several low-level features to the API making them difficult to use by non-expert users.</p>\n\n<p>Instead of using low-level programming languages, developers in industry and academia tend to use higher-level, object-oriented programming languages, typically executed on managed runtime environments, such as Java, R, and Javascript. Although many programmers might expect that such programming languages would have already been adapted for transparent execution on heterogeneous hardware, the reality is that their support is either very limited or absent.</p>\n\n<p>In this talk, we present TornadoVM (https://github.com/beehive-lab/TornadoVM), a heterogeneous programming framework for Java programs. TornadoVM co-exists with standard JVMs (e.g., OpenJDK) that implement the JVMCI. TornadoVM consists of three components: 1) a simple API for composing pipelines of existing Java methods, 2) an optimizing JIT compiler that extends the Graal compiler with hardware-aware optimizations that generate OpenCL C code, and 3) a runtime system that executes TornadoVM specific bytecodes, performs memory management, and schedules the code for execution on GPUs, multicore CPUs, and FPGAs. Essentially, TornadoVM is a “VM-in-a-VM” that can adapt execution completely dynamically and transparently to the user, always finding the highest-performing combination of hardware accelerators through dynamic reconfiguration.</p>",
    "description": "",
    "persons": [
      "Thanos Stratikopoulos"
    ]
  },
  {
    "start": 1577881200000,
    "duration": 40,
    "room": "H.1302 (Depage)",
    "title": "ByteBuffers are dead, long live ByteBuffers!",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>Abstract: Direct buffers are, to date, the only way to access foreign,\noff-heap memory. Despite their success, direct buffers suffer from some\nlimitations --- stateful-ness, lack of addressing space,\nnon-deterministic deallocation to name a few --- which makes them a\nless-than-ideal choice under certain workloads. In this talk we paint\nthe path to the future: a safe, supported and efficient foreign memory\naccess API for Java. By providing a more targeted solution to the\nproblem of accessing foreign memory, not only developers will be freed\nby the above limitations - but they will also enjoy improved\nperformances, as the new API is designed from the ground-up with JIT\noptimizations in mind - and all without sacrificing memory access safety.</p>",
    "description": "",
    "persons": [
      "Maurizio Cimadamore"
    ]
  },
  {
    "start": 1577883900000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Free at Last! The Tale of Jakarta EE",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>In September 2017 Oracle announced that it would be migrating governance of the Java EE platform to the Eclipse Foundation, the home of MicroProfile. Two years later Jakarta EE 8 shipped, signaling the successful completion of that move. As a result, Free Java has a new home for a significant piece of the Java ecosystem. A home which is purely open source, vendor neutral, and community led.</p>\n\n<p>This talk will be about how the long and painful journey from Java EE to Jakarta EE unfolded. But more importantly it will focus on how the new Jakarta EE community works, and how there is a new, open, specification process for Java APIs (other than SE) that is available for the community. We are looking forward to welcoming many of those interested in Free Java to participate in driving new innovation in Java APIs for cloud and other exciting use cases.</p>",
    "description": "",
    "persons": [
      "Mike Milinkovich"
    ]
  },
  {
    "start": 1577885700000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Shenandoah 2.0",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>Shenandoah GC landed in JDK12 about a year ago, giving OpenJDK\nanother low-pause garbage collector. It has undergone substantial\nchanges since then. Specifically we have a new barrier scheme, and have\neliminated the extra forwarding pointer word per object, thus\nsubstantially reducing memory footprint. After giving a general\nintroduction to OpenJDK GC landscape and Shenandoah GC, this talk\nfocuses on those recent changes in Shenandoah and what's in it for you.</p>",
    "description": "",
    "persons": [
      "Roman Kennke"
    ]
  },
  {
    "start": 1577887500000,
    "duration": 40,
    "room": "H.1302 (Depage)",
    "title": "JMC & JFR - 2020 Vision",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>JDK Flight Recorder provides production time profiling and diagnostics\nvia a compact events-based infrastructure inside the Hotspot JVM\nitself. JDK Mission Control is a stand-alone application that provides\nreal-time monitoring information for Hotspot JVMs, as well as tools to\nread, automatically analyze and visualize flight recording data\nproduced by JDK Flight Recorder.</p>\n\n<p>When this talk is presented, JMC 7.1.0 has (hopefully) been out for a\nlittle while. This presentation talks about what is new and the\nroadmap for the upcoming JMC 8. We will also discuss recent changes in\nthe project, such as the move to Skara. Towards the end we will demo\nhow JDK Flight Recorder and JMC core libraries can be used to diagnose\napplications deployed in container orchestration platforms.</p>",
    "description": "",
    "persons": [
      "Jie Kang"
    ]
  },
  {
    "start": 1577890200000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Hacking on GraalVM: A (very) Rough Guide",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>The GraalVM project provides, among other options, a means to deliver\nJava programs as compact, self-contained, fast-startup native images.\nGraalVM has been moving from research to development for quite a few\nyears now. However, it is only just beginning to be properly integrated\nwith the latest OpenJDK releases and there is still much to be done to\nget it fully productized and to improve usability and performance.</p>\n\n<p>This talk will recount our joint experiences of trying to add new and/or\nimproved capabilities to the the GraalVM code base. Our story will\nstumble gracelessly from one pitfall to the next cock-up in the hope\nthat by exposing and explaining our own history of lamentable error and\noccasional failure you will be able to avoid being doomed to repeat it.</p>\n\n<p>We will provide a guide to getting started and building GraalVM, an\noverview of the how the compiler, native image generator and other\nelements of the GraalVM toolkit operate plus a map of what code sits\nwhere in the source tree and how it fits together and offer tips for\ndebugging the Graal compiler and native image generator -- all the tasks\nyou will need to perform in order to attain a vantage point from which\nto change or add to the current functionality.</p>",
    "description": "",
    "persons": [
      "Andrew Dinn",
      "Josh Matsuoka"
    ]
  },
  {
    "start": 1577892000000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Reducing OpenJDK Java Garbage Collection times with stack allocation",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>In this talk we'll explore ways that the JVM can reduce the object allocation rate of Java programs automatically by performing stack allocation of objects that are known to be local to a method, or in compiler terms non-escaping. The discussion is focused on employing the escape analysis optimization in the OpenJDK Hotspot C2 compiler to determine which Java objects can be stack allocated, and how this optimization can reduce pressure on the Java JVM garbage collectors.</p>\n\n<p>We'll show some results on how various real world applications can benefit from such optimizations and describe the methodology of how we prototyped this in OpenJDK. Our work is only in prototype state at this moment and we are looking for more data to understand how broadly applicable this optimizations is. This work wouldn't be possible without free open source access to Java.</p>",
    "description": "",
    "persons": [
      "Nikola Grcevski"
    ]
  },
  {
    "start": 1577893800000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "G1: To infinity and beyond",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>Abstract: G1 has been around for quite some time now and since JDK 9 it\nis the default garbage collector in OpenJDK. The community working on G1\nis quite big and the contributions over the last few years have made a\nsignificant impact on the overall performance. This talk will focus on\nsome of these features and how they have improved G1 in various ways. We\nwill also take a brief look at what features we have lined up for the\nfuture.</p>",
    "description": "",
    "persons": [
      "Stefan Johansson"
    ]
  },
  {
    "start": 1577895600000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Just-in-time compiling Java in 2020",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>Client compiler, server compiler, JVMCI, Graal ... What are we using today and how do they work together?\nIn this talk I'll give and overview of the Just-in-time compilers included in OpenJDK and explain how to play with them.\nI'll also address Just-in-time compiler threads and resource related issues.</p>",
    "description": "",
    "persons": [
      "Martin Doerr"
    ]
  },
  {
    "start": 1577897400000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Helpful NullPointerExceptions - The little thing that became a JEP",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>One of the most prevalent - if not the most prevalent - exception type in Java is the NullPointerException. While Java set out to overcome the possibilities\nto do the mistakes one can do when programming in languages like C/C++ by not exposing pointers in the Java language, the misleading term 'pointer' sneaked\ninto this exception. To this day, NullPointerExceptions thrown by the runtime system didn't contain messages. All you had was a callstack and a line number.\nBut in typical expressions and statements there are several dereferences where an NPE can occur in one line.</p>\n\n<p>We - some engineers in the SAP team - thought this could be helped by a little enhancement. The new NPE message gives precise information about the location\nand tries to explain what was going on when a null reference was encountered. However, due to its prominent nature, it eventually became a JEP.</p>\n\n<p>In my talk I'll demonstrate the improvements that come with this enhancement. I will lift the hood a little and provide a glance at its implementation details.\nAnd finally I'll say some words about the current status and share some ideas for further improvements in the area of exception messages.</p>",
    "description": "",
    "persons": [
      "Christoph Langer"
    ]
  },
  {
    "start": 1577899200000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Taming Metaspace: a look at the machinery, and a proposal for a better one",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>When examining memory footprint of a JVM process, the delta between Java\nheap usage and actual working set size can be surprisingly large. The JVM\nuses off-heap memory for a lot of things: thread stacks, compiler arenas,\ncode heap, byte buffers, GC control... however, one of the largest\nconsumers of off-heap memory can be class metadata. Class metadata are\nstored in Metaspace, which includes the Compressed Class Space.</p>\n\n<p>The talk will explore what Metaspace actually is and what is stored there;\ndescribe the architecture of the Metaspace allocator and the Compressed\nClass Space; how it interacts with the GC; how it is sized. We will\nhighlight waste areas and demonstrate how to use jcmd to examine Metaspace\ninternals.</p>\n\n<p>The current implementation of the Metaspace allocator suffers from a number\nof shortcomings. They can manifest in excessive waste and a certain\n\"clinginess\" - an unwillingness to let go of unused memory.</p>\n\n<p>At SAP, we developed an improved version which is more frugal with memory\nand provides a much improved elasticity. So the second part of this talk\nwill concentrate on our new implementation. We will highlight the\ndifferences to the old architecture, demonstrate advantages and examine how\nit works.</p>",
    "description": "",
    "persons": [
      "Thomas Stüfe"
    ]
  },
  {
    "start": 1577901000000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "The OpenJDK JVM : Securing a moving target or What could possibly go wrong?",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>The OpenJDK Java Virtual Machine presents some interesting challenges\nwhen it comes to guarding against potential vulnerabilities. This talk\nwill explain how dynamic class-loading, JIT compilation, speculative\ncompilation and other aspects of the JVM's operation present a moving\nattack surface that presents some very different challenges to those\nfound in other programs or runtimes.</p>\n\n<p>This talk won't say anything about specific vulnerabilities but it will\nidentify a few areas of the OpenJDK JVM where some of these unique types\nof vulnerability have been identified and resolved. It may teach you\nsome things you didn't know about the complexity of the JVM and\nhopefully reassure you that the OpenJDK devs are very aware of what\ncould possibly go wrong. Whether we have got it all right is left as a\nfollow-up exercise for attendees.</p>",
    "description": "",
    "persons": [
      "Andrew Dinn"
    ]
  },
  {
    "start": 1577902800000,
    "duration": 40,
    "room": "H.1302 (Depage)",
    "title": "JRuby Startup and AOT",
    "subtitle": "",
    "track": "Free Java",
    "abstract": "<p>Rubyists work from a command line, which makes JRuby startup time a critical concern. Traditionally, the JVM has not been optimized for startup, but that's changing. This talk will explore all available options for making a heavy runtime like JRuby start up quickly, from using class data caching services like Hotspot's CDS and OpenJ9's Quickstart to ahead-of-time compilation of JRuby using GraalVM's Native Image. We'll compare approaches and trade-offs.</p>",
    "description": "",
    "persons": [
      "Charles Nutter",
      "Thomas Enebo"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 40,
    "room": "H.1308 (Rolin)",
    "title": "Fundamental Technologies We Need to Work on for Cloud-Native Networking",
    "subtitle": "",
    "track": "Software Defined Networking",
    "abstract": "<p>Many people and companies are betting that cloud-native networking\nwill be the preferred way of implementing network functions in an easy\nand scalable way. It is based around the tenants of modularity, high\navailability, scalability, low-overhead networking, and ease of\ndeployment. And a number of companies such as Google has shown that it\nis really possible to achieve these properties with it. But the\narchitectural basis of cloud-native is quite different from the ones\nof virtualization-based NFV, but nevertheless, in many cases we\ncontinue to use the software packages that were designed for that\ndrastically different architecture. The question is, how well does\nthe current set of open source projects used in NFV work in a\ncloud-native environment and what needs to change in them in order to\nrealize the cloud-native vision?</p>\n\n<p>In this presentation, I will define what I mean with cloud-native\nnetworking and from that derive the system requirements needed to realize\nthat vision. Based on these requirements, we can deduce a number of\nbasic architectural properties, features and services that are needed\nin the system to be able to satisfy these requirements. Then I will go\nthrough the most popular open source projects such as Linux, DPDK and\nOVS and see how they satisfy these architectural properties and\nfeatures. The main contribution of this presentation will be to show\nwhat we need to work on within these SW packages in order to realize\ncloud-native networking. Or maybe we need completely new SW projects\nto be able to achieve this.</p>",
    "description": "",
    "persons": [
      "Magnus Karlsson"
    ]
  },
  {
    "start": 1577877000000,
    "duration": 20,
    "room": "H.1308 (Rolin)",
    "title": "Skydive",
    "subtitle": "A real time network topology and protocols analyzer",
    "track": "Software Defined Networking",
    "abstract": "<p>Skydive is an open source real-time network topology and protocols analyzer providing a comprehensive way of understanding what is happening in your network infrastructure.</p>",
    "description": "<p>Skydive is a toolbox to monitor, visualize and troubleshoot an infrastructure.</p>\n\n<p>It first collects all the information about the physical and logical infrastructure : network interfaces, Linux and Openvswitch bridges, network namespaces, Docker/runc containers, Kubernetes objects, virtual machines and more. All these objects are stored into a graph to allow the operator to visualize and query the whole topology. On top of this, Skydive is able to inject, capture and analyze traffic at any point of this infrastructure - using various technics such as AFpacket, eBPF, DPDK or SFlow samples. This make possible use cases like :</p>\n\n<pre><code>• troobleshoot dropped packets\n\n• get metrics and bandwidth about some specific traffic\n\n• trigger alert on some abnormal traffic detection\n\n• get latency on the whole path of a flow\n\n• and a lot more...\n</code></pre>\n\n<p>Skydive is agnostic to any SDN, container orchestration engine or virtualization platform. That being said, Skydive has plugins for specific technologies such as Kubernetes/Istio, OpenStack, Openvswitch, Network Service Mesh, OpenContrail, VPP and more.</p>\n\n<p>This presentation will present the architecture of Skydive and demonstrate some use cases through a demo.</p>",
    "persons": [
      "Sylvain Baubeau",
      "Sylvain Afchain"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 40,
    "room": "H.1308 (Rolin)",
    "title": "Do you really see what’s happening on your NFV infrastructure?",
    "subtitle": "(and what can you do about it?)",
    "track": "Software Defined Networking",
    "abstract": "<p>As CoSP’s accelerate their adoption of SDN and NFV technologies, the increased need for metrics, performance measurement and benchmarking becomes a focus, to ensure the continued delivery of “best in class” services. As NFV environments have grown in size and complexity, the tools required to gain this greater visibility into the NFVi need to continue to evolve to meet the requirements for manageability, serviceability and resiliency.</p>\n\n<p>Using Collectd as a metrics collection tool, OPNFV Barometer monitors the performance of the NFVi resources and has the capability to expose these insights via open industry standard interfaces to analytics or MANO components for potential enforcement or corrective actions.  Barometer works with related open source technologies and communities (collectd, DPDK, OpenStack, Prometheus, SAF, etc.) to provide numerous metrics and events that address various different use cases such as service healing, power optimization and ensuring application QoS.</p>",
    "description": "",
    "persons": [
      "Emma Foley",
      "Krzysztof Kepka"
    ]
  },
  {
    "start": 1577880600000,
    "duration": 20,
    "room": "H.1308 (Rolin)",
    "title": "Endless Network Programming − An Update from eBPF Land",
    "subtitle": "",
    "track": "Software Defined Networking",
    "abstract": "<p>The Linux kernel networking capabilities have been undergoing major changes over the last years. At the heart of the performance gain, eBPF (extended Berkeley Packet Filter) and XDP (eXpress Data Path) have brought new possibilities in terms of tracing and network packet processing. eBPF is a trendy topic in the Linux world, and today it needs little introduction among the SDN and NFV community. But the technology is still under heavy development, bringing new features, more flexibility, and better performance to the users. This presentation is an update on the latest evolutions in the eBPF world!</p>\n\n<p>Many of those changes occur directly inside the eBPF subsystem architecture. New program types are being added. Early constraints such as the maximal number of instructions for programs, or the unavailability of loops, are changing. The internals are improved with support for debug information (BTF) or 32-bit instructions. And many new mechanisms are implemented, such as global data support, the “BPF trampoline”, batched map operations (in progress), XDP program chaining (in progress). Let's review all the latest trends in eBPF kernel development!</p>\n\n<p>But beyond kernel code, eBPF has grown as a full ecosystem, with a variety of tools used to work with it, or to build upon it. Bpftool, a reference utility to manage eBPF programs, keeps evolving. The networking projects using eBPF keep growing in number (e.g. Katran, Suricata, Sysdig, Hubble, Libkefir) or in features (e.g. Cilium). Let's review (briefly) some of those projects that assert eBPF as one of the essential fast dataplane solutions in the Linux world.</p>",
    "description": "",
    "persons": [
      "Quentin Monnet"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 20,
    "room": "H.1308 (Rolin)",
    "title": "Replacing iptables with eBPF in Kubernetes with Cilium",
    "subtitle": "",
    "track": "Software Defined Networking",
    "abstract": "<p>Michal Rostecki is a Software Engineer working at SUSE. He's working on Cilium, both upstream and on integrating it with openSUSE Kubic Project and SUSE CaaS Platform.</p>\n\n<p>Swaminathan Vasudevan is a Software Engineer working at SUSE. Worked on Neutron Networking Upstream and currently migrating to Cilium and openSUSE Kubic Project and SUSE CaaS Platform.</p>",
    "description": "<p>Cilium is an open source project which provides networking, security and load balancing for application services that are deployed using Linux container technologies by using the native eBPF technology in the Linux kernel.\nIn this presentation we would talk about:\n- The evolution of the BPF filters and will explain the advantages of eBPF Filters and its use cases today in Linux especially on how Cilium networking utilizes the eBPF Filters to secure the Kubernetes workload with increased performance when compared to legacy iptables.\n- How Cilium uses SOCKMAP for layer 7 policy enforcement\n- How Cilium integrates with Istio and handles L7 Network Policies with Envoy Proxies.\n- The new features since the last release such as running Kubernetes cluster without kube-proxy, providing clusterwide NetworkPolicies, providing fully distributed networking and security observability platform for cloud native workloads etc.</p>",
    "persons": [
      "Michal Rostecki"
    ]
  },
  {
    "start": 1577883000000,
    "duration": 20,
    "room": "H.1308 (Rolin)",
    "title": "Analyzing DPDK applications with eBPF",
    "subtitle": "Sharpening the toolset",
    "track": "Software Defined Networking",
    "abstract": "<p>One of the challenges of doing software network applications is observing the inputs, outputs, and what the application is doing with them. Linux provides a rich tool set with eBPF but integrating this into a DPDK application is challenging. The DPDK libraries for capturing is incomplete which leads to lots of time debugging the tools. This talk addresses these issues, recommends solutions and proposes enhancements to make developers live easier.</p>",
    "description": "<p>The DPDK provides a limited form of packet capture, but it only works on a single interface with no filtering and inaccurate timestamps.\nI go over  what packet capture does now, how it can be improved, and how it can be integrated with other tracing.</p>\n\n<p>This talk is an extension of the talk  (based on community feedback) given in 2019 at the DPDK userspace summit.</p>",
    "persons": [
      "Stephen Hemminger"
    ]
  },
  {
    "start": 1577884200000,
    "duration": 20,
    "room": "H.1308 (Rolin)",
    "title": "XDP and page_pool API",
    "subtitle": "",
    "track": "Software Defined Networking",
    "abstract": "<p>XDP support is an increasing trend on the network devices. XDP main goal is\nprocessing packets at the lowest point in the software stack avoiding\noverheads. Memory recycling of received buffers achieved through\nthe in kernel page<em>pool API plays a fundamental role in the increased performance.\nAdding XDP support on a driver can be non-trivial. In this talk we'll demonstrate\nhow porting a standard ethernet driver (mvneta/netsec) to XDP and the page</em>pool API can\nboost performance.\nPart of the page_pool evolution involves adding the recycling support\nin the kernel's SKB stack and leverage the increased performance\nattributes of the API.</p>",
    "description": "",
    "persons": [
      "Ilias Apalodimas",
      "Lorenzo Bianconi"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 40,
    "room": "H.1308 (Rolin)",
    "title": "Weave Net, an Open Source Container Network",
    "subtitle": "Five years with no central point of control",
    "track": "Software Defined Networking",
    "abstract": "<p>A tour of the internals of Weave Net, one of the most popular container networks:\ndesign challenges and lessons learned from five years in the wild. Including\nKubernetes integration and how CNI was born.</p>\n\n<p>Weave Net is written in Go, using many Linux kernel features such as veths, bridges and iptables.\nAimed at developers rather than network engineers, Weave Net tries to be self-configuring and\nfind the best available transport between nodes. The control plane operates via gossip,\nwith no central point of control.</p>",
    "description": "",
    "persons": [
      "Bryan Boreham"
    ]
  },
  {
    "start": 1577887800000,
    "duration": 50,
    "room": "H.1308 (Rolin)",
    "title": "Rethinking kubernetes networking with SRv6 and Contiv-VPP",
    "subtitle": "",
    "track": "Software Defined Networking",
    "abstract": "<p>Kubernetes (k8s) is currently the de-facto standard for containers orchestration. However, K8s does not provide any solution for handling containers networking. Instead, it offloads the networking to third-party certified plugins called CNI plugins. Contiv-VPP is a k8s CNI plugin that offers fast I/O by leveraging the carrier-grade capabilities of VPP and DPDK in the dataplane.</p>\n\n<p>The adoption of containers and microservices calls for IPv6 to provide addressing and reachability for such massive number of endpoints. SRv6 leverages the IPv6 dataplane to provide overlay networking, traffic engineering, load balancing, network policy and service chaining.</p>\n\n<p>In this talk, we present an SRv6-based solution for k8s networking. We will show how SRv6 is used for pod-to-pod communication, k8s services and service function chaining (SFC), and how SRv6 solves several k8s networking challenges. We will also show the integration of our solution in Contiv-VPP. This solution is the result of combined effort between Bell Canada, Cisco and Pantheon.tech.</p>",
    "description": "",
    "persons": [
      "Ahmed Abdelsalam",
      "Miroslaw Walukiewicz",
      "Filip Gschwandtner",
      "Daniel Bernier"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 40,
    "room": "H.1308 (Rolin)",
    "title": "Akraino Edge KNI blueprint",
    "subtitle": "A Kubernetes Native Infrastructure approach to the Edge",
    "track": "Software Defined Networking",
    "abstract": "<p>Blueprints in the Kubernetes-Native Infrastructure Blueprint Family leverage the best-practices and tools from the Kubernetes community to declaratively manage edge computing stacks at scale and with a consistent, uniform user experience from the infrastructure up to the services and from developer environments to production environments on bare metal or on public cloud.</p>\n\n<p>All blueprints in this family share the following characteristics:</p>\n\n<ul>\n<li>K8s Machine API: declarative API to manage a configure the infrastructure of a cluster.</li>\n<li>Operator Framework: automated and secure lifecycle of applications running on the edge stack.</li>\n<li>Kubernetes-native workloads: allowing the mix of CNFs and VM-based workloads via Kubevirt.</li>\n</ul>\n\n\n<p>Come and see the leading edge!</p>",
    "description": "<p>Launched in 2018, Akraino Edge Stack aims to create an open source software stack that supports high-availability cloud services optimized for edge computing systems and applications.</p>\n\n<p>As part of the Akraino project, Kubernetes-Native-Infrastructure blueprint family represents the reference edge stack managed as a declarative platform, where controllers monitor a system for deviations between the user-declared target state and reality and take corrective\nactions to reconcile reality with the declared target state.</p>\n\n<p>KNI blueprints cover up two different use cases:</p>\n\n<ul>\n<li><p>Provider Access Edge: as part of the network transformation, telco operators are moving to run its radio access network in a cloud-native manner. Technologies like vRAN will be only possible with a declarative approach, and leveraging open networking best practices.</p></li>\n<li><p>Industrial Edge: workloads such IoT, AI/ML, AR/VR, and ultra-low latency control will be run in the edge. These workloads will require specific hardware such GPUs and FPGAs. KNI can show how this needs can be a reality today.</p></li>\n</ul>\n\n\n<p>With Kubernetes Native Infrastructure learn about the k8s way of managing infrastructure. By defining a declarative state, the edge administrator will be able to manage thousands of sites by following an innovative GitOps approach.</p>\n\n<p>If you are interested in these exciting topics, don't miss the talk!</p>",
    "persons": [
      "Yolanda Robla Mota",
      "Ricardo Noriega"
    ]
  },
  {
    "start": 1577893200000,
    "duration": 40,
    "room": "H.1308 (Rolin)",
    "title": "Fast QUIC sockets for cloud networking",
    "subtitle": "Using vector packet processing for QUIC acceleration and offload",
    "track": "Software Defined Networking",
    "abstract": "<p>QUIC was introduced by Google to move the transport protocol implementation out of the kernel, and is now being standardized in the IETF. It provides both encryption and multiplexing, and will be the default transport for HTTP/3. In this talk we'll present the work we've done investigating whether QUIC would benefit from vectorized packet processing, the impact it has on performance and how it can be consumed by external applications.</p>\n\n<p>VPP (vector packet processing) is a fast network data plane, part of the Linux Foundation FD.io project providing fast network functions on top of DPDK. It provides an optimized support of TCP &amp; UDP allowing significant performance improvements. In this presentation, we'll discuss:</p>\n\n<ul>\n<li>How we took advantage of the open source protocol implementation quicly and vpp's hoststack, to implement fast QUIC sockets.</li>\n<li>How this can be consumed by external applications and to what ends.</li>\n<li>What this enables regarding hardware and software offloads.</li>\n</ul>",
    "description": "",
    "persons": [
      "Nathan Skrzypczak",
      "Aloys Augustin"
    ]
  },
  {
    "start": 1577895600000,
    "duration": 20,
    "room": "H.1308 (Rolin)",
    "title": "Mixing kool-aids! Accelerate the internet with AF_XDP & DPDK",
    "subtitle": "",
    "track": "Software Defined Networking",
    "abstract": "<p>\"With its recent advancements, AF<em>XDP is gaining popularity in the high performance packet processing space. As a result, existing frameworks for packet processing, such as DPDK, are integrating AF</em>XDP support to provide more options for moving packets to user space applications. The challenge with such integration is that both AF_XDP and frameworks like DPDK have their own assumptions and constraints about such things as, for example, how to align or manage packet buffers, making the integration less straight forward than it might appear at first glance.</p>\n\n<p>This talk takes a look at the usability of AF<em>XDP pre-kernel v5.4, before diving into the recent challenges we encountered when integrating DPDK and AF</em>XDP, and how we made changes (on both sides) to allow the two to work together in a much more seamless manner.\"</p>",
    "description": "",
    "persons": [
      "Ciara Loftus",
      "Kevin Laatz"
    ]
  },
  {
    "start": 1577896800000,
    "duration": 40,
    "room": "H.1308 (Rolin)",
    "title": "Dial your Networking Code up to 11",
    "subtitle": "Vectorizing your network app to break the performance barrier",
    "track": "Software Defined Networking",
    "abstract": "<p>Modern CPUs provide a wide variety of Single-instruction-multiple-data (SIMD) instructions, or vector instuctions, for operating on larger blocks of data than with regular instructions. Though thought of by many programmers primarily as instructions for doing calculations in parallel on arrays of data, these vector instructions can actually be used in other ways to accelerate packet processing applications. This talk goes through a number of examples in open-source projects, such as DPDK and OVS, where vector instructions have been used to boost performance significantly, and explains the general techniques used that can be applied to other applications.</p>",
    "description": "<p>The talk focuses on the work done on DPDK and OVS to leverage the SSE and AVX instruction sets for packet acceleration. It shows how the different tasks to be performed in those applications can be mapped to SIMD instructions, and presents general guidelines on how to think about packet processing work from a vectorization viewpoint. It also discusses some considerations in application design so as to allow the app to run with best performance on a variety of platforms, each of which may have different instruction sets available.</p>",
    "persons": [
      "Bruce Richardson",
      "Harry van Haaren"
    ]
  },
  {
    "start": 1577899200000,
    "duration": 20,
    "room": "H.1308 (Rolin)",
    "title": "Userspace networking: beyond the kernel bypass with RDMA!",
    "subtitle": "Using the RDMA infrastructure for performance while retaining kernel integration",
    "track": "Software Defined Networking",
    "abstract": "<p>While userspace networking has demonstrated great performance benefits, it does come with greater complexity than kernel networking.</p>\n\n<p>In parallel, Remote Direct Memory Access (RDMA) was developed as an efficient way to move data in HPC and storage clusters with great success.</p>\n\n<p>Key properties of this technology are also highly desirable for userspace networking: native integration with the operating system (OS), OS bypass and a very efficient software interface.\nRDMA-capable network adapters are now enabling standard Ethernet networking functions through the RDMA interface, allowing userspace networking software such as <a href=\"https://fd.io\">VPP</a> to achieve extreme performance while integrating transparently with the OS.</p>\n\n<p>We'll present:</p>\n\n<ul>\n<li>the RDMA Ethernet concepts, architecture and interfaces</li>\n<li>how VPP leverages it</li>\n<li>the problems solved by this architecture and the usecase it enables</li>\n</ul>",
    "description": "",
    "persons": [
      "Benoît Ganne"
    ]
  },
  {
    "start": 1577900400000,
    "duration": 20,
    "room": "H.1308 (Rolin)",
    "title": "Vita: high-speed traffic encryption on x86_64 with Snabb",
    "subtitle": "Coming to your cloud with XDP, AVF, and Kubernetes integration",
    "track": "Software Defined Networking",
    "abstract": "<p>Vita is a high-performance IPsec VPN gateway designed with medium and large network operators in mind. It is written in a high-level language (Lua) using the Snabb networking toolkit and achieves high performance via networking in userspace, i.e. bypassing the kernel network stack.</p>\n\n<p>This talk will discuss Vita and how it was developed using Snabb. Topics include: fast software networking using a dynamic, high-level language; cryptographic ciphers implemented in software software accelerated by x86 extensions; modern cryptography; limiting complexity; multi-core scaling; YANG enabled control planes; minimalist NIC drivers.</p>",
    "description": "",
    "persons": [
      "Max Rottenkolber"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 5,
    "room": "H.1309 (Van Rijn)",
    "title": "DNS Devroom Opening",
    "subtitle": "",
    "track": "DNS",
    "abstract": "<p>Welcome to the DNS DevRoom</p>",
    "description": "",
    "persons": [
      "Shane Kerr",
      "Pieter Lexis",
      "Peter van Dijk"
    ]
  },
  {
    "start": 1577874900000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "DNS Management in OpenStack",
    "subtitle": "What is the OpenStack DNS API?",
    "track": "DNS",
    "abstract": "<p>All major clouds have integrated DNS management these days, and OpenStack is one of them. We will run through the OpenStack DNS (Designate) project - how it works, why we laid it out the way we did, how you can use it, and how other OpenStack components can leverage it.</p>",
    "description": "<p>We will run through the general architecture of the project, and show how we can remain a simple control layer over multiple DNS servers and service providers.</p>\n\n<p>We will show how you can run Designate stand alone, as a multi tenant API for managing DNS inside your company, and how you can use the ability to have multiple pools of servers available for multiple purposes.</p>\n\n<p>Finally we will show the myriad of both OpenStack and other Open Source software integrations for DNS management, and DNS-01 ACME validation.</p>",
    "persons": [
      "Graham Hayes"
    ]
  },
  {
    "start": 1577877000000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "HashDNS and FQDNDHCP",
    "subtitle": "IPv6 DNS configuration made easy",
    "track": "DNS",
    "abstract": "<p>Would you like a DNS server for IPv6 where adding a new node is as simple as typing in its name?\nIf the answer is yes, try HashDNS.</p>",
    "description": "<p>IPv6 autoconfiguration methods can give addresses to interfaces but do not provide any means of configuring the DNS. So autoconfiguration is suitable for clients. If a host has to act as a server, it must have a fully qualified domain name and the DNS service has to map its name to its IP address.</p>\n\n<p>In the Internet of Thread scenario, the number of network nodes can be orders of magnitude higher than before, as each process or thread can be a node. This idea of hash based IPv6 addresses is a viable solution to the problem to manage the DNS resolution in IoTh environments.</p>\n\n<p>The host part of an IPv6 address can be computed as the result of a hash function computer on the fully qualified domain name.</p>\n\n<p>In this way it is possible to write a DNS server able to resolve the addresses of any hostname in a sub-domain provided the network prefix of that sub-domain.</p>\n\n<p>The installation of a new node of the network (computer, namespace, IoTh process) is as simple as providing it with its IPv6 address (the one obtained by concatenating the network prefix and the host address computed by the hash function).</p>\n\n<p>There is no need to change the configuration of the DNS.</p>\n\n<p>Actually the installation of a new node (or its renaming) is even simpler than that. The host can use a DHCP service designed to get the right address from the DNS server given its fully qualified domain name.</p>\n\n<p>So a system administrator has nothing to do more than assigning the new node its name. (They have just to baptize the new node)</p>",
    "persons": [
      "Renzo Davoli"
    ]
  },
  {
    "start": 1577879100000,
    "duration": 20,
    "room": "H.1309 (Van Rijn)",
    "title": "State of djbdnscurve6",
    "subtitle": "IPv6 LLU address support",
    "track": "DNS",
    "abstract": "<p>The fehQlibs and djbdnscurve6 provide both a DNS library which support IPv6 LLU addresses. The inclusion and use of IPv6 LLU addresses is discussed. While the typical Unix /etc/resolv.conf is applied system-wide and the Microsoft Window's pendent works interface-dependent, here application specific DNS settings can be used.</p>",
    "description": "<p>Overview:\n1. Background and heritage on fehQlibs and djbdnscurve6\n2. Application specific DNS resolver settings\n3. Integration of IPv6 LLU addresses - benefits\n4. Integration of IPv6 LLU addresses - recipe\n5. Outlook and future challenges</p>",
    "persons": [
      "Erwin Hoffmann (feh)"
    ]
  },
  {
    "start": 1577880600000,
    "duration": 20,
    "room": "H.1309 (Van Rijn)",
    "title": "Testing DoH and DoT servers, compliance and performance",
    "subtitle": "",
    "track": "DNS",
    "abstract": "<p>Of course, encrypting DNS is necessary for privacy and security, like\nfor every other Internet protocol. That's why DoT and DoH deployment\nis very important, so that users could safely go to a resolver they\ntrust. Now, it is time to assert the technical compliance and\nperformance of these trusted resolvers. We will talk about the things\nthat could and should be tested against DoT and DoH servers and how to\nimplement it. We will then discuss performance measurements, specially\nwith the opportunities brought by parallelism (both in DoT and DoH)\nand the challenges they create for measurements.\nThis talk will be inspired by the development of a tool which is, at\nthis stage, in a very alpha state.</p>",
    "description": "",
    "persons": [
      "Stéphane Bortzmeyer"
    ]
  },
  {
    "start": 1577882100000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "Improving BIND 9 Code Quality",
    "subtitle": "Why is concurrent programming so hard?",
    "track": "DNS",
    "abstract": "<p>BIND 9 consists of a huge and old codebase. In this talk, I would like you to show all the available tools that we use on regular basis to improve, refactor and make the BIND 9 code safer. I'll show the examples of various Google/LLVM Sanitizers, cppcheck, LLVM scan-build and semantic patching using coccinelle.</p>",
    "description": "",
    "persons": [
      "Ondřej Surý"
    ]
  },
  {
    "start": 1577884200000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "unwind(8)",
    "subtitle": "A privilege-separated, validating DNS recursive nameserver for every laptop",
    "track": "DNS",
    "abstract": "<p>DNS is easy. You type fosdem.org in your browser's address bar, hit enter and you will be greeted by your favorite open-source event's start page. Actually...</p>",
    "description": "<p>We will introduce unwind(8) - an always-running, validating DNS recursive nameserver, answering queries on localhost (127.0.0.1). We will explain its privilege-separated design and show that it is secure to run this daemon by default. We will then show how its novel approach of observing changes in network location and actively probing the quality of the local network improve the user experience in DNS resolution. The focus will be on laptops that move through many networks, some good, some bad, some outright hostile.</p>\n\n<p>We will compare unwind(8) to prior solutions and show how its design enables it to run without user intervention.</p>",
    "persons": [
      "Florian Obser"
    ]
  },
  {
    "start": 1577886300000,
    "duration": 15,
    "room": "H.1309 (Van Rijn)",
    "title": "extending catalog zones",
    "subtitle": "auto-maintain DNS servers",
    "track": "DNS",
    "abstract": "",
    "description": "",
    "persons": [
      "Leo Vandewoestijne"
    ]
  },
  {
    "start": 1577887500000,
    "duration": 20,
    "room": "H.1309 (Van Rijn)",
    "title": "The Different Ways of Minimizing ANY",
    "subtitle": "",
    "track": "DNS",
    "abstract": "<p>The DNS Protocol has features that have grown to become liabilities.  The query type \"ANY\" is one.  Earlier this year a published RFC document describes how a DNS server may respond to such queries while reducing the liability.  But the document does not define a definitive means for a server to signal that it is differing from the original protocol.  This presentation measures of the impact of having no definitive means specified and examines the \"fear, uncertainty, and doubt\" of lacking explicit signals.</p>",
    "description": "<p>The \"minimal ANY responses\" RFC (Providing Minimal-Sized Responses to DNS Queries That Have QTYPE=ANY, a.k.a. RFC 8482) results in about 1% of the TLD nameservers indicating they are minimizing ANY responses.\n That's (only) about 250 cases.\n What is troubling is that there are about 9 different responses observed to indicate the response is \"minimized\"\n 9 different ways in just 250 samples, \"fuzzing\" the protocol\n The morale of this tale is that \"fuzzying\" the protocol is worrisome.  (Not that minimizing ANY is a bad thing.)</p>",
    "persons": [
      "Edward Lewis"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 35,
    "room": "H.1309 (Van Rijn)",
    "title": "Check Yourself Before You Wreck Yourself",
    "subtitle": "Auditing and Improving the Performance of Boomerang",
    "track": "Web Performance",
    "abstract": "<p>Boomerang is an open-source Real User Monitoring (RUM) JavaScript library used by thousands of websites to measure their visitor's experiences. The developers behind Boomerang take pride in building a reliable and performant third-party library that everyone can use without being concerned about its measurements affecting their site.  We recently performed and shared an audit of Boomerang's performance, to help communicate its \"cost of doing business\", and in doing so we found several areas of code that we wanted to improve. We'll discuss how we performed the audit, some of the improvements we've made, how we're testing and validating our changes, and the real-time telemetry we capture for our library to ensure we're having as little of an impact as possible on the sites we're included on.</p>",
    "description": "<p>Boomerang is an open-source Real User Monitoring (RUM) JavaScript library used by thousands of websites to measure their visitor's experiences.</p>\n\n<p>Boomerang runs on billions of page loads a day, either via the open-source library or as part of Akamai's mPulse RUM service.  The developers behind Boomerang take pride in building a reliable and performant third-party library that everyone can use without being concerned about its measurements affecting their site.</p>\n\n<p>Recently, we performed and shared an audit of Boomerang's performance, to help communicate the \"cost of doing business\" of including Boomerang on a page while it takes its measurements.  In doing the audit, we found several areas of code that we wanted to improve and have been making continuous improvements ever since.  We've taken ideas and contributions from the OSS community, and have built a Performance Lab that helps \"lock in\" our improvements by continuously measuring the metrics that are important to us.</p>\n\n<p>We'll discuss how we performed the audit, some of the improvements we've made, how we're testing and validating our changes, and the real-time telemetry we capture on our library to ensure we're having as little of an impact as possible on the sites we're included on.</p>",
    "persons": [
      "Nic Jansma"
    ]
  },
  {
    "start": 1577893200000,
    "duration": 35,
    "room": "H.1309 (Van Rijn)",
    "title": "Metrics and models for Web performance evaluation",
    "subtitle": "or, How to measure SpeedIndex from raw encrypted packets, and why it matters",
    "track": "Web Performance",
    "abstract": "<p>The World Wide Web is still among the most prominent Internet applications. While the Web landscape has been in perpetual movement since the very beginning,\nthese last few years have witnessed some noteworthy proposals such as SPDY, HTTP/2 and QUIC, which profoundly reshape the application-layer protocols family.\nTo measure the impact of such changes,  going beyond the classic W3C notion of page load time, a number of Web  performance metrics has been proposed (such as\nSpeedIndex,  Above-The-Fold and variants).  At the same time, there is still a limited amount of understanding on how these metrics correlate with the user\nperception (e.g., such as user ratings, user-perceived page load time, etc.). In this talk, we discuss the state of the art in metrics and models for Web\nperformance evaluation, and their correlation with user experience through several real-world studies. Additional information, software and datasets are\navailable at https://webqoe.telecom-paristech.fr</p>",
    "description": "",
    "persons": [
      "Dario Rossi"
    ]
  },
  {
    "start": 1577895600000,
    "duration": 35,
    "room": "H.1309 (Van Rijn)",
    "title": "Hint, Hint, Font Loading Matters!",
    "subtitle": "Fonts are lovely but can slow down our loads. How can we make them faster?",
    "track": "Web Performance",
    "abstract": "<p>We all love fonts. From Google Fonts to Typekit, Hoefler&amp;Co and more, they give character and tone to our websites. The down side of fonts is that they can really slow down our loads. In this talk we'll learn about common pitfalls like critical requests depth and how to use resource hints to play tricks with latency to load web applications faster. We'll walk through a network profile to understand what's going on in the browser and how to make it faster.</p>",
    "description": "",
    "persons": [
      "Sia Karamalegos"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 35,
    "room": "H.1309 (Van Rijn)",
    "title": "The ultimate guide to HTTP resource prioritization",
    "subtitle": "How to make sure your data arrives at the browser in the optimal order",
    "track": "Web Performance",
    "abstract": "<p>Come learn about how browsers try to guess in what order web page resources should be loaded and how servers use that information to often (accidentally) make your web page slower instead.\nWe look at what resource prioritization is, how it's often implemented terribly in modern HTTP/2 stacks and how we're trying to fix it in QUIC and HTTP/3.\nWe use clear visualizations and images to help explain the nuances in this complex topic and also muse a bit on whether prioritization actually has that large an impact on web performance.</p>",
    "description": "<p>HTTP/2 started the move from multiple parallel TCP connections to a single underlying pipe. QUIC and HTTP/3 continue that trend.\nWhile this reduces the connection overhead and lets congestion controllers do their work, it also means we no longer send data in a truly parallel fashion.\nAs such, we need to be careful about how exactly we send our resource data, as some files are more important than others to achieve good web performance.</p>\n\n<p>To help regulate this, HTTP/2 introduced a complex prioritization mechanism. Browsers use complex heuristics to try and estimate the importance of a resource and, with various success, communicate their preferences to the servers.\nIt has however become clear that this scheme does not work well in practice. Between server implementation bugs, questionable browser choices and bufferbloat in caches and network setups, HTTP/2 prioritization is sometimes more a liability than a useful feature.</p>\n\n<p>For this reason, this feature is being completely reworked in HTTP/3 over QUIC. However, there a whole new can of worms is opened.\nOne of QUIC's main features for improving performance over TCP is that it removes \"head of line blocking\": if one resource suffers packet loss, other can still make progress.\nThat is... if there are other resources in progress! What performs well for lossy links turns out to be exactly what to prevent for high speed connections.</p>\n\n<p>Along the way, we also discuss existing options for web developers to impact the browser's heuristics and server behaviour (such as resource hints (e.g., preload) and the upcoming priority hints).</p>\n\n<p>Finally, we question about how we got in this terrible state of things to begin with: if people made so many mistakes implementing HTTP/2 prioritization, why didn't anyone really notice until 3 years later?\nCould it be its impact on web performance is actually limited? Or have we just not seen its full potential yet?</p>\n\n<p>We make this complex topic approachable with plenty of visualizations and animations.\nThe content is mainly based on our own research (and papers) and that of others in the web community, such as Patrick Meenan and Andy Davies.</p>",
    "persons": [
      "Robin Marx"
    ]
  },
  {
    "start": 1577900400000,
    "duration": 35,
    "room": "H.1309 (Van Rijn)",
    "title": "Shipping a performance API on Chromium",
    "subtitle": "Experiences from shipping the Element Timing API",
    "track": "Web Performance",
    "abstract": "<p>Adding new web performance APIs to the web is a complex process. In this talk, I'll go over the steps we went through to ship the Element Timing API in Chromium, which enables measuring rendering timing of image and text content. You'll learn about the process to ship an API exposing performance information to web developers. There were many steps involved in the process: engaging with developers and other browser vendors, brainstorming, privacy and security reviews, Origin Trials, posting an Intent, and addressing questions and ideas after the API has shipped.</p>",
    "description": "",
    "persons": [
      "Nicolás Peña Moreno"
    ]
  },
  {
    "start": 1577902800000,
    "duration": 35,
    "room": "H.1309 (Van Rijn)",
    "title": "The journey of building OpenSpeedMonitor",
    "subtitle": "Learnings from unexpectedly finding ourselves developing a FLOSS project",
    "track": "Web Performance",
    "abstract": "<p>Keeping track caring about web performance is hard with constantly changing standards, improving browsers, frameworks and devices.\nIt gets even harder when you develop a tool meeting these changing requirements.\nEight years ago, as an IT service provider, we were faced with the task of permanently monitoring the performance of one of the largest e-commerce platforms. After the initial use of WebPagetest, we quickly needed to develop our own features.\nWhat started as minor extensions became a separate project over time.\nIn this talk, we would like to take you on the journey we have taken developing OpenSpeedMonitor. You will hear about some unexpected challenges, what we learned the hard way and why we would have failed years ago, if we didn't decide to develop FLOSS.</p>",
    "description": "",
    "persons": [
      "Stefan Burnicki",
      "Nils Kuhn"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 20,
    "room": "H.2213",
    "title": "Designing and Producing Open Source Hardware with FOSS/OSHW tools",
    "subtitle": "We will show you how easy is now to design and setup your own production of Open Source Hardware with only FOSS/OSHW tools",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>We have possibility to setup small electronic assembly/production \"factory\" at our house for less than EUR 1000.\nI will try to explain every step from the design to final product:</p>",
    "description": "<p>We live in exciting times. It was never so easy to design and produce electronic devices like today.\nThis really unleash people's creativity.\nThe Open Source Hardware movement helps people to study, modify, improve and share designs and knowledge.\nToday we have FOSS CAD tools like KiCad to design our electronic boards.\nThere are multiply choices to manufacture PCBs even in small quantity.\nThere are lot of places to source components at low cost.\nWe have possibility to setup small electronic assembly/production \"factory\" at our house for less than EUR 1000.\nI will try to explain every step from the design to final product:</p>\n\n<ol>\n<li>How to design your product with KiCad</li>\n<li>How to generate files for production</li>\n<li>Where to order your PCBs</li>\n<li>Where to source the components for the assembly</li>\n<li>How to setup small \"factory\" at home at budget</li>\n<li>How to certify your OSHW project at OSHWA.org</li>\n</ol>\n\n\n<p>and will demonstrate Do-It-Yourself oven, solder paste printer, manual pick and place tools which could be used for production.</p>",
    "persons": [
      "Tsvetan Usunov"
    ]
  },
  {
    "start": 1577876100000,
    "duration": 20,
    "room": "H.2213",
    "title": "LibrePCB Status Update",
    "subtitle": "The progress of LibrePCB within the last two years",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>An overview about what's new in LibrePCB since the last presentation at FOSDEM 2018, and a short live demonstration to see LibrePCB in action.</p>",
    "description": "",
    "persons": [
      "Urban Bruhin"
    ]
  },
  {
    "start": 1577877600000,
    "duration": 30,
    "room": "H.2213",
    "title": "Open-source design ecosystems around FreeCAD",
    "subtitle": "",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>A walk through the different ways in which people from different areas and backgrounds use a same application (FreeCAD), and the impact this has on their workflows, and even on FreeCAD development</p>",
    "description": "<p>The FreeCAD project gathers a community of developers and users coming from a very large array of specialties: Makers, mechanical engineers, civil engineers, electronics engineers, architects, opticians, graphic designers, etc. All these people using the same software is a unique opportunity to explore and build cross-discipline workflows, and have people coming from one field learn unusual ways from other fields. This constant interchange of paradigms also influences FreeCAD development itself, and connects it to other fields and applications too, to create larger ecosystems. In this talk, we will show some examples of how this happens in different areas.</p>",
    "persons": [
      "Yorik van Havre",
      "Brad Collette"
    ]
  },
  {
    "start": 1577879700000,
    "duration": 20,
    "room": "H.2213",
    "title": "ngspice open source circuit simulator",
    "subtitle": "dev update and electrothermal simulation",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>An update of the development activities will be presented leading to ngspice-32. Its interface to KiCad has been extended, PSPICE device model compatibility and OpAmp convergence are improved, several bugs have been fixed.</p>\n\n<p>The VBIC bipolar model and the VDMOS power MOS model now incorporate the self heating effect.</p>\n\n<p>This will lead to the second part of the talk: ngspice may be very well used to simulate thermal device behavior. Heat generation, transport and temperatures are translated into electrical signals. Thus we simulate two circuits: The electrical circuit with its power losses, and the thermal circuit withany resulting device heating, its feedback on the electrical behavior, and the external cooling measures that need to be provided. Some ciruit examples will be given.</p>",
    "description": "",
    "persons": [
      "Holger Vogt"
    ]
  },
  {
    "start": 1577881200000,
    "duration": 20,
    "room": "H.2213",
    "title": "Towards CadQuery 2.0",
    "subtitle": "",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>CadQuery (CQ) [1] is a Python library for building of parametric 3D models. The overarching\ndesign goal is to be extremely fluent and as close as possible to the design intent. CQ is based on\nthe open source CAD kernel from OpenCascade and therefor offers industry standard B-Rep\nmodeling capabilities and allows exporting to lossless formats such as STEP as well as lossy ones\nsuch as STL. Originally it used Python bindings based on FreeCAD [2] but recently we switched to\nPythonOCC [3] to be more flexible and have full access to the underlying CAD kernel capabilities.\nIn the talk I will summarize the current status of the CQ project, show some interesting\nusage examples and discuss newly implemented features. Furthermore I will elaborate on the future\nplans of the core development team and touch on some of the challenges of maintaining a project\nsuch as CQ. I will also present a fairly new addition to the CQ ecosystem – CQ-editor [3]. It is a\nPython/PyQt5 based lightweight cross-platform GUI editor that allows to quickly develop and\npreview CQ 3D models. It also offers graphical debugging and CQ stack introspection capabilities\nwhich dramatically lowers the entry barrier for trying out and using CQ.</p>\n\n<p>References</p>\n\n<p>[1] https://github.com/CadQuery/cadquery\n[2] https://www.freecadweb.org\n[3] https://github.com/tpaviot/pythonocc-core\n[4] https://github.com/CadQuery/CQ-editor</p>",
    "description": "",
    "persons": [
      "Adam Urbanczyk"
    ]
  },
  {
    "start": 1577882700000,
    "duration": 30,
    "room": "H.2213",
    "title": "KiCad: Back to the Future",
    "subtitle": "KiCad and it's role in the growing open hardware movement",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>I will talk about KiCad's role in the Open Hardware design movement and how it is remarkably similar to the early days of the Free, Libre, Open Source Software (FLOSS) movement and what it means for the future of Open Hardware.</p>",
    "description": "",
    "persons": [
      "Wayne Stambaugh"
    ]
  },
  {
    "start": 1577884800000,
    "duration": 20,
    "room": "H.2213",
    "title": "Pocket Science Lab from Development to Production",
    "subtitle": "",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>In this talk we will cover the development path of the Pocket Science Lab (PSLab) board from version one in 2014 to today and outline how we use tools like KiCad to bring the device to large scale production. We will also share some major issues that we solved to get the device manufacturing ready and challenges that lie ahead of us like ensuring thorough device testing at production.</p>",
    "description": "<p>In this talk we will cover the development path of the Pocket Science Lab (PSLab) board from version one in 2014 to today and outline how we use tools like KiCad to bring the device to large scale production. We will also share some major issues that we solved to get the device manufacturing ready and challenges that lie ahead of us like ensuring thorough device testing at production. The goal of Pocket Science Lab is to create an Open Source hardware device (open on all layers) and software applications that can be used for experiments. The tiny pocket lab provides an array of instruments for doing science and engineering experiments. It provides functions of numerous measurement tools including an oscilloscope, a waveform generator, a frequency counter, a programmable voltage, current source and even a component to control robots with up to four servos.</p>",
    "persons": [
      "Mario Behling"
    ]
  },
  {
    "start": 1577886300000,
    "duration": 20,
    "room": "H.2213",
    "title": "Designing functional objects with functional objects",
    "subtitle": "OpenSCAD: Past, present and/or future",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>Reflecting on OpenSCAD's 10 years of history and what we've learned and discovered along the way. Discussion on opportunities and potential avenues forward, and some stories from the trenches.</p>",
    "description": "",
    "persons": [
      "Marius Kintel"
    ]
  },
  {
    "start": 1577887800000,
    "duration": 10,
    "room": "H.2213",
    "title": "Leveraging Open Source Designs",
    "subtitle": "Creating a component search engine for reference designs used in practice",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>Incorporating new components into PCBs is a difficult task that often requires reading multiple datasheets and creating prototypes to get it right. The funny thing is: every engineer needs to re-read reference designs! Even though there are tens of thousands of designs with new components documented and available on Github. The reason: it is almost impossible to find a relevant project. The solution? Instead of using Github search, which only retrieves files by filename, our approach creates a local database that takes the search results from Github, and then parses the used components inside the PCB designs to index them. That way, you can easily search a component and get the most relevant designs as a reference.</p>\n\n<p>This talk will give an overview of the software that was created, discusses the difficulties that were overcome and the potential for improvement in future work.</p>\n\n<p>Currently 300 of an estimated 30000 KiCad-projects on GitHub have been indexed as a proof-of-concept. We expect the data to be completed at the end of December. The project was kindly supported by AISLER with a server instance. The prototype of the search engine can be accessed at https://search-dev.aisler.net The release candidate should be ready at the end of December with the full dataset.</p>",
    "description": "",
    "persons": [
      "Lasse Mönch"
    ]
  },
  {
    "start": 1577888700000,
    "duration": 20,
    "room": "H.2213",
    "title": "Fritzing - the past, the present and the future",
    "subtitle": "Restarting with 1100 technical issues, and a few legal ones",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>Making electronics accessible to the broad public was mainly made possible by Arduino, Raspberry PI and last but not least Fritzing. Back in 2009, it was a pain to get from a loose wiring on a breadboard to a PCB. Fritzing came up first with a unique breadboard view and a simple to use PCB layout. Fast forward 10 years to Fosdem 2019, Fritzing was in a major crisis. Despite well over 200.000 users, thousands of downloads per day and an enthusiastic community, development had stalled. It has now been rebooted, and the project is back to gaining momentum. So what has happened between last year and this year?\nThis talk will give a rough introduction to Fritzing and its ecosystem, including how we overcame the problems, learned from our mistakes and how we plan to keep improving Fritzing in the future.</p>",
    "description": "",
    "persons": [
      "Kjell Morgenstern"
    ]
  },
  {
    "start": 1577890200000,
    "duration": 20,
    "room": "H.2213",
    "title": "Sparselizard: a general purpose multiphysics FEM library",
    "subtitle": "Mechanics, fluids, electricity, magnetics, EM and more",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>This presentation describes sparselizard: a fast, general, robust and user-friendly finite element c++ library with high potential for low-maintenance integration to open-source simulation tools. It is demonstrated with a large range of validated examples that the library has the ability to simulate heavily nonlinear multiphysics problems involving at least mechanic, fluid, electric, magnetic and electromagnetic physics. Its robustness, speed and user-friendliness are also demonstrated.</p>",
    "description": "",
    "persons": [
      "Alexandre Halbach"
    ]
  },
  {
    "start": 1577891700000,
    "duration": 30,
    "room": "H.2213",
    "title": "Open CASCADE Technology - an introduction and overview",
    "subtitle": "",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>Open Cascade Technology is a framework for B-Rep modeling. The presentation highlights key features available in the toolkits.\nThe following topics are covered:\n- What is OCCT?\n- License\n- OCCT in numbers\n- History\n- Modeling data\n- Modeling algorithms\n- Visualization\n- Data exchange</p>",
    "description": "",
    "persons": [
      "Alexander Malyshev"
    ]
  },
  {
    "start": 1577893800000,
    "duration": 20,
    "room": "H.2213",
    "title": "News from gEDA/gaf",
    "subtitle": "including an introduction to gschem",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>This talk gives an overview of the recent gEDA/gaf development and the new features in gEDA/gaf 1.10.0.  It is followed by an introduction to gschem and how to use it to create symbols and schematics.</p>",
    "description": "",
    "persons": [
      "Roland Lutz"
    ]
  },
  {
    "start": 1577895300000,
    "duration": 20,
    "room": "H.2213",
    "title": "Gmsh",
    "subtitle": "",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>Gmsh (http://gmsh.info) is an open source finite element mesh generator with built-in pre- and post-processing facilities. Under continuous development for the last two decades, it has become the de facto standard for open source finite element mesh generation, with a large user community in both academia and industry. In this talk I will present an overview of Gmsh, and highlight recent developments including the support for constructive solid geometry, new robust and parallel meshing algorithms, flexible solver integration and a new multi-language Application Programming Interface in C++, C, Python and Julia.</p>",
    "description": "",
    "persons": [
      "Christophe Geuzaine"
    ]
  },
  {
    "start": 1577896800000,
    "duration": 20,
    "room": "H.2213",
    "title": "AXIOM - open source cinema camera",
    "subtitle": "Project Introduction and current state of development",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>The presentation will give a brief overview of the projects history &amp; lessons learned during the course of developing a high tech camera device as community project. We also want to demo and explain the produced hardware, enclosures and sample footage then look at the challenges still ahead. Last 10 minutes reserved for Q&amp;A</p>",
    "description": "",
    "persons": [
      "Sebastian Pichelhofer"
    ]
  },
  {
    "start": 1577898300000,
    "duration": 20,
    "room": "H.2213",
    "title": "Horizon EDA - Version 1.0",
    "subtitle": "",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>This talk covers my motivation for starting a new EDA package in 2016 and the main ideas behind horizon as well as what has changed in the last year. I'll also go into my short- and long-term plans for the project.</p>",
    "description": "<p>Horizon EDA is a from-scratch EDA package with focus on useful parts management, rule-driven design and good usability. It has already proven its suitability for medium-complexity projects in the board design for my master thesis and in various hobby projects.</p>\n\n<p>This talk covers my motivation for starting a new EDA package in 2016 and the main ideas behind horizon as well as what has changed in the last year. I'll also go into my short- and long-term plans for the project.</p>",
    "persons": [
      "Lukas Kramer"
    ]
  },
  {
    "start": 1577899800000,
    "duration": 20,
    "room": "H.2213",
    "title": "OpenPiton: An Open-Source Framework for EDA Tool Development",
    "subtitle": "",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>As contemporary industrial ASIC designs have reached hundreds of billions transistor count, EDA tools must have the scalability to handle such large designs. However, few open-source RTL designs reflect the scale that industrial ASICs have reached. In this talk, we will present OpenPiton, a scalable, tiled manycore design that can reach as many as 65,536 cores in a single chip, and up to 500 million cores on a multi-chip design. The modularity and scalability of the OpenPiton design can enable EDA tool developers to test their tools' functionality at contemporary scales and adapt their development for future larger designs. With its many configurability options, extensive scalability, and heterogeneity, the OpenPiton platform is well placed to supercharge open-source EDA tool development and pave the way for a completely open-source ASIC synthesis and back-end flow tested using open-source designs.</p>",
    "description": "<p>Title:\nOpenPiton: An Open-Source Framework for EDA Tool Development</p>\n\n<p>Abstract:\nAs contemporary industrial ASIC designs have reached hundreds of billions transistor count, EDA tools must have the scalability to handle such large designs. However, few open-source RTL designs reflect the scale that industrial ASICs have reached. In this talk, we will present OpenPiton, a scalable, tiled manycore design that can reach as many as 65,536 cores in a single chip, and up to 500 million cores on a multi-chip design. The modularity and scalability of the OpenPiton design can enable EDA tool developers to test their tools' functionality at contemporary scales and adapt their development for future larger designs. With its many configurability options, extensive scalability, and heterogeneity, the OpenPiton platform is well placed to supercharge open-source EDA tool development and pave the way for a completely open-source ASIC synthesis and back-end flow tested using open-source designs.</p>\n\n<p>Preferred Session length:\nShort (20 minutes)</p>\n\n<p>Speaker: Prof. David Wentzlaff (Princeton University)</p>\n\n<p>Speaker bio:\nDavid Wentzlaff is an associate professor of electrical engineering at Princeton University. Wentzlaff's research has earned several awards, among them an NSF CAREER award, DARPA Young Faculty Award, AFOSR Young Investigator Prize, induction into the MICRO Hall of Fame, and the ASPLOS WACI Test-of-Time Award.  He received his M.S. and Ph.D. from MIT and received a B.S. in electrical engineering from the University of Illinois at Urbana-Champaign.  He was Lead Architect and Founder of Tilera Corporation, a multicore chip manufacturer now owned by Mellanox.  David's current research interests include how to create manycore microprocessors customized specifically for Cloud computing environments, how to design computer architectures in a post Moore’s Law world, and how to reduce the impact of computing on the environment by optimizing computer architecture for fully biodegradable substrates.  Many of the research projects created by Wentzlaff’s group have been open-sourced including the PriME simulator, OpenPiton, and PRGA.</p>\n\n<p>Link to any hardware / code / slides for the talk:\nhttps://parallel.princeton.edu/openpiton/\nhttps://github.com/PrincetonUniversity/openpiton</p>",
    "persons": [
      "David Wentzlaff"
    ]
  },
  {
    "start": 1577901300000,
    "duration": 20,
    "room": "H.2213",
    "title": "Designing Hardware, Journey from Novice to Not Bad",
    "subtitle": "Reflections from the OpenElectronicsLab",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>The three main contributors to the OpenElectronicsLab projects started out as relative novices. The wealth of online resources and some trial-and-error opens the doors to the world of hardware design.</p>\n\n<p>This will reflect on what lowered the barriers, insights gained, what needed to be done to handle things which turned out to be harder than expected, and to encourage hesitant novices to get started designing their own hardware.</p>",
    "description": "",
    "persons": [
      "Eric Herman",
      "Kendrick Shaw",
      "Stephanie Medlock"
    ]
  },
  {
    "start": 1577902800000,
    "duration": 30,
    "room": "H.2213",
    "title": "Finite element modeling with the deal.II software library",
    "subtitle": "",
    "track": "Open Source Computer Aided Modeling and Design",
    "abstract": "<p>The finite element method has been the method of choice to simulate the deformation of solids as well as the flow of many kinds of fluids for nearly 70 years now. In the case of solids, it provides a general framework to describe how a body reacts to external stimuli by modeling how deformation affects the internally stored energy. While most software that implements the method used to be homegrown for a particular purpose, the 2000s have seen the emergence of large and professionally developed. open source software libraries that provide a broad range of functionality that makes the implementation of such codes straightforward. I will give an overview of one of these libraries, deal.II, and how and where it is used.</p>",
    "description": "",
    "persons": [
      "Wolfgang Bangerth"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 10,
    "room": "H.2214",
    "title": "Welcome to the MySQL, MariaDB & Friends Devroom 2020",
    "subtitle": "Community Welcome",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>Welcome to the FOSDEM MySQL, MariaDB &amp; Friends Devroom 2020</p>",
    "description": "",
    "persons": [
      "Frédéric Descamps",
      "Ian Gilfillan"
    ]
  },
  {
    "start": 1577875200000,
    "duration": 20,
    "room": "H.2214",
    "title": "MySQL 8 vs MariaDB 10.4",
    "subtitle": "",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>MySQL 8 and MariaDB 10.4 are the latest Major versions for MySQL and MariaDB.  While MariaDB started by being slightly different MySQL variant,  now it has grown into very much different database platforms which grows more different from every release.</p>\n\n<p>In this presentation, we will look into the differences between MySQL and MariaDB in the core areas such as SQL features, query optimizations, replication, storage engines, and security as well as discuss unique features and capabilities MySQL 8 and MariaDB 10.4 offers compared to each other.</p>",
    "description": "",
    "persons": [
      "Peter Zaitsev"
    ]
  },
  {
    "start": 1577877000000,
    "duration": 20,
    "room": "H.2214",
    "title": "MyRocks in the Wild Wild West!",
    "subtitle": "",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>In this talk, we'll walk through RocksDB technology and look into areas where MyRocks is a good fit by comparison to other engines such as InnoDB. We will go over internals, benchmarks, and tuning of MyRocks engine. We also aim to explore the benefits of using MyRocks within the MySQL ecosystem. Attendees will be able to conclude with the latest development of tools and integration within MySQL.</p>",
    "description": "",
    "persons": [
      "Alkin Tezuysal"
    ]
  },
  {
    "start": 1577878800000,
    "duration": 20,
    "room": "H.2214",
    "title": "How Safe is Asynchronous Master-Master Setup?",
    "subtitle": "",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>It is common knowledge that built-in asynchronous master-master (active-active) replication is not safe. I remember times when the official MySQL User Reference Manual stated that such an installation is not recommended for production use. Some experts repeat this claim even now.</p>\n\n<p>While this statement is generally true, I worked with thousands of shops that successfully avoided asynchronous replication limitations in active-active setups.</p>\n\n<p>In this talk, I will show how they did it, demonstrate situations when asynchronous master-master replication is the best possible high availability option and beats such solutions as Galera or InnoDB Clusters. I will also cover common mistakes, leading to disasters.</p>",
    "description": "",
    "persons": [
      "Sveta Smirnova"
    ]
  },
  {
    "start": 1577880600000,
    "duration": 20,
    "room": "H.2214",
    "title": "The consequences of sync_binlog != 1.",
    "subtitle": "",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>Have you ever needed to get some additional write throughput from MySQL ?  If yes, you probably found that setting sync_binlog to 0 (and trx_commit to 2) gives you an extra performance boost.  As all such easy optimisation, it comes at a cost.  This talk explains how this tuning works, presents its consequences and makes recommendations to avoid them.  This will bring us to the details of how MySQL commits transactions and how those are replicated to slaves.  Come to this talk to learn how to get the benefit of this tuning the right way and to learn some replication internals.</p>",
    "description": "",
    "persons": [
      "Jean-François Gagné"
    ]
  },
  {
    "start": 1577882400000,
    "duration": 20,
    "room": "H.2214",
    "title": "Overview of encryption features",
    "subtitle": "",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>MariaDB/MySQL/Percona Server provide some features in this space, but currently there isn't one product that covers all the needs (at least not available as FOSS).\nThis talk will provide an overview of Data-at-Rest-Encryption features in MySQL, MariaDB and Percona Server for MySQL, their availability across versions, and status (experimental/GA).</p>",
    "description": "",
    "persons": [
      "Hrvoje Matijakovic"
    ]
  },
  {
    "start": 1577884200000,
    "duration": 20,
    "room": "H.2214",
    "title": "Whats new in ProxySQL 2.0?",
    "subtitle": "Exploring the latest features in ProxySQL 2.0",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>ProxySQL, the high performance, high availability, protocol-aware proxy for MySQL is now GA in version 2.0. This version introduces several new features, like causal reads using GTID, better support for AWS Aurora, native support for Galera Cluster, LDAP authentication and SSL for client connections.</p>\n\n<p>This session provides an overview of the most important new features.</p>",
    "description": "<p>Slide agenda:</p>\n\n<ul>\n<li>Supported OSes &amp; Packaging</li>\n<li>Query Cache Tunables</li>\n<li>GTID Causal Reads</li>\n<li>Native Galera Support</li>\n<li>Amazon Aurora Features</li>\n<li>LDAP Integration</li>\n<li>SSL, Audit Log &amp; Security</li>\n<li>JSON Support</li>\n<li>Performance Enhancements</li>\n<li>MySQL 8</li>\n</ul>",
    "persons": [
      "Nick Vyzas"
    ]
  },
  {
    "start": 1577886000000,
    "duration": 20,
    "room": "H.2214",
    "title": "SELinux fun with MySQL and friends",
    "subtitle": "",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>SELinux (Security Enhanced Linux) provides enhanced security mechanism for more advanced access control and auditing. It allows your application software and your system users to only access the resources it's been preconfigured to allow. Of course when you want to move your data- or log files to a non-standard location these policies will stop MySQL from starting.</p>\n\n<p>The easy way out is obviously to set SELinux to disabled or permissive. But someone once said: \"Every time you disable SELinux a kitten dies\". We'll show you a few ways how you can find out if it actually is SELinux that is blocking you and how to update the policies to properly keep you system secured.</p>",
    "description": "",
    "persons": [
      "Matthias C",
      "Ivan Groenewold"
    ]
  },
  {
    "start": 1577887800000,
    "duration": 20,
    "room": "H.2214",
    "title": "Running MySQL in Kubernetes in real life",
    "subtitle": "",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>Running databases in Kubernetes has come a long way.</p>\n\n<p>Focusing on MySQL, we will explore the challenges and issues of running production databases in Kubernetes. We'll look at the opportunities and benefits of running in Kubernetes too.\nWhile rolling out a database is easy enough, things can get interesting when production tasks are undertaken.\nHow do you achieve scaling – whether that's scaling up or down? How do you know that your latest backup will restore safely?\nWe will also take a look at an open source solution for monitoring your database deployments, adding support for Kubernetes as a robust production environment.</p>",
    "description": "<p>Focusing on MySQL, we will explore the challenges and issues of running production databases in Kubernetes. We'll look at the opportunities and benefits of running in Kubernetes too.\nWhile rolling out a database is easy enough, things can get interesting when production tasks are undertaken.\nHow do you achieve scaling – whether that's scaling up or down? How do you know that your latest backup will restore safely?\nWe will also take a look at an open source solution for monitoring your database deployments, adding support for Kubernetes as a robust production environment.</p>\n\n<p>Outline:\n- Introduction\n- Installing MySQL in Kubernetes\n- Scaling up, scaling down\n- Backup, restore, verification\n- An open source monitoring solution\n- What could possibly go wrong?</p>\n\n<p>Takeaways:\nThis presentation should encourage the audience to embrace the possibilities of running production databases on Kubernetes, and will help attendees understand the \"do's and dont's\" of such a deployment.</p>",
    "persons": [
      "Sami Ahlroos"
    ]
  },
  {
    "start": 1577889600000,
    "duration": 20,
    "room": "H.2214",
    "title": "ALTER TABLE improvements in MariaDB Server",
    "subtitle": "Optimized or instantaneous schema changes, including ADD/DROP COLUMN",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>ALTER TABLE in MySQL used to copy the table contents row by row. We can do much better; in the best case, allow instantaneous schema changes, even for nontrivial changes, such as ADD COLUMN…AFTER and DROP COLUMN. This talk describes how ALTER TABLE has been improved over the years for the InnoDB storage engine in MySQL 5.1, 5.5, 5.6, 5.7, and MariaDB Server 10.2, 10.3, 10.4, 10.5, mostly by the presenter.</p>",
    "description": "<p>The talk enumerates different classes of ALTER TABLE operations: (1) ones not involving other than metadata, (2) operations that can be performed instantly by introducing a backward-compatible data file format change and 'faking' the operation (ADD or DROP COLUMN), (3) operations that can avoid rebuilding a table, and (4) operations that must rebuild the table, and variants of (3) and (4) that allow concurrent modifications to the table. We also show how ALTER TABLE can be executed concurrently on multiple nodes in statement-based replication. Finally, we show the theoretical limits of what kind of ALTER TABLE operations can be supported without rebuilding the table, by introducing an optional validation step and on-demand conversion of records in previous schema versions of the table.</p>",
    "persons": [
      "Marko Mäkelä"
    ]
  },
  {
    "start": 1577891400000,
    "duration": 20,
    "room": "H.2214",
    "title": "Rewinding time with System Versioned Tables",
    "subtitle": "",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>Imagine, you're given a time machine. A fairly limited one, it cannot transport you anywhere. Still, it can show you the past, what your tables looked like at any given point in time. This is exactly what the SQL:2011 standard and MariaDB 10.3+ are giving you. System versioned tables allow you to rewind time and see their content as it was in the past — all using normal SELECT statements. This talk will show how to create system versioned tables, how to use them and how not to kill the performance when doing that. It will present various new applications and use cases that became possible now. Having a time machine, what will you use it for?</p>",
    "description": "<p>Imagine, you're given a time machine. A fairly limited one, it cannot transport you anywhere. Still, it can show you the past, what your tables looked like at any given point in time. This is exactly what the SQL:2011 standard and MariaDB 10.3+ are giving you. System versioned tables allow you to rewind time and see their content as it was in the past — all using normal SELECT statements. This talk will show how to create system versioned tables, how to use them and how not to kill the performance when doing that. It will present numerous different applications and use cases that became possible now. Having a time machine, what will you use it for?</p>",
    "persons": [
      "Sergei Golubchik"
    ]
  },
  {
    "start": 1577893200000,
    "duration": 20,
    "room": "H.2214",
    "title": "Knocking down the barriers of ORDER BY LIMIT queries with MariaDB 10.5",
    "subtitle": "",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>The talk will start with a recap of how MariaDB(or MySQL) handles the\nORDER BY LIMIT optimization and examples demonstrating why the current\noptimizer is not good enough.</p>\n\n<p>Further, the talk will describe how the optimizer in MariaDB 10.5 mostly\nsolves the issue, the remaining unresolved issues and how DBAs can tackle them.</p>",
    "description": "<p>FULL DESCRIPTION:</p>\n\n<p>For the first part of the talk, I will discuss the possible strategies by\nwhich ORDER BY LIMIT optimization is handled in MariaDB (or MySQL)</p>\n\n<p>The strategies are:\n1) Using an ordered index (ref, range or index scan)\n2) Using filesort on the first non-const table\n3) Using filesort on the temporary table, that stores the output of the join</p>\n\n<p>Then I will discuss how the current MariaDB/MySQL optimizer makes the choice between the strategies and show\nthe situations where it will never get a good query plan</p>\n\n<p>For the second part of the talk, I will describe how a new cost-based\noptimization in MariaDB 10.5 solves the above issue.\nThe talk will contain details about how the costs were taken into account\nduring the optimization phase. Further, with the help of examples\nI would demonstrate how the execution differs for this new optimization\nand how this leads to improved performance for ORDER BY LIMIT queries.</p>",
    "persons": [
      "Varun Gupta"
    ]
  },
  {
    "start": 1577895000000,
    "duration": 20,
    "room": "H.2214",
    "title": "CPU performance analysis for MySQL using Hot/Cold Flame Graph",
    "subtitle": "",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>Come to see some real-life examples of how you can do CPU profiling with perf and eBPF/BCC, to create FlameGraphs and ColdGraphs visualizations of the on-CPU/off-CPU time spent by the database. Based on these visualizations and reading the database source code (this is why we love Open Source!) you can quickly gain insight about what's burning CPU (FlameGraphs) and what's causing CPU to wait (ColdGraphs), and with this knowledge you will be several steps closer to answering \"what's consuming all that CPU time\".</p>",
    "description": "",
    "persons": [
      "Vinicius Grippa"
    ]
  },
  {
    "start": 1577896800000,
    "duration": 20,
    "room": "H.2214",
    "title": "Hash Join in MySQL 8",
    "subtitle": "",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>JOIN is one of the most common operation in a database system, and for a long time, the only algorithm for executing a join in MySQL has been variations of the nested loop algorithm. But starting from MySQL 8.0.18, it is now possible to execute joins using hash join. This presentation will walk you through how we were able to implement hash join using our new iterator executor, how hash join in MySQL works, when it is used, and everything else that is worth knowing about hash join in MySQL.</p>",
    "description": "",
    "persons": [
      "Erik Frøseth"
    ]
  },
  {
    "start": 1577898600000,
    "duration": 20,
    "room": "H.2214",
    "title": "Comparing Hash Join solution, the good, the bad and the worse.",
    "subtitle": "",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>MySQL 8.0.18 comes (finally) with a long waited and desired hash-join implementation.\nThat was already present in other pseudo MySQL distributions like MariaDb.\nBut, what is has-join, how it works, what problems it solves, when and how to use it.\nLast but not least are all the different implementations doing the same things, or are they acting and performing differently.\nWe are going to perform a short journey in hash-join implementations and answer all these questions.</p>",
    "description": "",
    "persons": [
      "Marco Tusa (the Grinch)"
    ]
  },
  {
    "start": 1577900400000,
    "duration": 20,
    "room": "H.2214",
    "title": "MySQL 8.0: Secure your MySQL Replication Deployment",
    "subtitle": "",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>Data protection is an extensive and hot topic. Making sure that\nwhoever accesses your data has identity well established and is\nauthorized can be a complex and hard task. Moreover, nowadays\ndata tends to move around quickly between different instances of the\nsame service, different services and different data consumers.  This\noften implies that data traverses different administrative domains. It\nis key that MySQL handles, stores and replicates data complying\nwith the security requirements that business and regulations demand.</p>\n\n<p>This session showcases the new developments in MySQL 8.0 that tighten\nrelated replication security setups, and reduce the attack surface of\nthe different replication topologies. We will talk about secure\ninter-server communication, encryption of replication data at rest\nand the new features that make the replication applier run under a\nspecific security context. Come and learn about security related\nreplication features in MySQL 8.0.</p>",
    "description": "",
    "persons": [
      "Pedro Figueiredo"
    ]
  },
  {
    "start": 1577902200000,
    "duration": 20,
    "room": "H.2214",
    "title": "Automating schema migration flow with GitHub Actions, skeema & gh-ost",
    "subtitle": "And end-to-end schema migration automation, from design to production, at GitHub",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>Schema migration is more than running an ALTER TABLE. It is about designing, reviewing, approving, queuing, scheduling, executing, auditing, controlling and versioning the changes.</p>\n\n<p>At GitHub we run multiple migrations per day, and much of this flow used to be manual, taking a significant toll from the databases team. In this session we illustrate how we automated away migration using free and open source solutions, and based on trusted development flow.</p>",
    "description": "<p>Schema migration is more than running an ALTER TABLE. It is about designing, reviewing, approving, queuing, scheduling, executing, auditing, controlling and versioning the changes.</p>\n\n<p>At GitHub we run multiple migrations per day, and much of this flow used to be manual, taking a significant toll from the databases team. In this session we illustrate how we automated away migration using free and open source solutions, and based on trusted development flow.</p>\n\n<p>We highlight the use of the skeema tool, with GitHub Actions, git flow and gh-ost.</p>",
    "persons": [
      "Shlomi Noach"
    ]
  },
  {
    "start": 1577904000000,
    "duration": 20,
    "room": "H.2214",
    "title": "20 mins to write a MySQL Shell Plugin",
    "subtitle": "Extend the MySQL Shell with a plugin created from scratch",
    "track": "MySQL, MariaDB and Friends",
    "abstract": "<p>MySQL Shell is a new client for MySQL. It comes with multiple functionalities like the adminAPI commands to setup and operate a MySQL InnoDB Cluster but also check for upgrades, import JSON, parallel import and more... It also allows you to communicate with the MySQL Server in SQL, Python or Javascript !\nDuring this session we will write a plugin from scratch to extend the Shell using the MySQL Shell Plugin Framework. The code will be written live in Python.</p>",
    "description": "",
    "persons": [
      "Frédéric Descamps"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 60,
    "room": "H.3242",
    "title": "Apache Camel BoF",
    "subtitle": "Meeting of the Apache Camel community",
    "track": "BOFs (Track B - in H.3242)",
    "abstract": "<p>Apache Camel is a free software integration framework from the Apache Software Foundation. This meetup is for anyone wishing to meet and discuss Apache Camel development, share experiences and meet in meat space other folk in the Apache Camel community.</p>",
    "description": "<p>Apache Camel has been around and its used quite widely for in all sorts of software integration projects. Camel version 3 was recently released and the community has kicked of several sub-projects: Camel K - a serverless, cloud native integration on top of Kubernetes, Camel Quarkus - low memory, fast startup support utilizing Quarkus, and Camel Kafka Connector - for running Camel inside of Kafka. Let's meet and discuss these initiatives or just talk and exchange ideas around Camel. Anyone wishing to share can present on any topic relating to Apache Camel is welcome to do so, though please be mindful of the time - we have one hour in this session and would like that everyone would have the chance to participate.</p>",
    "persons": [
      "Zoran Regvart"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 60,
    "room": "J.1.106",
    "title": "TinyGo",
    "subtitle": "TinyGo on microcontrollers and WebAssembly",
    "track": "BOFs (Track A - in J.1.106)",
    "abstract": "",
    "description": "",
    "persons": [
      "Ron Evans"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 60,
    "room": "J.1.106",
    "title": "Sourcehut & aerc meetup",
    "subtitle": "Email enthusiasts meet up to enthuse about email",
    "track": "BOFs (Track A - in J.1.106)",
    "abstract": "<p>Members of the closely linked <a href=\"https://sourcehut.org\">Sourcehut</a> and <a href=\"https://aerc-mail.org\">aerc</a> communities meet up to put faces to names and discuss the present and future of both projects, and to collect stickers.</p>",
    "description": "<p><a href=\"https://sourcehut.org\">Sourcehut</a> is a free/libre project hosting platform with sophisticated git hosting, mailing lists, continuous integration, and more. We'll be discussing the remaining tasks for the alpha, planning and seeking feedback for the beta, and showing off cool stuff added in the past year.</p>\n\n<p><a href=\"https://aerc-mail.org\">aerc</a> is a FOSS email client for your terminal designed especially for software developers which integrates nicely into the mailing list workflow endorsed by Sourcehut. Early in its development, you'll have a chance to discuss what you'd like to see in your dream email client and speculate wildly on the lofty goals it aims to achieve.</p>",
    "persons": [
      "Drew DeVault"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 90,
    "room": "J.1.106",
    "title": "Replicant Meetup",
    "subtitle": "Meeting for the Replicant community",
    "track": "BOFs (Track A - in J.1.106)",
    "abstract": "<p>Replicant is a fully free Android distribution running on several devices, a free software mobile operating system putting the emphasis on freedom and privacy/security.</p>\n\n<p>This meeting is for everyone interested in the Replicant project (users, developers, devices vendors, etc.). Among other things, we will present the ongoing efforts on Replicant 9 and discuss how we should move forward. Everyone's point of view is welcomed.</p>",
    "description": "<p>Here's a non-exhaustive list of the topics that will be up for debate:</p>\n\n<ul>\n<li>Replicant 9 status on the i9300/i9305: bootloader, modem, LCD, audio, graphics.</li>\n<li>Future targets: PinePhone, Librem5.</li>\n<li>WebView dependency on non-free Chromium.</li>\n<li>Android build system.</li>\n<li>Replicant infrastructure: servers, test benches, build machines.</li>\n<li>Long term project sustainability.</li>\n<li>Supported mainline kernel phones, what kernel versions brought what support and what is upcomming in the next kernel versions.</li>\n</ul>",
    "persons": [
      "dllud"
    ]
  },
  {
    "start": 1577899800000,
    "duration": 90,
    "room": "J.1.106",
    "title": "Creating Sustainable Public Sector Open Source Communities",
    "subtitle": "",
    "track": "BOFs (Track A - in J.1.106)",
    "abstract": "<p>The Open Source Observatory (OSOR) of the European Commission is an online collection that provides its community with an information observatory, community building activities, as well as assistance and support services. On behalf of OSOR, we propose to make a presentation on the currently ongoing study towards a guidelines document for creating sustainable open source communities in the public sector.\nIn this context, OSOR is producing guidelines for creating sustainable open source communities within the public sector. The purpose of the guidelines is to act as a practical tool that can be used by public sector officials interested in establishing open source communities or by members of such communities. The production of the guidelines is a multi-step process, involving desk research, primary data collection, development of four case studies and conduction of interviews with key stakeholders.\nAfter presenting the objectives and approach to produce the guidelines, the OSOR representatives will present the preliminary findings related to the guidelines, including the key success factors associated with healthy communities. The audience will be also invited to further brainstorm in groups the key success factors of sustainable OSS communities as well as to identify the key components that our guidelines should contain.</p>",
    "description": "<p>The production of the guidelines is a multi-step process, involving both desk research and primary data collection. More specifically, our team has conducted an in-depth literature review, followed by a questionnaire targeting OSS communities in the public sector, which will be running between January and February 2020. Following the analysis of data collected from the questionnaire, our team will develop four case studies illustrating successful and failed OSS initiatives in the public sector. Interviews with key case study stakeholders are to be conducted within each case study.\nThe key objective of our workshop at FOSDEM is not only to present our preliminary findings to the audience but to also obtain their views on our findings and future guidelines. We want to ensure that OSOR puts forward truly community driven guidelines.\nDuring the workshop, we will recount the findings so far which are the data of the literature review and survey. More specifically, we will present the key success factors and their components associated with sustainable OSS communities as well as some interesting failed and successful public sector OSS initiatives. The audience will be then invited to further brainstorm in groups the key success factors of sustainable OSS communities as well as to identify the key components that our guidelines should contain.\nWe will also invite the audience to contribute to our ongoing survey and invite them to get in touch with the OSOR community for further ideas on our guidelines and case studies.</p>\n\n<p>The target group of OSOR is wide-ranging within the open source community. It includes policy makers, IT managers, IT developers, researchers and students, and OSS advocates and enthusiasts in general. OSOR invites members of the audience who are interested in the development of OSS within public administrations, community managers and members, developers and organisations who would like to learn more about the project of measuring the health and sustainability of an open source project. The audience is also invited to follow the work of OSOR and the European Commission on OSS more closely. OSOR promotes collaboration, sharing and digital development through various services provided to its community.</p>",
    "persons": [
      "OSOR team"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 25,
    "room": "AW1.120",
    "title": "How a CMS system and linked data can make a distributed knowledge base",
    "subtitle": "Implementing the vision of the web of data",
    "track": "Collaborative Information and Content Management Applications",
    "abstract": "",
    "description": "",
    "persons": [
      "Sander Van Dooren"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 25,
    "room": "AW1.120",
    "title": "From 0 to Intranet in 20 minutes with XWiki",
    "subtitle": "",
    "track": "Collaborative Information and Content Management Applications",
    "abstract": "<p>Sharing knowledge in a team of people working together (company, association, study group or any other kind of project) is key for its long-term success. Even if this is not properly identified from the beginning as a main concern, setting up tools and processes that allow knowledge to be accumulated and organized correctly ends up being on the TODO list at some point, and may come with the wish to have done it earlier. Thus, the best solution is one that is quick enough to setup from the very beginning and versatile enough to be able to incrementally develop into a proper solid fortress of knowledge.</p>",
    "description": "<p>In this presentation I will use the XWiki platform to incrementally build a collaborative intranet from scratch and will try to address some frequent needs of knowledge sharing in a team, using already-made add-ons or new tools (list not exhaustive):\n* Blog, for unidirectional communication,\n* Meeting notes, for spoken knowledge not to be lost,\n* File Manager for just dropping files to share,\n* Task manager for lightweight ticketing,\n* Holiday requests and Recruitment applications for team management,\n* Unstructured free content sharing, using standard wiki pages and page templates,\n* Structured content tailored to fit your exact specific needs,\n* Multi-wiki separation for teams isolation,\n* Powerful search in all this,\n* Authentication add-ons to plug your own users management,\n* Easy look &amp; feel setup, to brand it as your own.</p>",
    "persons": [
      "Anca Luca"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 25,
    "room": "AW1.120",
    "title": "ONLYOFFICE: How to securely collaborate on documents within content management applications",
    "subtitle": "",
    "track": "Collaborative Information and Content Management Applications",
    "abstract": "<p>ONLYOFFICE is an open-source HTML5-based office suite for editing and collaborating on text documents, spreadsheets, and presentations online. Here, maximizing format compatibility, establishing browser-agnostic content display and optimizing real-time data transfer in co-authoring are the core principles in building applicable editing software.</p>\n\n<p>End user-side demand indicates that deeper integration in productivity solutions and content management applications is one of the main directions for ONLYOFFICE. We would like to share our experience in building connectors that allow users to edit and co-author their documents securely right within a platform they use.</p>\n\n<p>Presentation milestones:</p>\n\n<ul>\n<li>Review of the technical basis of ONLYOFFICE editors (HTML5 Canvas, JavaScript, etc.);</li>\n<li>Open API that allows creating third-party connectors;</li>\n<li>Integration with content management platforms such as XWiki, Nextcloud, ownCloud (including connectors/integration apps development);</li>\n<li>Security measures (JSON Web Token, limited cache lifetime, etc.)</li>\n<li>Connecting desktop and mobile environments to content management platforms;</li>\n<li>ONLYOFFICE integration roadmap.</li>\n</ul>",
    "description": "",
    "persons": [
      "Alex Mikheev"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 25,
    "room": "AW1.120",
    "title": "A dozen more things you didn't know Nextcloud could do",
    "subtitle": "And a little of what you DID know",
    "track": "Collaborative Information and Content Management Applications",
    "abstract": "<p>With Nextcloud you can sync, share and collaborate on data, but you don't need to put your photos, calendars or chat logs on an American server. Nope, Nextcloud is self-hosted and 100% open source! Thanks to hundreds of apps, Nextcloud can do a lot and in this talk, I will highlight some cool things.</p>",
    "description": "<p>Consider this a follow-up from my talk about 200 things Nextcloud can do last year! An update on what's new and some cool new stuff. What, what is <code>Nextcloud</code>? Let's see. A private cloud is one way to put it, though that's a contradiction of course. It is a way to share your data, sync your files, communicate and collaborate with others - without giving your data to GAFAM! Keep it on your own server, or something close (like a local hosting provider or data center). Nextcloud is a PHP app that does all that, and more! Easy to use, secure (really) and fully open source of course.</p>",
    "persons": [
      "Jos Poortvliet"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 25,
    "room": "AW1.120",
    "title": "Bringing Collabora Online to your web app",
    "subtitle": "Its easy to add rich document collaboration to your web app",
    "track": "Collaborative Information and Content Management Applications",
    "abstract": "<p>The Collabora Online code-base can bring the power of LibreOffice into\nan iframe inside your web app. Come and hear how this works, how to integrate\nsecure, collaborative document editing with your software, and about all the\nlatest greatest work going on there.</p>",
    "description": "<p>Collabora Online uses a WOPI-like protocol, and it is rather simple to integrate.\nCome hear about the total of three REST methods you need for a simple\nintegration, as well as the wealth of options to control how collaboration works.</p>\n\n<p>Hear about some of our integrations into Nextcloud, Kolab,\nMattermost, Moodle, ownCloud, and many more.</p>\n\n<p>See the internals of Collabora Online, and how you can get involved with\nbuilding, debugging, and developing it, and checkout some of the new features\nfor Mobile and PC browser that we've been working on to make life better\nfor our users.</p>",
    "persons": [
      "Michael Meeks"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 25,
    "room": "AW1.120",
    "title": "More than one tool for collaborating on writing the Tiki CMS",
    "subtitle": "This talk reviews the many collaboration tools used by the Tiki community for writing knowledge management and collaboration software",
    "track": "Collaborative Information and Content Management Applications",
    "abstract": "<p>The <em>Tiki Wiki CMS Groupware</em> software community obviously uses Tiki itself for collaboration and knowledge management.\nYet, many other software tools or infrastructures are used. I will review and explain how and why.</p>",
    "description": "<p><em>Tiki Wiki CMS Groupware</em> was initially released in 2002 and is still very much alive with a vibrant community.\nWe obviously want to use Tiki itself for collaboration and knowledge management as much as possible, but the real situation is, we use a lot more tools.\nWe leverage the fact that Tiki is part of wikisuite by using the other wikisuite software, but these still don't cover everything we use.\nThe reasons vary from old habits to convenience or improved efficiency and they are different for each collaboration tool.\nI will review them and explain why we use them and how useful they are to us. Also, I will mention our plans for the future.</p>",
    "persons": [
      "Jean-Marc Libs"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 25,
    "room": "AW1.120",
    "title": "Wikibase Ecosystem",
    "subtitle": "taking Wikidata further",
    "track": "Collaborative Information and Content Management Applications",
    "abstract": "<p>Wikidata, Wikimedia's knowledge base, has been very successful since its inception 7 years ago. Wikidata's general purpose data about the world is powering everything from Wikipedia to your digital personal assistant. Its linked, machine readable data is collected and maintained by a community of over 20000 people. But not all data should and can be in Wikidata. Instead we are taking the software powering Wikidata, Wikibase, to new places. We empower communities and institutions all around the world who want to collect structured, machine-readable data about a topic area of their choice to run their own Wikibase. These Wikibase instances are then connected to form a thriving ecosystem. In this talk we'll go over what Wikibase is, where it's coming from and what it is enabling right now.</p>",
    "description": "",
    "persons": [
      "Lydia Pintscher"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 25,
    "room": "AW1.120",
    "title": "Decentralized collaborative applications",
    "subtitle": "Peer-to-peer collaboration, search & discovery",
    "track": "Collaborative Information and Content Management Applications",
    "abstract": "<p>A data-centric, offline-first approach to decentralized collaborative application development focusing on data ownership and privacy.</p>",
    "description": "<p>Exploring replicated mergeable data structure stores as building blocks of decentralized applications that enable asynchronous collaboration and offline search in combination with peer-to-peer gossip-based protocols that provide pub/sub, dissemination, and recommendation services both over the internet as well as on local and mobile proximity networks, thereby forming interest-based networks that facilitate discovery of personally relevant content and people.</p>",
    "persons": [
      "TG x"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 60,
    "room": "AW1.120",
    "title": "The unsupervised free CAT for low resource languages",
    "subtitle": "Building a pipeline for the communities",
    "track": "Coding for Language Communities",
    "abstract": "<p>We present: 1) a full pipeline for unsupervised machine translation training (making use of monolingual corpora) for languages with low available resources; 2) a translation server making use of that unsupervised MT with an API compatible with the EU funded free Computer Aided Translation (CAT) tool MateCAT; 3) a Docker packaged version of MateCAT for ease of deployment.\nThis full translation pipeline enables a non technical user, speaking a non-FIGS language for which there is scarcity of parallel corpora, to start translating documents and software following translation industry standards.</p>",
    "description": "<p>Localization within community suffers from the fragmentation of technologies (too wide wedge between commercial Computer Aided Translation tools and free ones), available language resources (making difficult to train a Machine Translation) and lack of clear and robust pipelines to get started.\nLow resource language communities suffer the most, since MT systems require training corpora of millions of words and industry has settled to expecting the massive corpora available to FIGS (French, Italian, German, Spanish) languages.\nMoreover, the community suffers from a lack of adoption of established technologies and workflows, leading to reinventing the wheel and suboptimal efforts’ outcomes.\nToday we would like to present a connector for the implementation of an unsupervised MT (made by Artetxe et al.), that claims a BLEU of 26 on limited language resources (which is enough as a support system) integrated with MateCAT, an industry level, free, web based tool funded by EU, in order to provide a more viable alternative to resorting to Google Translate and commercial LSPs.</p>",
    "persons": [
      "Alberto Massidda"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 30,
    "room": "AW1.120",
    "title": "Lexemes in Wikidata",
    "subtitle": "structured lexicographical data for everyone",
    "track": "Coding for Language Communities",
    "abstract": "<p>Wikidata, Wikimedia's knowledge base, has been collecting general purpose data about the world for 7 years now. This data powers Wikipedia but also many applications outside Wikimedia, like your digital personal assistant. In recent years Wikidata's community has also started collecting lexicographical data in order to provide a large data set of machine-readable data about words in hundreds of languages. In this talk we will explore how Wikidata enables thousands of volunteers to describe their languages and make it available as a source of data for systems that do automated translation, text generation and more.</p>",
    "description": "",
    "persons": [
      "Lydia Pintscher"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 30,
    "room": "AW1.120",
    "title": "Nuspell: version 3 of the new spell checker",
    "subtitle": "FOSS spell checker implemented in C++17 with aid of Mozilla",
    "track": "Coding for Language Communities",
    "abstract": "<p>Nuspell version 3 is a FOSS checker that is written in pure C++17. It extensively supports character encodings, locales, compounding, affixing and complex morphology. Existing spell checking in web browsers, office suits, IDEs and other text editors can use this as a drop-in replacement. Nuspell supports 90 languages, suggestions and personal dictionaries.</p>",
    "description": "<p>In this talk we will summarize the functionality of Nuspell version 3 and provide easy to follow examples on how to use it as a command-line tool or link to the C++ library. Newly made integrations in Firefox and Enchant will be discussed. The audience will be invited to further integrate Nuspell into their software, create new language bindings, port it to other operating systems and help grow its community. This new spell checker has outgrown from an MVP to a faster and more complete spell checker.</p>",
    "persons": [
      "Sander van Geloven"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 60,
    "room": "AW1.120",
    "title": "Weblate! Localize your project the developer way: continously, flawlessly, community driven, and open-source",
    "subtitle": "Don’t bother your development process with manual work. Connect Weblate to your VCS and let the localization magic happen.",
    "track": "Coding for Language Communities",
    "abstract": "<p>The presentation will show you how to localize your project easily with little effort, open-source way. Why we started Weblate? We said no to repetitive work, no to manual work with translation files anymore. Weblate is unique for its tight integration to VCS. Set it up once and start engaging the community of translators. More languages translated means more happy users of your software. Be like openSUSE, Fedora, and many more, and speak your users' language now thanks to Weblate!</p>",
    "description": "<p>The presentation will show you how to localize your project easily with little effort, open-source way. Why we started Weblate? We said no to repetitive work, no to manual work with translation files anymore. Weblate is unique for its tight integration to VCS. Set it up once and start engaging the community of translators. More languages translated means more happy users of your software. Be like openSUSE, Fedora, and many more, and speak your users' language now thanks to Weblate! I will show you the main perks of Weblate and the setup of the project. If you have a project with open repo and you want to start translating it, take your git:// link, and we will set it up right on the spot. FOSDEM is a great time and place to found your translating community. And I am looking forward to answer all your questions!</p>",
    "persons": [
      "Václav Zbránek"
    ]
  },
  {
    "start": 1577901600000,
    "duration": 30,
    "room": "AW1.120",
    "title": "Open Edge Hardware and Software for Natural Language Translation and Understanding",
    "subtitle": "",
    "track": "Coding for Language Communities",
    "abstract": "<p>The last half decade has seen a major increase in the accuracy of deep learning methods for natural language translation and understanding. However many users still interact with these systems through proprietary models served on specialized cloud hardware. In this talk we discuss co-design efforts between researchers in natural language processing and computer architecture to develop an open-source software/hardware system for natural language translation and understanding across languages. With this system, users can access state-of-the-art models for translation, speech, and classification, and also run these models efficiently on edge device open-hardware designs.</p>\n\n<p>Our work combines two open-source development efforts, OpenNMT and FlexNLP.  The OpenNMT project is a multi-year collaborative project for creating an ecosystem for neural machine translation and neural sequence learning. Started in December 2016 by the Harvard NLP group and SYSTRAN, the project has since been used in many research and industry applications. The project includes highly configurable model architectures and training procedures, efficient model serving capabilities for use in real world applications, and extensions to tasks such as text generation, tagging, summarization, image to text, and speech to text. FlexNLP is an open-source fully retargetable hardware accelerator targeted for natural language processing. Its hardware design is targeted to key NLP computational functions such as attention mechanisms and layer normalization that are often overlooked by today’s CNN or RNN hardware accelerators. FlexNLP’s rich instruction set architecture and microarchitecture enable a diverse set of computations and operations that are paramount for end-to-end inference on state-of-the-art attention-based NLP models. Together they provide an open pipeline for both model training and edge device deployment.</p>",
    "description": "",
    "persons": [
      "Alexander Rush",
      "Thierry Tambe"
    ]
  },
  {
    "start": 1577903400000,
    "duration": 30,
    "room": "AW1.120",
    "title": "Poio Predictive Text",
    "subtitle": "Grassroots Technology for Language Diversity",
    "track": "Coding for Language Communities",
    "abstract": "<p>The Poio project develops language technologies to support communication in lesser-used and under-resourced languages on and with electronic devices. Within the Poio project we develop text input services with text prediction and transliteration for mobile devices and desktop users to allow conversation between individuals and in online communities.</p>",
    "description": "<p>In this lightning talk I will present the current architecture of the Poio Corpus, our corpus collection and data management pipeline. I will show how to add a new language to the corpus and how you can use the pipeline to build language models for the predictive text technology. Our goal is to make collaboration with language communities as smoothless as possible, so that developers, data engineers and speakers of under-ressourced language can collaborate to build grassroots language technologies. Poio started as a language revitalization project at the Interdisciplinary Centre for Social and Language Documentation in Minde/Portugal, a non-profit organization dedicated to the documentation and preservation of linguistic heritage.</p>",
    "persons": [
      "Peter Bouda"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Farwest Demo",
    "subtitle": "A website/API for a document oriented database in 20 minutes",
    "track": "Erlang, Elixir and Friends",
    "abstract": "<p>Farwest is an Erlang framework for building RESTful Web applications and APIs.</p>\n\n<p>Well written Farwest applications apply the HATEOAS principles and as a result can be interacted with using a single client. This removes entirely the need to write a separate client per API and lets servers decide how the data is best consumed by everyone.</p>\n\n<p>This demo will show how to use Farwest to write a simple API to a document oriented database.</p>",
    "description": "",
    "persons": [
      "Loïc Hoguin"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 20,
    "room": "AW1.121",
    "title": "OpenTelemetry: an XKCD 927 Success Story",
    "subtitle": "",
    "track": "Erlang, Elixir and Friends",
    "abstract": "<p>Learn how distributed tracing can revolutionize the way you troubleshoot errors and performance issues, in both monolithic and distributed micro-service architectures.</p>\n\n<p>OpenTelemetry is an industry standard for distributed tracing, merging the tech and communities of OpenCensus and OpenTracing.</p>",
    "description": "<ul>\n<li>Introduce the concepts of application tracing and the state of the ecosystem (Spans, Traces, Context Propagation, OpenTracing, OpenCensus, OpenTelemetry)</li>\n<li>Explain how this is different and complementary to Erlang’s built-in tracing tools</li>\n<li>Show what insights can be more easily gained from trace data as opposed to logs and metrics</li>\n<li>Give an overview of the tools and vendors available to start using this technology today</li>\n</ul>",
    "persons": [
      "Greg Mefford"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Debugging and tracing a production RabbitMQ node",
    "subtitle": "",
    "track": "Erlang, Elixir and Friends",
    "abstract": "<p>In this talk, we will see how to debug/trace on a running RabbitMQ node.\nErlang remote access and remote debugging are builtin features in Erlang/Elixir.<br/>\nWith these features, it is possible to see what's happening inside a BEAM node (as RabbitMQ).\nI will show also how to use \"dynamic loading\"  to add a not native code in a running beam.</p>",
    "description": "<p>Erlang remote access and remote debugging are builtin features in Erlang/Elixir.<br/>\nWith these features, it is possible to see what's happening inside a BEAM node (as RabbitMQ).\nThere are a set of tools inside the beam like etop, eprof, dbg, fprof ... that work in the same Linux way.\nIn this talk, we will see how to use some of these features on a running RabbitMQ node.\nI will show also how to use \"dynamic loading\"  to add a not native code in a running beam.</p>",
    "persons": [
      "Gabriele Santomaggio"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Keep Calm and Use Nerves",
    "subtitle": "",
    "track": "Erlang, Elixir and Friends",
    "abstract": "<p>Intended as a introduction to Nerves, the IoT platform for the BEAM, this talk is a journey through the land of library ecosystems, device drivers and pixel manipulators, in search for the holy grail: a stable and maintainable IoT device.</p>",
    "description": "<p>The Nerves project (https://nerves-project.org/) is a framework for building IoT devices with Elixir. In this talk I will explain how a Nerves project is structured and then move on to show and demonstrate one of the projects that I did with it, focussing on the development experience and the state of the Nerves ecosystem.</p>",
    "persons": [
      "Arjan Scherpenisse"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Lumen",
    "subtitle": "Elixir in the browser",
    "track": "Erlang, Elixir and Friends",
    "abstract": "<p>Lumen is an alternative compiler, interpreter and runtime to the BEAM designed for WebAssembly.  Lumen allows Erlang and Elixir to run in the browser.</p>",
    "description": "<p>The Lumen project is a reimplementation of the BEAM in Rust.  Using Rust, Lumen is able to leverage the cutting edge tools of the Rust WASM ecosystem.  Compiling Elixir and Erlang from source to LLVM IR, the Lumen compiler is able to do whole program optimizations allowing for dead-code elimination of parts of the user application, OTP, and the runtime itself.  Eliminating the dead code makes shipping OTP size-competitive with JavaScript frameworks, while retaining the benefits of thousands of concurrent processes and supervision trees.</p>",
    "persons": [
      "Luke Imhoff"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 20,
    "room": "AW1.121",
    "title": "CoffeeBeam",
    "subtitle": "A BEAM VM for Android",
    "track": "Erlang, Elixir and Friends",
    "abstract": "<p>The speaker started to experiment with running BEAM modules on Android during summer of 2019. A prototype called CoffeeBeam has been created that is capable of loading and running BEAM files on Android. The solution also contains a virtual machine that provides a lightweight Erlang runtime system. Most of the implemented functionality is independent of the source language of the BEAM files, so the platform is easily extensible to support further languages on the BEAM. During the talk, the speaker is going to present a real-life example of running a BEAM file on Android, while presenting the concepts of the implementation and sharing the story of this journey.</p>",
    "description": "<h1>CoffeeBeam: a BEAM VM for Android</h1>\n\n<h2>Goal</h2>\n\n<p>CoffeeBeam is a lightweight Erlang virtual machine that provides easy integration of BEAM files with Android applications. The current alternative solutions provide almost fully functional Erlang runtime systems in the form of Erlang shell on Android devices. However, CoffeeBeam follows a different approach, targeting easy integration of pre-compiled BEAM files into standalone Android applications. The characteristics of Android devices are in focus: they provide large amount of memory while CPU usage needs to be optimized to provide longer device lifetime. It is preferred to make the communication between Erlang and the Android application transparent to provide better user experience.</p>\n\n<h2>Use Case</h2>\n\n<p>Let's assume that you chose a language over the BEAM to implement an application logic efficiently. CoffeeBeam provides a framework to build on this logic and enable communication between the BEAM and the Android application with only minor changes to your original code. The demonstrated example is a <code>TicTacToe</code> game where the game logic is implemented in Erlang that is extended with a graphical user interface implemented as an Android activity in Java.</p>\n\n<h2>Application Architecture</h2>\n\n<h3>Android Activity</h3>\n\n<p>The <code>TicTacToe</code> game is implemented as an Android activity which is a common way of creating interactive applications. The activity contains the view for displaying textual information (game name and user instructions depending on the state of the game) and widgets (game board and new game button) for initiating user actions towards the game logic.</p>\n\n<h3>CoffeeBeam VM</h3>\n\n<p>The CoffeBeam VM provides the runtime system for the game logic. It is written in Java and included as a <code>.jar</code> library inside the Android application source code. Starting and stopping the VM is connected to the <code>onCreate()</code> and <code>onDestroy()</code> callbacks of the activity.</p>\n\n<h3>Game logic</h3>\n\n<p>The flow of the game and the computer player's intelligence is implemented as an Erlang module (approximately 250 lines of code) and the compiled <code>.beam</code> file is packaged into the Android application as resource.</p>\n\n<h2>Communication</h2>\n\n<p>The <code>BeamClient</code> class provides interface for starting and stopping the VM, and manages communication between the VM and the Android application through function calls and callback functions. The default behavior can be redefined by extending the <code>BeamClient</code> class. The forms of communication are described in detail below.</p>\n\n<h3>Function call in the VM</h3>\n\n<p>User actions in the Android application are translated into function calls in the VM using the <code>apply(String module, String function, ErlList args)</code> method of the <code>BeamClient</code> class. The function call implies creating a new process in the Erlang VM and applying <code>module:function</code> with the list of <code>args</code>. The <code>TicTacToe</code> game logic provides the following functions:</p>\n\n<ul>\n<li>Start the game process: <code>start()</code>. The game process is spawned that initializes the board for a new game.</li>\n<li>Start a new game: <code>new_game(GamePid)</code>. The game board is cleared and a new game starts in the game process identified by <code>GamePid</code>.</li>\n<li>The player selects a field: <code>put(GamePid, X, Y)</code>. The player marks the <code>(X,Y)</code> field of the game board with an <code>X</code> sign.</li>\n</ul>\n\n\n<h3>Handle function result in Android</h3>\n\n<p>When the Erlang function is executed in the VM, the result of the function initiates a callback in the <code>BeamClient</code> as <code>handleResult(ErlTerm result)</code>. In the <code>TicTacToe</code> example, the process identifier of the game process is returned as the result of the <code>tictactoe:start()</code> function. The returned value can be used to send Erlang messages to the game process during the game.</p>\n\n<h3>Handle function callback in Android</h3>\n\n<p>Each call in the form of <code>beamclient:function(arg)</code> in the Erlang modules results in a <code>BeamClient</code> callback <code>handleCall(String function, ErlTerm arg)</code>. Each game event invokes a <code>beamclient:update({Event, Board})</code> function call that is translated into <code>handleCall</code> callback in the Android application.</p>\n\n<h2>Summary and contribution</h2>\n\n<p>CoffeeBeam executes BEAM files in a lightweight VM that can be packaged into the Android application. The above <code>TicTacToe</code> example showed how to include the Erlang game logic in the Android application that provides the graphical user interface. The game flow runs in a separate process in the CoffeBeam VM, and the communication with Android is done through <code>BeamClient</code> function calls and callbacks.</p>\n\n<p>The CoffeeBeam VM is open source and available for further development to extend the VM functionality or implement customizations for other languages running on the BEAM. The source code with documented interface is available at: https://github.com/vikger/coffeebeam.</p>",
    "persons": [
      "Viktor Gergely"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Going Meta with Elixir's Macros",
    "subtitle": "Running at compile-time and compiling at runtime",
    "track": "Erlang, Elixir and Friends",
    "abstract": "<p>Compilation and execution are as different as night and day. Or are they? By blurring the lines, Elixir (and the BEAM VM) enable some very powerful and useful meta-programming techniques.</p>\n\n<p>In this presentation, Marten will talk about running and generating code at compile-time, Elixir's hygienic macros, and how to compile and hot-reload altered or extra code, while your program is running!\nBesides explaining these concepts, their usefulness will be motivated using various practical real-world examples.</p>",
    "description": "",
    "persons": [
      "Wiebe-Marten Wijnja"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Processes & Grains",
    "subtitle": "A Journey in Orleans",
    "track": "Erlang, Elixir and Friends",
    "abstract": "",
    "description": "<p>A popular way to manage long-running state in Erlang and Elixir programs is by using processes; this model is well-understood and well-supported, but remains firmly rooted within known orthodoxy. Within this session, I shall demonstrate application of the Orleans model to existing Erlang and Elixir applications, review existing work done by the community, and compare this way of programming against other models. For maximum enjoyment, the audience is advised to possess working knowledge of Erlang and Elixir. Some background knowledge in Web applications will be helpful as well. The session will be presented with live demo in Elixir.</p>",
    "persons": [
      "Evadne Wu"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Designing a performant and scalable graph processing python package",
    "subtitle": "",
    "track": "Graph Systems and Algorithms",
    "abstract": "<p>Python has proven to be a popular choice for data scientists in\nthe domain of graph analytics. The multitude of freely available\nframeworks and python packages allow to develop applications\nquickly through ease of expressibility and reuse of code.\nWith petabytes of data generated everyday and an ever evolving\nlandscape of hardware solutions, we observe a graph processing\nframework should expose the following characteristics: ease of\nuse, scalability, interoperability across data formats, and\nportability across hardware vendors.\nWhile existing python packages have been helping to drive\napplication development, our assessment is that none of the\npackages address all the aforementioned challenges.\nWe propose a community led, open source effort, to design and\nbuild  a graph processing python library to specifically address\nthese challenges.</p>",
    "description": "",
    "persons": [
      "Vincent Cave"
    ]
  },
  {
    "start": 1577892300000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Graffiti",
    "subtitle": "A historical, distributed graph engine",
    "track": "Graph Systems and Algorithms",
    "abstract": "<p>Graffiti is the graph engine of Skydive - an open source networking analysis tool. Graffiti was created from scratch to provide the features required by Skydive : distributed, replicated, store the whole history of the graph, allow subcribing to events on the graph using WebSocket and visualization.</p>",
    "description": "<p>Skydive (https://skydive.network) is an open source analysis tool. It collects information about an infrastructure topology - such as network interfaces, Linux bridges, network namespaces, containers, virtual machines, ... and store them into a graph database called Graffiti (https://github.com/skydive-project/skydive/tree/master/graffiti)</p>\n\n<p>The graph is :</p>\n\n<pre><code>- distributed : some agents only have a portion of the graph\n- replicated : for high availability and load distribution\n- historical : every change on the graph is archived, allowing retrieval of the graph at any point in time or getting all the revisions of a set of nodes and edges during a period of time\n</code></pre>\n\n<p>A custom implementation of the Gremlin language is used to query the graph, with some additional steps to specify the time context of the query for instance.</p>\n\n<p>In addition to the core engine, a WebSocket based user interface - based on D3JS - is available to visualize and interact with the graph.</p>\n\n<p>This presentation will showcase a demo of Graffiti and try to advocate its use in your own project.</p>",
    "persons": [
      "Sylvain Baubeau"
    ]
  },
  {
    "start": 1577893800000,
    "duration": 20,
    "room": "AW1.121",
    "title": "The Neo4j Graph Algorithms Library: An Overview",
    "subtitle": "",
    "track": "Graph Systems and Algorithms",
    "abstract": "<p>Graph algorithms play an increasingly important role in real-world applications. The Neo4j Graph Algorithms library contains a set of ~50 graph algorithms covering a lot of different problem domains. In our talk, we’ll present the architecture of the library and demonstrate the different execution phases using a real world example.</p>",
    "description": "<p>Graph algorithms play an increasingly important role in real-world applications. Use-cases that we see in the wild are related to fraud detection, fraud detection, retail recommendation and identifying influencers for marketing campaigns. The Neo4j Graph Algorithms library contains a set of ~50 graph algorithms covering the above-mentioned problem domains.</p>\n\n<p>Running a graph algorithm in Neo4j involves three essential steps: loading the graph from the database in an optimized in-memory format, executing the algorithm, and streaming or writing of results. For the user, these steps are hidden behind single procedure calls, integrated in the Cypher query language.</p>\n\n<p>In our talk, we will explain and demonstrate what happens in the system when a user calls an algorithm procedure. This involves scanning Neo4j store files, constructing our in-memory graph representation and executing an algorithm via our Java Graph API.</p>\n\n<p>Attendees will learn how to setup and use the Neo4j Graph Algorithms Library. Furthermore, they will get a good understanding of how the library works internally and how to tune it for specific use-cases.</p>",
    "persons": [
      "Paul Horn"
    ]
  },
  {
    "start": 1577895300000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Gunrock: High-Performance Graph Analytics for the GPU",
    "subtitle": "",
    "track": "Graph Systems and Algorithms",
    "abstract": "<p>Gunrock is a CUDA library for graph-processing designed specifically for the GPU. It uses a high-level, bulk-synchronous, data-centric abstraction focused on operations on vertex or edge frontiers. Gunrock achieves a balance between performance and expressiveness by coupling high-performance GPU computing primitives and optimization strategies, particularly in the area of fine-grained load balancing, with a high-level programming model that allows programmers to quickly develop new graph primitives that scale from one to many GPUs on a node with small code size and minimal GPU programming knowledge.</p>\n\n<p>Features of Gunrock include:\n- Best of class performance among GPU graph analytics frameworks\n- A large number of graph applications (28 at last count)\n- A data-centric programming model targeted at GPUs that offers advantages over other programming models\n- A programming model that scales to multiple GPUs with high performance while still using the same code as a single-GPU primitive</p>\n\n<p>Gunrock began in 2013 as a project under DARPA's XDATA program and is currently the performance reference in DARPA's HIVE program. Gunrock is also in development as a component in NVIDIA's RAPIDS platform for data analytics. The Gunrock team actively develops and improves Gunrock under an Apache 2.0 license.</p>\n\n<p>https://gunrock.github.io/</p>",
    "description": "",
    "persons": [
      "Muhammad Osama"
    ]
  },
  {
    "start": 1577896800000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Hardware-Software Co-Design for Efficient Graph Application Computations on Emerging Architectures",
    "subtitle": "",
    "track": "Graph Systems and Algorithms",
    "abstract": "<p>Graph databases and applications have attracted much attention in the past few years due to the efficiency with which they can represent big data, connecting different layers of data structures and allowing analysis while preserving contextual relationships.\nThis has resulted in a fast-growing community that has been developing various database and algorithmic innovations in this area, many of which will be gathering together in this conference. We joined this field as computer architecture researchers and are currently building a complete hardware-software design, called DECADES, that aims to accelerate the execution of these algorithms.</p>\n\n<p>From a computer architecture point of view, applications involving dense matrix operations such as neural networks have garnered much attention for their acceleration through specialized hardware such as GPUs and TPUs, while graph applications remain difficult to improve even with modern specialized accelerator designs. The reason for this is the characteristic pointer-based data structures of graph applications and the resulting irregular memory accesses performed by many of these workloads. Such irregular memory accesses result in memory latency bottlenecks that dominate the total execution time. In this talk, as part of the DECADES infrastructure, we present an elegant hardware-software codesign solution, named FAST-LLAMAs, to overcome these memory-bottlenecks, and thus, accelerate graph and sparse applications in an energy efficient way.</p>",
    "description": "<p>Graph databases and applications have attracted much attention in the past few years due to the efficiency with which they can represent big data, connecting different layers of data structures and allowing analysis while preserving contextual relationships.\nThis has resulted in a fast-growing community that has been developing various database and algorithmic innovations in this area, many of which will be gathering together in this conference. We joined this field as computer architecture researchers and are currently building a complete hardware-software design, called DECADES, that aims to accelerate the execution of these algorithms.</p>\n\n<p>From a computer architecture point of view, applications involving dense matrix operations such as neural networks have garnered much attention for their acceleration through specialized hardware such as GPUs and TPUs, while graph applications remain difficult to improve even with modern specialized accelerator designs. The reason for this is the characteristic pointer-based data structures of graph applications and the resulting irregular memory accesses performed by many of these workloads. Such irregular memory accesses result in memory latency bottlenecks that dominate the total execution time. In this talk, as part of the DECADES infrastructure, we present an elegant hardware-software codesign solution, named FAST-LLAMAs, to overcome these memory-bottlenecks, and thus, accelerate graph and sparse applications in an energy efficient way.</p>\n\n<p>We propose a 40 minute talk which includes a rigorous characterization of the problem, and an in-depth analysis of our software-hardware co-design solution, FAST LLAMAs. We will present results based on a simulated model of our system which show significant performance improvements (up to 8x), as well as energy improvements (up to 20x) on a set of fundamental graph algorithms and important real-world datasets. Our system is completely open-source, and includes a compiler and cycle-accurate simulator. Our proposed system is compatible and easily extendable to many of the open-source graph analytic and database frameworks and we are excited to engage with the open-source community of this increasingly important domain.</p>\n\n<p>The work is part of a large collaboration from three academic groups: Margaret Martonosi (PI Princeton), David Wentzlaff (PI Princeton), Luca Carloni (PI Columbia) with students/researchers: Juan L. Aragón (U. of Murcia, Spain), Jonathan Balkind, Ting-Jung Chang, Fei Gao, Davide Giri, Paul J. Jackson, Aninda Manocha, Opeoluwa Matthews, Tyler Sorensen, Esin Türeci, Georgios Tziantzioulis, and Marcelo Orenes Vera. In addition to the submission author, portions of the talk may be offered by others in the collaboration.</p>",
    "persons": [
      "Margaret Martonosi"
    ]
  },
  {
    "start": 1577898300000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Programmable Unified Memory Architecture (PUMA)",
    "subtitle": "",
    "track": "Graph Systems and Algorithms",
    "abstract": "<p>Large scale graph analytics is essential to analyze relationships in big data sets. Thereto, the DARPA HIVE program targets a leap in power efficient graph analytics. In response to this program, Intel proposes the Programmable Unified Memory Architecture (PUMA). Based on graph workload analysis insights, PUMA consists of many multi-threaded cores, fine-grained memory and network accesses, a globally shared address space and powerful offload engines. In this talk, we will describe the PUMA architecture, both in terms of hardware and the software ecosystem. We will provide initial simulation based performance estimations, showing that for graph analysis applications, a PUMA node will outperform a conventional compute node by one to two orders of magnitude. Additionally, PUMA will continue to scale across multiple nodes, which is a challenge in conventional multinode setups.</p>",
    "description": "",
    "persons": [
      "Stijn Eyerman"
    ]
  },
  {
    "start": 1577899800000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Cypher enhancements for sharded and federated graph databases",
    "subtitle": "",
    "track": "Graph Systems and Algorithms",
    "abstract": "<p>In this talk we will introduce enhancements to the Cypher graph query language, enabling queries spanning multiple graphs, intended for use in sharding and federation scenarios.\nWe will also present our experience with sharding the LDBC Social Network Benchmark dataset.</p>",
    "description": "",
    "persons": [
      "Tobias Johansson",
      "Petr Janouch"
    ]
  },
  {
    "start": 1577901300000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Raphtory: Streaming analysis of distributed temporal graphs",
    "subtitle": "",
    "track": "Graph Systems and Algorithms",
    "abstract": "<p>Temporal graphs capture the development of relationships within data throughout time. This model fits naturally within a streaming architecture, where new events can be inserted directly into the graph upon arrival from a data source, being compared to related entities or historical state. However, the vast majority of graph processing systems only consider traditional graph analysis on static data, with some outliers supporting batched updating and temporal analysis across graph snapshots. This talk will cover recent work defining a temporal graph model which can be updated via event streams and investigating the challenges of distribution and graph maintenance. Some notable challenges within this include partitioning a graph built from a stream, with the additional complexity of managing trade-offs between structural locality (proximity to neighbours) and temporal locality (proximity to an entities history). Synchronising graph state across the cluster and handling out-of-order updates, without a central ground truth limiting scalability. Managing memory constraints and performing analysis in parallel with ongoing update ingestion.\nTo address these challenges, we introduce Raphtory, a system which maintains temporal graphs over a distributed set of partitions, ingesting and processing parallel updates in near real-time. Raphtory's core components consist of Graph Routers and Graph Partition Managers. Graph Routers attach to a given input stream and convert raw data into graph updates, forwarding this to the Graph Partition Manager handling the affected entity. Graph Partition Managers contain a partition of the overall graph, inserting updates into the histories of affected entities at the correct chronological position. This removes the need for centralised synchronisation, as commands may be executed in any given arrival order whilst resulting in the same history. To deal with memory constraints, Partition Managers both compress older history and set an absolute threshold for memory usage. If this threshold is met a cut-off point is established, requiring all updates prior to this time to be transferred to offline storage. Once established and ingesting the selected input, analysis on the graph is permitted via Analysis Managers. These connect to the cluster, broadcasting requests to all Partition Managers who execute the algorithm. Analysis may be completed on the live graph (most up-to-date version), any point back through its history or as a temporal query over a range of time.  Additionally, multiple Analysis Managers may operate concurrently on the graph with previously unseen algorithms compiled at run-time, thus allowing modification of ongoing analysis without re-ingesting the data.\nRaphtory is an ongoing project, but is open source and available for use now. Raphtory is fully containerised for ease of installation and deployment and much work has gone into making it simple for users to ingest their own data sources, create custom routers and perform their desired analysis.\nThe proposed talk will discuss the benefits of viewing data as a temporal graph, the current version of Raphtory and how someone could get involved with the project. We shall also touch on several areas of possible expansion at the end for discussion with those interested.</p>",
    "description": "<p>The intended audience for this talk is a mixture of data scientists and graphy engineers. It is going to be quite high level, but introducing some interesting ideas of how to view data through the lens of a temporal graph as well as novel systems solutions for distribution, maintenance and processing.</p>",
    "persons": [
      "Ben Steer"
    ]
  },
  {
    "start": 1577902800000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Temporal Graph Analytics with GRADOOP",
    "subtitle": "",
    "track": "Graph Systems and Algorithms",
    "abstract": "<p>The temporal analysis of evolving graphs is an important requirement in many domains but hardly supported in current graph database and graph processing systems. We, therefore, extended the distributed graph analytics framework Gradoop for time-related graph analysis by introducing a new temporal property graph data model. Our model supports bitemporal time dimensions for vertices and edges to represent both rollback and historical information. In addition to the data model, we introduce several time-dependent operators (e.g, Snapshot, Diff and Grouping) that natively support the natural evolution of the graph. Since this is an extension of Gradoop, the temporal operators are compatible and can be combined with the already known operators to build complex analytical tasks in a declarative way. In our talk, we will give a brief overview of the Gradoop system, the temporal property graph model and how we support the time-dependent analysis of large graphs. Based on real-world use-cases, we show the expressiveness and flexibility of our temporal operators and how they can be composed to answer complex analytical questions.</p>",
    "description": "",
    "persons": [
      "gomez"
    ]
  },
  {
    "start": 1577904000000,
    "duration": 20,
    "room": "AW1.121",
    "title": "Weaviate OSS Smart Graph",
    "subtitle": "feature updates, demo and use cases",
    "track": "Graph Systems and Algorithms",
    "abstract": "<p>Weaviate is an open-source smart graph that aims to allow anyone, anywhere, any time to create their own semantic search engines, knowledge graphs or knowledge networks. Weaviate is RESTful and GraphQL API based and built on top of a semantic vector storage mechanism called the contextionary. Because all data is stored in the vector space, Weaviate is ideal for;\n- Semantically search through the knowledge graph.\n- Automatically classify entities in the graph.\n- Create easy to use knowledge mappings.</p>\n\n<p>Because the use of formal ontologies are optional, Weaviate can be used to create a P2P knowledge network which we want to present during this conference.</p>\n\n<p>This is a follow up after the initial design was shared during last year's FOSDEM.</p>",
    "description": "<h1>Problem</h1>\n\n<p>Creating a knowledge graph can be a complex endeavor, let alone the integration of semantic search models. Bain &amp; Company research under US enterprise CTO's shows that 59% of them believe they lack the capabilities to generate meaningful business insights from their data, and 85% said it would require substantial investments to improve their data platforms.</p>\n\n<h1>Solution</h1>\n\n<p>Weaviate aims anyone to create large, enterprise-scale knowledge graphs as straight forward as possible. Weaviate's feature set allows anyone to;\n- Semantically search through the knowledge graph.\n- Automatically classify entities in the knowledge graph.\n- Create easy to use knowledge mappings.</p>\n\n<h1>Weaviate's Contextionary</h1>\n\n<p>Weavite's Contextionary is the semantic vector storage mechanism that stores data -unlike traditional storage mechanisms- based on its semantic meaning. For example, if someone stores information about a company with the name Apple, this data object would be found closely related to concepts like the iPhone.</p>\n\n<p>Because of the algorithmic use (as opposed to retraining) of the pre-trained machine learning model, Weaviate is able to learn new concepts fast and near-realtime. This allows the user to update and manipulate the knowledge graph directly.</p>\n\n<h1>Demo &amp; Use cases</h1>\n\n<p>During the session, we want to show a few -recent- use cases to demo how Weaviate can be used. The demo will include;\nquerying;\nsemantic querying;\nadding concepts;\ngoing from an ontology to a schema;\nand more.</p>\n\n<h1>Knowledge network</h1>\n\n<p>Because of Weaviate's contextionary, a formal ontology is optional (e.g., \"a company with the name Netflix\" is semantically similar to \"a business with the identifier Netflix Inc.\") this allows multiple Weaviate to connect and communicate over a peer to peer (P2P) network to exchange knowledge. Aka, the knowledge network. During the session, we want to demonstrate the first prototype of this network.</p>\n\n<h1>more information</h1>\n\n<p>more information can be found on our website: https://www.semi.technology/documentation/weaviate/current/</p>",
    "persons": [
      "Bob van Luijt"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 5,
    "room": "AW1.125",
    "title": "Welcome to the Ada DevRoom",
    "subtitle": "",
    "track": "Ada",
    "abstract": "<p>Welcome to the Ada Developer Room at FOSDEM 2020, which is organized\nby Ada-Belgium in cooperation with Ada-Europe.</p>",
    "description": "<p>Ada-Belgium and Ada-Europe are non-profit organizations set up\nto promote the use of the Ada programming language and related\ntechnology, and to disseminate knowledge and experience into academia,\nresearch and industry in Belgium and Europe, resp.  Ada-Europe has\nmember-organizations, such as Ada-Belgium, in various countries,\nand direct members in many other countries.</p>",
    "persons": [
      "Dirk Craeynest"
    ]
  },
  {
    "start": 1577874900000,
    "duration": 45,
    "room": "AW1.125",
    "title": "An Introduction to Ada for Beginning and Experienced Programmers",
    "subtitle": "",
    "track": "Ada",
    "abstract": "<p>An overview of the main features of the Ada language, with special\nemphasis on those features that make it especially attractive for\nfree software development.</p>",
    "description": "<p>Ada is a feature-rich language, but what really makes Ada stand-out is\nthat the features are nicely integrated towards serving the goals of\nsoftware engineering.  If you prefer to spend your time on designing\nelegant solutions rather than on low-level debugging, if you think\nthat software should not fail, if you like to build programs from\nreadily available components that you can trust, you should really\nconsider Ada</p>",
    "persons": [
      "Jean-Pierre Rosen"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 20,
    "room": "AW1.125",
    "title": "HAC: the Compiler which will Never Become Big",
    "subtitle": "",
    "track": "Ada",
    "abstract": "<p>In the Ada world, we are surrounded by impressive and professional\ntools that can handle large and complex projects.  Did you ever\ndream of a tiny, incomplete but compatible system to play with?\nAre you too impatient, when developing small pieces of code, for\nlong compile-bind-link-run cycles?  Are you a beginner intimidated by\nproject files and sophisticated tools?  Then HAC (the HAC Ada Compiler,\nor the Hello-world Ada Compiler) is for you.</p>",
    "description": "<p>HAC is a revival of the SmallAda project, which supported the \"Pascal\nsubset\" plus tasking.</p>",
    "persons": [
      "Gautier de Montmollin"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 50,
    "room": "AW1.125",
    "title": "Tracking Performance of a Big Application from Dev to Ops",
    "subtitle": "",
    "track": "Ada",
    "abstract": "<p>This talk describes how performance aspects of a big Air Traffic Flow\nManagement mission critical application are tracked from development\nto operations.</p>",
    "description": "<p>Tracking performance is needed when new functionality is added, to\nbalance the additional services versus the resource increase needed.\nMeasuring and tracking performance is also critical to ensure a new\nrelease can cope with the current or expected load.</p>\n\n<p>We will discuss various aspects such as which tools and techniques\nare used for performance tracking and measurements, what are the\ntraps and pitfalls encountered for these activities.  The application\nin question is using Ada, but most of the items discussed are not\nparticularly Ada related.</p>",
    "persons": [
      "Philippe Waroquiers"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 20,
    "room": "AW1.125",
    "title": "Cappulada: What we've Learned",
    "subtitle": "",
    "track": "Ada",
    "abstract": "<p>Last year I presented Cappulada, a C++ binding generator for Ada that\nintended to overcome the shortcomings of existing solutions and to\nprovide usable bindings even for complex C++ code.</p>\n\n<p>This year I want to show our conclusions on why automatic bindings\nbetween C++ and Ada are hard (if not impossible) and where existing\nsolutions (including our own) fail.</p>",
    "description": "",
    "persons": [
      "Johannes Kliemann"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 20,
    "room": "AW1.125",
    "title": "Programming ROS2 Robots with RCLAda",
    "subtitle": "",
    "track": "Ada",
    "abstract": "<p>The Robot Operating System (ROS) is one of the chief frameworks\nfor service robotics research and development.  The next iteration\nof this framework, ROS2, aims to improve critical shortcomings of\nits predecessor like deterministic memory allocation and real-time\ncharacteristics.</p>\n\n<p>RCLAda is a binding to the ROS2 framework that enables the programming\nof ROS2 nodes in pure Ada with seamless integration into the ROS2\nworkflow.</p>",
    "description": "",
    "persons": [
      "Alejandro Mosteo"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 50,
    "room": "AW1.125",
    "title": "Live Demo of Ada's Distribution Features",
    "subtitle": "",
    "track": "Ada",
    "abstract": "<p>Ada incorporates in its standard a model for distributed execution.\nIt is an abstract model that does not depend on a particular kind of\nnetwork or any other communication mean, and that preserves full typing\ncontrol across partitions.  This presentation briefly exposes the\nprinciples of Ada's distribution model, then shows the possibilities\nwith life demos across different machines and operating systems.</p>",
    "description": "",
    "persons": [
      "Jean-Pierre Rosen"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 20,
    "room": "AW1.125",
    "title": "Writing Shared Memory Parallel Programs in Ada",
    "subtitle": "Multitasked Newton's Method for Power Series",
    "track": "Ada",
    "abstract": "<p>Tasks in Ada are effective to speed up computations on multicore\nprocessors.  In writing parallel programs we determine the granularity\nof the parallelism with respect to the memory management.  We have to\ndecide on the size of each job, the mapping of the jobs to the tasks,\nand on the location of the input and output data for each job.</p>\n\n<p>A multitasked Newton's method will show the effectiveness of Ada to\nspeed up the computation of power series.  This application belongs\nto the free and open source package PHCpack, a package to solve\npolynomial systems by polynomial homotopy continuation.</p>",
    "description": "",
    "persons": [
      "Jan Verschelde"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 20,
    "room": "AW1.125",
    "title": "Spunky: a Genode Kernel in Ada/SPARK",
    "subtitle": "",
    "track": "Ada",
    "abstract": "<p>The Genode OS framework is an open-source tool kit for building highly\nsecure component-based operating systems scaling from embedded devices\nto dynamic desktop systems.  It runs on a variety of microkernels\nlike SeL4, NOVA, and Fiasco OC as well as on Linux and the Muen SK.\nBut the project also features its own microkernel named \"base-hw\"\nwritten in C++ like most of the Genode framework.</p>\n\n<p>Spunky is a pet project of mine.  Simply put it's an approach to\nre-implement the design of the \"base-hw\" kernel first in Ada and\nlater in SPARK with the ultimate goal to prove its correctness.\nIt is also an opportunity to learn how Genode can benefit from Ada\nand SPARK in general and promote the use of safety-oriented languages\nin the project.</p>",
    "description": "",
    "persons": [
      "Martin Stein"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 50,
    "room": "AW1.125",
    "title": "Alire: Ada Has a Package Manager",
    "subtitle": "",
    "track": "Ada",
    "abstract": "<p>Alire (Ada LIbrary REpository) is a package manager project for the\nAda/SPARK community.  The goal of a package manager is to facilitate\ncollaboration within the community and to lower the barrier of entry\nfor beginners.</p>",
    "description": "<p>In this talk we will present the Alire project, what it can do for\nyou and how you can contribute and give more visibility to your\nAda/SPARK projects.</p>\n\n<p>We will also provide a tutorial to show how to use Alire to create\na library and then publish it for others to use.</p>",
    "persons": [
      "Fabien Chouteau",
      "Pierre-Marie de Rodat",
      "Alejandro Mosteo"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 20,
    "room": "AW1.125",
    "title": "Protect Sensitive Data with Ada Keystore",
    "subtitle": "",
    "track": "Ada",
    "abstract": "<p>Storing passwords and secret configuration is a challenge for an\napplication.  Ada Keystore is a library that stores arbitrary content\nby encrypting them in secure keystore (AES-256, HMAC-256).</p>",
    "description": "<p>The talk presents the project and shows how to use the Ada Keystore\nlibrary to get or store secret information in a secure manner.\nThe presentation explains how the Ada features such as types, protected\ntypes, tasks, pre/post conditions have helped during the development\nof this project.</p>",
    "persons": [
      "Stephane Carrez"
    ]
  },
  {
    "start": 1577899800000,
    "duration": 20,
    "room": "AW1.125",
    "title": "EUgen: a European Project Proposal Generator",
    "subtitle": "",
    "track": "Ada",
    "abstract": "<p>Whoever wrote a research project proposal knows how much unnerving it\ncan be.  The actual project description (made of work packages, tasks,\ndeliverable items, ...) has lots of redundancies and cross-references\nthat makes its coherency as frail as a house of cards.  For example,\nif the duration of a task is changed most probably you'll need to\nupdate the effort in person-months of the task and of the including\nwork package; you must update the start date of depending tasks and\nthe deliver date of any deliverable items; most probably also the\nWP efforts and length need update too; not to mention the need of\nupdating all the summary tables (summary of efforts, deliverable,\n..) and the GANTT too.  Any small changes is likely to start a ripple\nof updates and the probability of forgetting something and getting an\nincoherent project description is large.  Given the harsh competition\nin project funding, if your project is incoherent the probability of\ngetting funded is nil.</p>\n\n<p>One day I got sick of this state of affair and I wrote my own project\ngenerator: 10k lines of Ada code that reads a non-redundant project\ndescription from a simple-format text file and produces a set of files\nready to be imported in the proposal, GANNT chart included.  The user\ncan specify dependences between different items (e.g., this deliverable\nis produced at the end of this task, this milestone is reached when\nthis deliverable is available, this task must begin after this other\ntask...) and the program automatically computes all the dates.</p>",
    "description": "<p>Both input parser and output processors are implemented using a plugin\nstructure that makes it easy to write new parsers to read different\nformats or new output processors to produce output in different\nformats.  Currently a parser for a simple ad-hoc format and an output\nprocessor that produces LaTeX files are provided; a new processor based\non the template expander <em>protypo</em> is currently being implemented.</p>\n\n<p>Did I eat my own dog food?  Well, yes, I did.  I used it to write a\nproposal (still under evaluation) and it served me well.</p>",
    "persons": [
      "Riccardo Bernardini"
    ]
  },
  {
    "start": 1577901600000,
    "duration": 20,
    "room": "AW1.125",
    "title": "On Rapid Application Development in Ada",
    "subtitle": "",
    "track": "Ada",
    "abstract": "<p>In the Ada world we typically write mission critical software that\njust has to work, but in a way one could argue that a lot more software\nis mission critical than is usually admitted.</p>\n\n<p>What does it take to actually perform rapid application development\nin any language?  Can we do it in Ada and why would we do so?</p>",
    "description": "<p>A quick look into some language features that can be [ab]used for\nenabling quick development of 'just a prototype' - which, as practice\nshows is often deployed into production, usually without proper\nquality controls and predictable outcome.</p>",
    "persons": [
      "Tomasz Maluszycki"
    ]
  },
  {
    "start": 1577903400000,
    "duration": 20,
    "room": "AW1.125",
    "title": "Ada-TOML: a TOML Parser for Ada",
    "subtitle": "",
    "track": "Ada",
    "abstract": "<p>The world of generic structured data formats is full of contenders:\nthe mighty XML, the swift JSON, the awesome YAML, ...  Alas, there\nis no silver bullet: XML is very verbose, JSON is not convenient for\nhumans to write, YAML is known to be hard to parse, and so on.</p>\n\n<p>TOML is yet another format whose goal is to be a good configuration\nlanguage: obvious semantics, convenient to write and easy to parse\nin general-purpose programming languages.</p>\n\n<p>In this talk, I'll shortly describe the TOML format and show a few\nuse cases in the real world.  I'll then present the ada-toml library\nitself: its high-level architecture and examples.</p>",
    "description": "",
    "persons": [
      "Pierre-Marie de Rodat"
    ]
  },
  {
    "start": 1577904600000,
    "duration": 10,
    "room": "AW1.125",
    "title": "Informal Discussions & Closing",
    "subtitle": "",
    "track": "Ada",
    "abstract": "",
    "description": "",
    "persons": [
      "Dirk Craeynest"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 30,
    "room": "AW1.126",
    "title": "The good and the bad sides of developing open source tools for neuroscience",
    "subtitle": "",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>The reproducibility crisis has shocked the scientific\ncommunity. Different papers describe this issue and the scientific\ncommunity has taken steps to improve on it. For example, several\ninitiatives have been founded to foster openness and standardisation\nin different scientific communities (e.g. the INCF[1] for the\nneurosciences). Journals encourage sharing of the data underlying\nthe presented results, some even make it a requirement.</p>\n\n<p>What is the role of open source solutions in this respect? Where are the problems with\nopen source projects in (neuro-)sciences?</p>\n\n<p>In this presentation I will address these questions at the example\nof the entirely open-source based workflow in our laboratory[2] and\nour efforts in developing generic solutions for storing metadata[3]\nas well as unifying data and metadata storage[4] that we take together\nwith the German Neuroinformatics Node (G-Node[5]).</p>\n\n<p>[1] https://incf.org\n[2] https://github.com/bendalab\n[3] https://github.com/g-node/python-odml\n[4] https://github.com/g-node/nix\n[5] https://g-node.org</p>",
    "description": "",
    "persons": [
      "Jan Grewe"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 30,
    "room": "AW1.126",
    "title": "Challenges and opportunities in scientific software development",
    "subtitle": "An example from the neurosciences",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>The approaches used in software development in an industry setting and a scientific environment are exhibit a number of fundamental differences. In the former industry setting modern team development tools and methods are used (version control, continuous integration, Scrum, ...) to develop software in teams with a focus on the final software product. In contrast, in the latter scientific environment a large fraction of scientific code is produced by individual scientists lacking thorough training in software development with a specific research goal in mind. Indeed, it is only in the last decades that scientific software development started to become a fully recognized part of scientific work. Still, formal training in software development is largely missing in the scientific curricula of many universities. Additionally, due to the exploratory nature of the scientific method at the frontier of knowledge, most projects require the implementation of custom code. The combination of these circumstances promotes the development of scientific code not suited for sharing and long term maintenance, limiting the reusability and reproducibility of scientific data and findings. The systematic development and adoption of open source packages by the scientific community can emend this situation. Here we present examplary open source packages from the field of neuroscience and discuss the special requirements for open source software development and services in this research area.</p>\n\n<p>Acknowledgements:\nThis project has received funding from the European Union’s Horizon 2020 Framework Programme for Research and Innovation under Specific Grant Agreement No. 785907 (Human Brain Project SGA2). Supported by the NFDI Neuroscience Initiative.</p>",
    "description": "",
    "persons": [
      "Julia Sprenger"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 30,
    "room": "AW1.126",
    "title": "NeuroFedora: Enabling Free/Open Neuroscience",
    "subtitle": "",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>NeuroFedora is an initiative to provide a ready to use Fedora-based Free/Open source software platform for neuroscience. We believe that similar to Free software; science should be free for all to use, share, modify, and study. The use of Free software also aids reproducibility, data sharing, and collaboration in the research community. By making the tools used in the scientific process more comfortable to use, NeuroFedora aims to take a step to enable this ideal.</p>",
    "description": "<p>The computer has become an indispensable resource in modern neuroscience. From the gathering of data, simulation of computational models, analysis of large amounts of information, collaboration, and communication tools for community development, software is now a necessary part of the research pipeline.</p>\n\n<p>The Neuroscience community is gradually moving to the use of Free/Open Source software (FOSS) <a href=\"Gleeson,\">1</a>; however, the software tools used in Neuroscience and research are generally complicated and sophisticated to use. Researchers that hail from a different field other than computing must spend considerable resources on setting up and managing the computing environment and the software tools. This limits the portability of the software, making the installation of software very time-consuming and sometimes tricky.</p>\n\n<p>We present NeuroFedora - A Fedora-based operating system for Neuroscientists.\nWe have leveraged the infrastructure resources of the FOSS Fedora community <a href=\"RedHat.\">2</a> to develop an operating system that includes a plethora of ready-to-use Neuroscience software.\nWe follow the standard software development and quality assurance practices set out by the Fedora community to provide an integrated platform for researchers to use.\nFurthermore, NeuroFedora is well integrated with other software such as desktop environments, text editors, and other daily use and development tools.</p>\n\n<p>A NeuroFedora lab image is now available, with over 130 neuroscience packages ready to use. With an up to date documentation at (neuro.fedoraproject.org) and about 120+ packages in the queue, we encourage more FOSS enthusiasts to join the team to help NeuroFedora better aid the open (neuro)-science and research community.</p>",
    "persons": [
      "Aniket Pradhan"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 30,
    "room": "AW1.126",
    "title": "Spotlight on Free Software Building Blocks for a Secure Health Data Infrastructure",
    "subtitle": "",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>Health Data is traditionally held and processed in large and complex mazes of hospital information systems. The market is dominated by vendors offering monolithic and proprietary software due to the critical nature of the supported processes and - in some cases - due to legal requirements. The “digital transformation”, “big data” and “artificial intelligence” are some of the hypes that demand for improved exchange of health care data in routine health care and medical research alike. Exchanging data at these scales requires open data formats and protocols, multi-stakeholder collaboration, and agile development. As an example, the de-facto messaging standard organization in medicine HL7 noticed a much more positive response from the medical research community regarding their openly available FHIR specification in comparison to the for-members-only and XML-based HL7v3 messaging standard specification.\nWhile some past (or rather: ongoing) projects on a national scale in the German health care system have tried centralized, top-down specification and development approaches, more recent infrastructure projects embrace the competitive collaboration of a decentralized, bottom-up strategy. As a result, importance and recognition of free software increase in the Medical Informatics research community. In a series of rapid spotlights, we present tools and frameworks that serve as cornerstones for the envisioned health data exchange infrastructure, including: Organization and collaboration tools; data extraction from clinical source systems, data transformation and de-identification; data management systems and long-term archival using persistent globally-unique object identifiers; federated queries across multiple independently managed clinical data integration centers.\nWe aim to encourage participants to actively add tools and frameworks within the discussion and highlight their experiences and challenges with using open systems in Medical Informatics.</p>",
    "description": "",
    "persons": [
      "Markus Suhr"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 30,
    "room": "AW1.126",
    "title": "DataLad",
    "subtitle": "Perpetual decentralized management of digital objects for collaborative open science",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>Contemporary sciences are heavily data-driven, but today's data management technologies and sharing practices fall at least a decade behind software ecosystem counterparts.\nMerely providing file access is insufficient for a simple reason: data are not static. Data often (and should!) continue to evolve; file formats can change, bugs will be fixed, new data are added, and derived data needs to be integrated.\nWhile (distributed) version control systems are a de-facto standard for open source software development, a similar level of tooling and culture is not present in the open data community.</p>\n\n<p>The lecture introduces DataLad, a software that aims to address this problem by providing a feature-rich API (command line and Python) for joint management of all digital objects of science: source code, data artifacts (as much as their derivatives), and essential utilities, such as container images of employed computational environments.\nA DataLad dataset represents a comprehensive and actionable unit that can be used privately, or be published on today's cyberinfrastructure (GitLab, GitHub, Figshare, S3, Google Drive, etc.) to facilitate large and small-scale collaborations.</p>\n\n<p>In addition to essential version control tasks, DataLad aids data discovery by supporting a plurality of evolving metadata description standards. Moreover, Datalad is able to capture data provenance information in a way that enables programmatic re-execution of computations, and as such provides a key feature for the implementation of reproducible science.\nDataLad is extensible and customizable to fine tune its functionality to specific domains (e.g., field of science or organizational requirements).</p>\n\n<p>DataLad is built on a few key principles:</p>\n\n<ol>\n<li><p><strong>DataLad only knows about two things: Datasets and files.</strong>\nA DataLad dataset is a collection of files in folders.\nAnd a file is the smallest unit any dataset can contain.\nAt its core, <strong>DataLad is a completely domain-agnostic, general-purpose tool to manage data</strong>.</p></li>\n<li><p><strong>A dataset is a Git repository</strong>.\nA dataset is a Git repository. All features of the version control system Git\nalso apply to everything managed by DataLad.</p></li>\n<li><p><strong>A DataLad dataset can take care of managing and version controlling arbitrarily large data</strong>.\nTo do this, it has an optional <em>annex</em> for (large) file content:\nThanks to this annex, DataLad can track files that are TBs in size\n(something that Git could not do, and that allows you to restore previous versions of data,\ntransform and work with it while capturing all provenance,\nor share it with whomever you want). At the same time, DataLad does all of the magic\nnecessary to get this important feature to work quietly in the background.\nThe annex is set-up automatically, and the tool <a href=\"https://git-annex.branchable.com\">git-annex</a> manages it all underneath the hood.</p></li>\n<li><p>DataLad follows the social principle to\n<strong>minimize custom procedures and data structures</strong>. DataLad will not transform\nyour files into something that only DataLad or a specialized tool can read.\nA PDF file (or any other type of\nfile) stays a PDF file (or whatever other type of file it was)\nwhether it is managed by DataLad or not. This guarantees that users will not loose\ndata or data access if DataLad would vanish from their system, or even when DataLad\nwould vanish from the face of Earth. Using DataLad thus does not require or generate\ndata structures that can only be used or read with DataLad -- DataLad does not\ntie you down, it liberates you.</p></li>\n<li><p>Furthermore, DataLad is developed for <strong>complete decentralization</strong>.\nThere is no required central server or service necessary to use DataLad. In this\nway, no central infrastructure needs to be maintained (or paid for) --\nyour own laptop is the perfect place to live for your DataLad project, as is your\ninstitutions webserver, or any other common computational infrastructure you\nmight be using.</p></li>\n<li><p>Simultaneously, though, DataLad aims to\n<strong>maximize the (re-)use of existing 3rd-party data resources and infrastructure</strong>.\nUsers <em>can</em> use existing central infrastructure should they want to.\nDataLad works with any infrastructure from GitHub to\nDropbox, Figshare, or institutional repositories,\nenabling users to harvest all of the advantages of their preferred\ninfrastructure without tying anyone down to central services.</p></li>\n</ol>",
    "description": "",
    "persons": [
      "Michael Hanke"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 30,
    "room": "AW1.126",
    "title": "Frictionless Data for Reproducible Research",
    "subtitle": "",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>Generating insight and conclusions from research data is often not a straightforward process. Data can be hard to find, archived in difficult to use formats, poorly structured and/or incomplete. These issues create “friction” and make it difficult to use, publish and share data. The Frictionless Data initiative (https://frictionlessdata.io/) at Open Knowledge Foundation (http://okfn.org) aims to reduce friction in working with data, with a goal to make it effortless to transport data among different tools and platforms for further analysis, and with an emphasis on reproducible research and open data. The Frictionless Data project is comprised of a set of specifications (https://frictionlessdata.io/specs/) for data and metadata interoperability, accompanied by a collection of open source software libraries (https://frictionlessdata.io/software/) that implement these specifications, and a range of best practices for data management. Over the past year and a half, we have been working specifically with the researcher community to prototype using Frictionless Data’s open source tools to improve researchers’ data workflows and champion reproducibility. This talk will discuss the technical ideas behind Frictionless Data for research and will also showcase recent collaborative use cases, such as how oceanographers implemented Frictionless Data tooling into their data ingest pipelines to integrate disparate data while maintaining quality metadata in an easy to use interface.</p>",
    "description": "<h2>Expected prior knowledge / intended audience</h2>\n\n<p>The audience should be familiar with the themes of researching, using data in various forms from various sources, scientific computing, and the talk is intended for those that are interested in data management, data cleaning, metadata, and using open research data.</p>\n\n<h2>Speaker bio</h2>\n\n<p>Lilly Winfree is the Product Owner of the Frictionless Data for Reproducible Research Project at Open Knowledge Foundation, where she solves researchers’ technical data management issues. She has her PhD in neuroscience, and has been active in the open data, open source, and open science communities for four years. Lilly has given numerous conference presentations and workshops over the past decade, and enjoys presenting on technical topics to technical and non-technical audiences.</p>\n\n<h2>Links to code / slides / material for the talk (optional)</h2>\n\n<p>https://github.com/frictionlessdata/\nhttp://frictionlessdata.io/software/</p>\n\n<h2>Links to previous talks by the speaker</h2>\n\n<p>Workshop presentation: http://bit.ly/FDepfl\nTalk from a previous position: https://youtu.be/4Jqu8mBXcmA</p>",
    "persons": [
      "Lilly Winfree"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 15,
    "room": "AW1.126",
    "title": "On the road to sustainable research software.",
    "subtitle": "",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>ELIXIR is an intergovernmental organization that brings together life science resources across Europe. These resources include databases, software tools, training materials, cloud storage, and supercomputers.</p>",
    "description": "<p>One of the goals of ELIXIR [1] is to coordinate these resources so that they form a single infrastructure. This infrastructure makes it easier for scientists to find and share data, exchange expertise, and agree on best practices. ELIXIR's activities are divided into the following five areas Data, Tools, Interoperability, Compute and Training known as “platforms”. The ELIXIR Tools Platform works to improve the discovery, quality and sustainability of software resources. Software Best Practices task of the Tools Platform aims to raise the quality and sustainability of research software by producing, adopting, promoting and measuring information standards and best practices applied to the software development life cycle. We have published four (4OSS) simple recommendations to encourage best practices in research software [2]  and the Top 10 metrics for life science software good practices [3].</p>\n\n<p>The 4OSS simple recommendations are as follows:\n- Develop a publicly accessible open-source code from day one.\n- Make software easy to discover by providing software metadata via a popular community registry.\n- Adopt a license and comply with the licence of third-party dependencies.\n- Have a clear and transparent contribution, governance and communication processes.</p>\n\n<p>In order to encourage researchers and developers to adopt the 4OSS recommendations and build FAIR (Findable, Accessible, Interoperable and Reusable) software, best practices group in partnership with the ELIXIR Training platform, The Carpentries [4][5], and other communities are creating a collection of training materials [6]. The next step is to adopt, promote, and recognise these information standards and best practices, by developing comprehensive guidelines for software curation, and through workshops for training researchers and developers towards the adoption of software best practices and improvement of the usability of research software tools.</p>\n\n<p>Additionally, the ELIXIR Software Best Practices WG is currently developing a Software Management Plan under the context of the necessary metrics for assessing adoption of good software development practices [7] and will subsequently develop practical guidelines to support its implementation in ELIXIR projects. We will work with the newly formed  ReSA (Research Software Alliance) to facilitate the adoption of this plan to the broader community.\nIn the past year, the Working Group has also been working on improving the tooling and practices around software citation. This work has been done in collaboration with the eLife journal, the Software Sustainability Institute, Datacite, and Software Heritage, over multiple sprint events: (i) BOSC CoFest, (ii) eLife Innovation Sprint, (iii) FORCE19 Research Software Hackathon, and (iv) BioHackathon.</p>\n\n<p>[1] “ELIXIR | A distributed infrastructure for life-science information” Internet: https://www.elixir-europe.org/, [Sep. 16, 2018]\n[2] Jiménez RC, Kuzak M, Alhamdoosh M et al. (2017) “Four simple recommendations to encourage best practices in research software” F1000Research [Online]. 6:876. https://doi.org/10.12688/f1000research.11407.1\n[3] Top 10 metrics for life science software good practices https://doi.org/10.12688/f1000research.9206.1\n[4] “carpentries.org” Internet: carpentries.org, Sep. 15, 2018 [Sep. 16, 2018]\n[5] “ELIXIR teams up with The Carpentries to boost its training programme | ELIXIR”, Internet: https://www.elixir-europe.org/news/elixir-carpentries-agreement, Aug. 17, 2018 [Sep. 16, 2018]\n[6] SoftDev4Research/4OSS-lesson https://doi.org/10.5281/zenodo.2565040\n[7] Top 10 metrics for life science software good practices https://doi.org/10.12688/f1000research.9206.1</p>",
    "persons": [
      "Mateusz Kuzak"
    ]
  },
  {
    "start": 1577886300000,
    "duration": 15,
    "room": "AW1.126",
    "title": "Stylo : a user friendly text editor for humanities scholars",
    "subtitle": "",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>As an editor for WYSIWYM text, Stylo is designed to change the entire digital editorial chain of scholarly journals the field of human sciences.</p>\n\n<p>Stylo (https://stylo.ecrituresnumeriques.ca) is designed to simplify the writing and editing of scientific articles in the humanities and social sciences. It is intended for authors and publishers engaged in high quality scientific publishing. Although the structuring of documents is fundamental for digital distribution, this aspect is currently delayed until the end of the editorial process. This task should, however, be undertaken early on in the process; it must be considered by the author himself. The philosophy behind Stylo consists in returning the task of managing the publication markup to researchers. This repositioning of tasks relating to the editorial process relies on the author’s semantic rather than graphic skills.</p>\n\n<p>This lightning talk will be the opportunity to present this tool and several publishing projects realized with Stylo.</p>",
    "description": "",
    "persons": [
      "Antoine Fauchié"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 15,
    "room": "AW1.126",
    "title": "Using Advene to accompany research in AudioVisual Digital Humanities",
    "subtitle": "",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>Advene is a video annotation platform (free software) that aims at accompanying scholars in their audiovisual analyses workflow. It promotes flexible and evolving annotation structures and interfaces in order to deal with the inherent dynamic nature of analysis. In this presentation, I will present the platform itself, and illustrate its usage through existing Digital Humanities projects that use it, from structuring videos for interview analyses to implementing a workflow for semantic annotation of movies.</p>",
    "description": "",
    "persons": [
      "Olivier Aubert"
    ]
  },
  {
    "start": 1577888100000,
    "duration": 15,
    "room": "AW1.126",
    "title": "Shrivelling world",
    "subtitle": "A Three dimensional visualisation development for representing geographical time-space",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>Representing geographical time-space is a fundamental issue in geography, addressing core questions of the discipline, i.e. where are places and what distance separate them. Yet, considering the properties of geographical time space shaped by transport means, no satisfying cartographic representation – including classical maps and plastic space approaches –  has been proposed so far.\nThe \"shriveling_world\" project aims at producing images of the global geographical time-space, using the third dimension, as in time-space relief maps. The word \"shriveling\" was introduced by Waldo Tobler in his comments of Mathis-L'Hostis time-space relief image, in order to describe the complex contraction process suggested by the model.\nThe FOSDEM presentation aims at opening the code to the scientific community, now that the application is close to a first functional version.</p>",
    "description": "",
    "persons": [
      "Nicolas Roelandt"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 30,
    "room": "AW1.126",
    "title": "Empowering social scientists with web mining tools",
    "subtitle": "Why and how to enable researchers to perform complex web mining tasks",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>Web mining, as represented mostly by the scraping &amp; crawling practices, is not a straightforward task and requires a variety of skills related to web technologies.\nHowever, web mining can be incredibly useful to social sciences since it enables researchers to tap into a formidable source of information about society.</p>\n\n<p>But researchers may not have the possibility to invest copious amount of times into learning web technologies in and out. They usually rely on engineers to collect data from the web.\nThe object of this talk is to explain how Sciences Po's <a href=\"https://medialab.sciencespo.fr/en\">médialab</a> designed &amp; developed tools to empower researchers and enable them to perform web mining tasks to answer their research questions. Here is an example of issues we will tackle during this talk:</p>\n\n<ul>\n<li>How a social sciences laboratory life can be a very fruitful context for tool R&amp;D regarding webmining</li>\n<li>How to create performant &amp; effective webmining tools that anyone can use (multithreading, parallelism, JS execution, complex spiders etc.)</li>\n<li>How to re-localize data collection: researchers should be able to conduct their own collections without being dependent on external servers or resources</li>\n<li>How to teach researchers the necessary skills: HTML, the DOM, CSS selection etc.</li>\n</ul>\n\n\n<p>Examples will be taken mainly from the <a href=\"https://github.com/medialab/minet\">minet</a> CLI tool and the <a href=\"https://medialab.github.io/artoo/\">artoo.js</a> bookmarklet.</p>\n\n<h2>Speaker</h2>\n\n<p><a href=\"https://github.com/Yomguithereal\">Guillaume Plique</a> is a research engineer working for SciencesPo's <a href=\"https://medialab.sciencespo.fr/en\">médialab</a>. He assists social sciences researchers daily with their methods and maintain a variety of FOSS tools geared toward the social sciences community and also developers.</p>",
    "description": "",
    "persons": [
      "Guillaume Plique"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 30,
    "room": "AW1.126",
    "title": "Revamping OpenRefine",
    "subtitle": "a reproducible data wrangler",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>OpenRefine is a data transformation tool popular in many communities: data journalism, semantic web, GLAMs, scientific research… In this talk I give an overview of our recent efforts to revamp this project as it approaches its 10th anniversary. We are working on exciting improvements which should help alleviate some of the most salient issues faced by our users. My intention is not to lecture the attendance about how to deal with technical debt or to grow a contributor community - I instead seek feedback and spark discussions about our choices. Let us know what you think and help us take good care of this fantastic tool!</p>",
    "description": "",
    "persons": [
      "Antonin Delpeuch"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 30,
    "room": "AW1.126",
    "title": "Pocket infrastructures to bridge reproducible research, live coding, civic hacktivism and data feminism for/from the Global South",
    "subtitle": "",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>We will showcase Grafoscopio, a flexible, extensible, self contained \"pocket infrastructure\", which simplifies infrastructure to amplify participation, so reproducible research and publishing, agile data storytelling and custom data visualization can be used in fields like investigative journalism, data feminism and civic hacktivism. We will show prototypes developed with Grafoscopio in the previously mentioned domains, the motivations behind Grafoscopio and the local community practices around it that deconstruct binary relations of power (software developer/user, data producer / consumer, software binary / source code, male/female) and approach reproducible research practices and tools from a perspective located and embodied in a particular place of the Global South in Latin America and in contrast/dialogue with Global North perspectives.</p>",
    "description": "<p>Reproducible research (and publishing) has been confined mostly to academic places. But it has a lot of potential in several other places like investigative journalism, data feminism and civic hacktivism, as we have showcased by building several prototypes, including: making the so called \"Panama Papers\" data leak story reproducible; creating domain specific visualizations for medicine information released by 16 governments; porting the Spanish Data Journalism Handbook and the Data Feminism book to our \"pocket infrastructures\" and the creation of agile and resilient tools and practices to write and publish together (see proposal links for a detailed view of such prototypes).</p>\n\n<p>To bridge reproducible research and publishing, agile data storytelling and custom data visualization, with the previously mentioned domains, we have co-designed, developed, used and extended a set of inclusive approaches and tools for/from the Global South, that we have called \"pocket infrastructures\". Our \"pocket infrastructures\" simplify infrastructure to amplify participation, and they are mostly self contained, flexible, extensible, and work well with good, low or non connectivity and run from a variety of hardware, from a USB drive, to low end computers, to servers and the \"cloud\" and anything in between and beyond.\nThis is in sharp contrast with exclusionary approaches like \"Big Data\" or others that start with big and/or overcomplex infrastructures and are getting traction in the Global North (or are being imported from there to the Global South as the \"only way\" forward regarding reproducibility).</p>\n\n<p>Grafoscopio is one of such pocket infrastructures for reproducible research and publishing, agile visualization and data storytelling and this lecture will showcase Grafoscopio, the motivations behind it, and some prototypes developed with it, and the community practices that allow the development of such projects and prototypes deconstructing also binary relations of power (software developer/user, data producer / consumer, software binary / source code, male/female).</p>",
    "persons": [
      "Offray Luna"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 30,
    "room": "AW1.126",
    "title": "Developing Open-Science Research Platforms",
    "subtitle": "Zotero, PressForward, Tropy, DHARPA",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>From Zotero to PressForward to Tropy, my research team has developed a wide range of open-science research tools over the last 13 years. I'll talk about how the availability and selection of our technology stacks has influenced humanities (and non-humanities) research methodologies, and I'll provide an update on my team's newest and most comprehensive research platform, DHARPA.</p>",
    "description": "<p>I'm flexible on the length of the talk and the format. No expected prior knowledge.</p>",
    "persons": [
      "Sean Takats"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 30,
    "room": "AW1.126",
    "title": "Developing from the field.",
    "subtitle": "Shifting design processes and roles between makers and practitioners around research tools development within an interdisciplinary research lab.",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>Software design and development within interdisciplinary research teams is a specific activity which closely associates makers and practitioners in the equipment of experimental research methods and practices. This closeness allows practitioners to tackle research endeavours’ specific requirements, such as understanding the methodological assumptions encoded within the tools. It also induces a specific relationship between “makers” and their publics of “users” : a non-commercial, situated and case-based crafting process, implying shifting roles and complex decision making. How does this peculiar context affect the design and valorization practices around open research tools and their evolution ? What are the benefits and difficulties of such settings, in terms of work organization, pedagogical approaches, and scientific methodology ? What can be shared for other contexts such as activism or journalism ? Grounding on the presentation of several case studies of research tools’ design and development elaborated at the médialab of Sciences Po, this talk will offer an account of how an interdisciplinary research environment affects and dialogs with established methods of design (“participative design”, “user experience research”), development (“agile methods”), and tool valorization and socialization.</p>",
    "description": "<p>Audrey Baneyx has a PhD in artificial intelligence from Paris 6 university. She is a research engineer at the médialab (Sciences Po, Paris) where she works at the intersection of digital methods, knowledge modelling and designing pedagogical storytellings. She is teaching digital culture and methods and, as a mediator, developing médialab tools communities of practitioners. She is co-leading a research group focusing on gender issues online.</p>\n\n<p>Robin de Mourat is research designer at the médialab laboratory (Sciences Po, Paris). He works at the intersection between academic equipment and inquiry practices, combining a background in product design, design history &amp; theory, and human-computer interactions, with diverse material and discursive experiments in the Humanities and Social Sciences. He has participated to the making of texts, things and conversations about the epistemology of design activities, interdisciplinary methodologies, and social &amp; cultural studies of scholarly practices. He has been involved for several years in the development of advanced tools for academic writing and publishing in humanities and social sciences contexts.</p>",
    "persons": [
      "Audrey Baneyx",
      "Robin De Mourat"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 30,
    "room": "AW1.126",
    "title": "Transforming scattered analyses into a documented, reproducible and shareable workflow",
    "subtitle": "",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>This presentation is a feedback from experience on helping a researcher transforming a series of scattered analyses into a documented, reproducible and shareable workflow.<br/>\nTime allocated by researchers to program / code the analyses required to answer their scientific questions is usually low compared to other tasks. As a result, multiple small experiments are developed and outputs are gathered as best as possible to be presented in a scientific paper. However, science is not only about sharing results but also sharing methods. How can we make our results reproducible when we developed multiple, usually undocumented analyses? What do we do if the program is only applicable to our computer directory architecture? This is always possible to take time to rewrite, re-arrange and document analyses at the time we want/have to share them. Here, I will take the exemple of a \"collaboration fest\" where we dissected R scripts of a researcher in ecology. We started a reproducible, documented and open-source R-package along with its website, automatically built using continuous integration: <a href=\"https://cesco-lab.github.io/Vigie-Chiro_scripts/\">https://cesco-lab.github.io/Vigie-Chiro_scripts/</a>.<br/>\nHowever, can we think, earlier in the process, a better way to use our small programming time slots by adopting a method that will save time in our future? In this aim, I will present a documentation-first method using little time while writing analyses, but saving a lot when the time has come to share your work.</p>",
    "description": "<h2>Session type (Lecture or Lightning Talk)</h2>\n\n<p>Lecture</p>\n\n<h2>Session length (20-40 min, 10 min for a lightning talk)</h2>\n\n<p>30 min</p>\n\n<h2>Expected prior knowledge / intended audience</h2>\n\n<p>No prior knowledge expected. Example will be about building documentation for R software but any developper, using any programming language may be interested in the method adopted.</p>\n\n<h2>Speaker bio</h2>\n\n<p>Sébastien Rochette has a PhD in marine ecology. After a few years has a researcher in ecology, he joined ThinkR (https://rtask.thinkr.fr), a company giving courses and consultancy around the R-software. Along with commercial activities, he is highly involved in the development of open-source R packages. He also shares his experience with the R-community through free tutorials, blog posts, online help and other conferences. https://statnmap.com/</p>\n\n<h2>Links to code / slides / material for the talk (optional)</h2>\n\n<p>I wrote a blog post in French about what I am planning to present: https://thinkr.fr/transformer-plusieurs-scripts-eparpilles-en-beau-package-r/<br/>\nThis topic is also related to another blog post: https://rtask.thinkr.fr/when-development-starts-with-documentation/</p>\n\n<h2>Links to previous talks by the speaker</h2>\n\n<p>Talks about R are in my Github repository: https://github.com/statnmap/prez/. The \"README\" lists talks that have a live recorded video.<br/>\nAs a researcher, I also gave multiple talks about marine science, modelling and other topics related to my research.</p>",
    "persons": [
      "Sébastien Rochette"
    ]
  },
  {
    "start": 1577899800000,
    "duration": 30,
    "room": "AW1.126",
    "title": "A community-driven approach towards open innovation for research communication",
    "subtitle": "",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>The advancement of web technologies has created an opportunity for developing tools for real-time collaborations, text-mining, interactive data visualisations, sharing reproducible compute environments, etc. These tools can change the ways researchers share, discover, consume and evaluate research and help promote open science and encourage responsible research behaviours.</p>\n\n<p>Through its Innovation Initiative, eLife invests heavily in software development, new product design, collaboration and outreach so that the potential for improvements in the digital communication of new research can start to be realised. In particular, we support exclusively the development of open-source tools, with extensible capabilities, that can be used, adopted and modified by any interested party and actively engage the community of open innovators.</p>\n\n<p>In this talk, we will introduce the following projects:\n* Reproducible Document Stack (RDS), an open-tool stack capturing code, data and compute environment in a live paper to improve research reproducibility (see demo <a href=\"https://elifesci.org/reproducible-example\">here</a>)\n* Fostering collaboration and innovation through hacking: <a href=\"https://sprint.elifesciences.org\">eLife Innovation Sprint</a></p>\n\n<p>We believe that openness is crucial to the future of research, and by supporting the community and promoting open-source research software, we can help build a culture towards integral, collaborative, open and reusable research. We hope to share some of our visions and learnings, and invite feedback and contributions from the wider open-source community on the next steps forward.</p>",
    "description": "<h1>Speaker bio</h1>\n\n<p><em>Emmy Tsang</em> is the Innovation Community Manager at eLife, a non-profit organisation with the mission to accelerate research communication and discovery. She is responsible for the day-to-day running of the eLife Innovation Initiative, which supports the development of open-source tools, technologies and processes aimed at improving the discovery, sharing, consumption and evaluation of scientific research. Prior to joining eLife, Emmy completed a PhD in neuroscience at the European Molecular Biology Laboratory in Rome, Italy. She is passionate about building communities, fostering collaborations and developing technological solutions to make research more open, reproducible and user-friendly.</p>\n\n<p>Twitter: <a href=\"https://twitter.com/eLifeInnovation\">@eLifeInnovation</a> / <a href=\"https://twitter.com/emmy_ft\">@emmy_ft</a></p>\n\n<h1>Previous talks by speaker</h1>\n\n<ul>\n<li>Invited speaker, Biohackathon-Europe, November 2019 ( <a href=\"https://doi.org/10.6084/m9.figshare.11295032\">slides</a> )</li>\n<li>Mozilla Open Leaders X Launch party, October 2019 ( <a href=\"https://youtu.be/t7o3bEGTW8s?t=97\">video</a> )</li>\n<li>Invited panellist, EMBL Open Access Week panel on “How is scientific publishing changing?”, September 2019</li>\n<li>Invited speaker, SciLifeLab workshop on “Reproducibility and Data Reuse in Life Science”, September 2019 ( <a href=\"https://youtu.be/fKnef69wT3g?t=10668\">video</a>, <a href=\"https://figshare.com/articles/190919_SciLifeLab_eLife_workshop_talk_on_reproducible_publishing/9868625\">slides</a>)</li>\n<li>Tech talk, ISMB/ECCB, July 2019 (<a href=\"https://www.youtube.com/watch?v=7mOadlm7-WI\">video</a>, <a href=\"https://figshare.com/articles/190723_RDS_talk_at_ISMB_ECCB/8983640\">slides</a>)</li>\n<li>Speaker, OAI11, June 2019 (<a href=\"https://figshare.com/articles/190620_OAI11_Reproducibility/8299226\">slides</a>)</li>\n<li>Invited demo, SSI Collaboration Workshop, April 2019</li>\n</ul>",
    "persons": [
      "Emmy Tsang"
    ]
  },
  {
    "start": 1577901600000,
    "duration": 30,
    "room": "AW1.126",
    "title": "The Journal of Open Source Software:  credit for invisible work",
    "subtitle": "",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>Researchers rarely cite software they use as part of their research. As a result, research software and the time spent developing it have become invisible scholarly contributions. This lack of visibility reduces the incentives that are necessary to produce and share high quality software that are essential for the progress of science.  The Journal of Open Source Software (JOSS) is an open source, open access journal primarily designed to make it easier for those individuals authoring research software to gain career credit for their work by publishing short software papers. Software papers are a recognized mechanism for authors of research software to create a citable ‘entity’ which can easily be cited in journals and as such directly impact a researcher’s career via established metrics such as the h-index. JOSS is unique in that it only accepts very short (~ 1-2 pages) papers, with short summaries and links to the software repository. In that sense, the software papers are not the focus of the review. Instead, we ask reviewers to conduct a thorough review of the associated software (which must be open source) ensuring that it is well documented, straightforward to install and functions as expected. In this talk I will describe the origin and impact that JOSS has had on research open source and also touch upon issues such as sustainability and credit.</p>",
    "description": "",
    "persons": [
      "Karthik Ram"
    ]
  },
  {
    "start": 1577903400000,
    "duration": 30,
    "room": "AW1.126",
    "title": "DSpace 7: A major leap forward for the leading institutional repository platform",
    "subtitle": "Tale of a mature, international FOSS community embracing Angular",
    "track": "Open Research Tools and Technologies",
    "abstract": "<p>The DSpace community is anticipating the largest release ever in 2020 with DSpace 7 ( https://wiki.duraspace.org/display/DSPACE/DSpace+Release+7.0+Status ). The platform is used in thousands of research institutions around the globe and powers systems including dspace.mit.edu, dash.harvard.edu and openknowledge.worldbank.org. If you download an academic paper through Google Scholar today, the chance is large that it is served to you thanks to a DSpace institutional repository.</p>\n\n<p>The talk aims to briefly introduce the scope and usage of the DSpace software. Attendees will learn how the governance of the DSpace community is structured, and what lead to the decision to drop the two legacy UIs, JSPUI and XMLUI, in favour of an endeavour to introduce Angular as the new UI layer.</p>\n\n<p>The most relevant piece of the presentation for the Fosdem audience, will be an outline of the tooling and best practices applied in the community, together with a pro and con evaluation.</p>\n\n<p>We are very keen on learning from other participants in the audience what they could advise, both on a technical and organisational level, going forward.</p>\n\n<h2>Previous presentations on DSpace 7</h2>\n\n<p><a href=\"https://lecture2go.uni-hamburg.de/l2go/-/get/v/24819\">Introducing DSpace 7</a>\n<a href=\"https://lecture2go.uni-hamburg.de/l2go/-/get/v/24831\">DSpace 7 Configurable Entities</a>\n<a href=\"https://lecture2go.uni-hamburg.de/l2go/-/get/v/24820\">The DSpace 7 Angular UI from a user perspective</a></p>",
    "description": "",
    "persons": [
      "Bram Luyten"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 10,
    "room": "K.3.201",
    "title": "Welcome to game development devroom",
    "subtitle": "",
    "track": "Game Development",
    "abstract": "<p>Welcome to FOSDEM game development devroom! We'll present what this is all about and invite you to participate.</p>",
    "description": "",
    "persons": [
      "Julian Murgia",
      "George Marques"
    ]
  },
  {
    "start": 1577875500000,
    "duration": 25,
    "room": "K.3.201",
    "title": "Python for Godot",
    "subtitle": "",
    "track": "Game Development",
    "abstract": "<p>Godot is an incredible open source game engine. Among it key features, it comes packed with a script language called GDscript and loosely based on Python.\nBut could it be even better ? Could we use the real Python to code our game on Godot ?</p>\n\n<p>And maybe even more important, is it really a good idea ?</p>",
    "description": "<p>Godot is an advanced, feature-packed, multi-platform 2D and 3D open source game engine.\nThe project has joined the Software Freedom Conservancy project and it growing community makes it hopes to become a real alternative to Unity&amp;GameMaker.</p>\n\n<p>This talk present the Godot-Python project aiming at bringing Python as a fully integrated language into Godot.</p>\n\n<p>We will have a look at Godot’s internal architecture as is it itself a real interpreter with it garbage collector, dynamic typing, introspection and even builtin custom scripting language.\nAll of this having to work next to our Python interpreter and communicate back and forth with it.</p>\n\n<p>We will then dig into Godot-Python design choices, both past and current, this project came through a looot of errors and trials ;-)</p>\n\n<p>Finally we will discuss the pros and cons about using Python as a script language for Godot vs the traditional GDscript.</p>\n\n<p>The audience should have some basic knowledge of C level computing (static vs dynamic language, compilation &amp; linking).</p>",
    "persons": [
      "Emmanuel Leblond"
    ]
  },
  {
    "start": 1577877300000,
    "duration": 45,
    "room": "K.3.201",
    "title": "0 A.D.: Graphics Pipeline",
    "subtitle": "How open-source game graphics works",
    "track": "Game Development",
    "abstract": "<p>A story about graphics pipeline of 0 A. D. (an open-source game of Wildfire Games) and its issues.</p>\n\n<p>Talking structure:</p>\n\n<ul>\n<li><p>A little history of 0AD (https://play0ad.com/about/the-story-of-0-a-d/)</p></li>\n<li><p>How our graphics pipeline works</p></li>\n<li><p>Used technologies (SDL, OpenGL 1/2, ARB/GLSL shaders)</p></li>\n<li><p>Known problems (old OpenGL, legacy support of OpenGL drivers on macOS 10.14)</p></li>\n<li><p>Future plans</p></li>\n</ul>",
    "description": "",
    "persons": [
      "Vladislav Belov"
    ]
  },
  {
    "start": 1577880300000,
    "duration": 25,
    "room": "K.3.201",
    "title": "Can anyone need Veripeditus?",
    "subtitle": "Reviving a Python AR gaming framework",
    "track": "Game Development",
    "abstract": "<p>Veripeditus is a framework for teh creation of mobile augemented reality games in Python, designed, but not limited to, use in education. The project is currently sleeping due to various reasons. We would like to find out whether its technology is still current and if there is value in reviving it.</p>",
    "description": "",
    "persons": [
      "Dominik George"
    ]
  },
  {
    "start": 1577882100000,
    "duration": 25,
    "room": "K.3.201",
    "title": "Game development with OpenXR",
    "subtitle": "",
    "track": "Game Development",
    "abstract": "<p>Last year Khronos released OpenXR, an open API for using XR hardware. In this talk we will look at the practical side of creating VR applications and games with OpenXR.</p>",
    "description": "<p>Since the Oculus VR development kit started a resurgence of consumer VR, game development has largely been relegated to proprietary VR APIs and runtimes. Khronos reacted by creating an open API for using XR hardware and released it OpenXR 1.0 in July 2019. Collabora implemented the OpenXR API in a runtime nicknamed Monado, built on open source VR hardware drivers. With these building blocks VR applications can now use standardized APIs and run on a FOSS stack.</p>\n\n<p>In this talk, Christoph will give an overview of the feature set of the OpenXR API and the practical side of creating VR applications and games with OpenXR. We will look at low level code using the OpenXR API directly as well as an OpenXR plugin for the godot engine.</p>",
    "persons": [
      "Christoph Haag"
    ]
  },
  {
    "start": 1577883900000,
    "duration": 55,
    "room": "K.3.201",
    "title": "Open lightning talks",
    "subtitle": "Showcase your open source project",
    "track": "Game Development",
    "abstract": "<p>This hour is dedicated to people who want to come up and shortly present their project, without having to schedule a full talk.</p>",
    "description": "<p>Bring your open source game-related project (be it an engine, game, demo, tool, or something else) and showcase to our fellow gamedev friends. Talks should have at most five minutes with no time allotted for questions. If you want to bring a laptop to show something, keep it ready and install/uninstall it as fast as possible (will be part of your five minutes). Please be understanding so everyone can have a chance. Contact the room managers on the event day if you want to present something.</p>",
    "persons": [
      "George Marques"
    ]
  },
  {
    "start": 1577887500000,
    "duration": 55,
    "room": "K.3.201",
    "title": "Java & Games",
    "subtitle": "A rivalrous case-study from porting Doom 3",
    "track": "Game Development",
    "abstract": "<p>According to the interwebs, Java is one of the most popular programming languages in the multiverse.<br/>\nAnd yet, when it comes to games, its popularity seems to dwindle.</p>\n\n<p>Why though...<br/>\nIs it really not suited for game development?<br/>\nIs the language as bad as the critics claim??<br/>\nIs GC...EVIL!?</p>\n\n<p>I have this side-project of porting Doom 3 from C/C++ to Java. And even though Doom 3 is a ~15 year old game, it is still a massive AAA code-base. So I believe my experiences there can adequately answer a lot of the questions that come to mind when you think of Java &amp; Games.</p>\n\n<p>During this talk, I will try to convince you that Java is a very viable game prototyping/development tool.<br/>\nEither that, or will add more fuel to the flame...</p>",
    "description": "",
    "persons": [
      "Mahmoud Abdelghany"
    ]
  },
  {
    "start": 1577891100000,
    "duration": 30,
    "room": "K.3.201",
    "title": "Double your contributors using these 3 simple tricks!",
    "subtitle": "Why would someone work on your project?",
    "track": "Game Development",
    "abstract": "<p>For some arcane reason contributors spend their precious time on open source game projects. Why do they do this? And more importantly: What can you do to make them do it on your project?</p>",
    "description": "<p>FOSS game projects live on the motivation of their contributors. This motivation usually consists of two parts: The desire to contribute to FOSS game development in general and the desire to contribute to a specific project. While the former is very interesting to discuss, our ability to affect it is negligible. Thankfully, the control of the latter is definitely possible, and it is crucial to the survival of our projects.\nIn this talk we will analyse different characteristics of FOSS game projects with regards to contributor recruitment and retention. We will try to present practical steps to lower the risk of your current or future projects dying to inactivity and obscurity.</p>",
    "persons": [
      "Eshed Shaham"
    ]
  },
  {
    "start": 1577893500000,
    "duration": 60,
    "room": "K.3.201",
    "title": "Benefits of porting Godot Engine to Vulkan",
    "subtitle": "List of benefits observed from porting Godot Engine to Vulkan",
    "track": "Game Development",
    "abstract": "<p>Godot 4.0 is in the process of being ported from OpenGL to a Vulkan rendering API.\nThis new technology provides new challenges and benefits for improving quality and performance,\nwhich will be explained and compared during this presentation.</p>",
    "description": "<p>Godot 4.0 is in the process of being ported from OpenGL to a Vulkan rendering API.\nThis new technology provides new challenges and benefits for improving quality and performance,\nwhich will be explained and compared during this presentation.</p>",
    "persons": [
      "Juan Linietsky"
    ]
  },
  {
    "start": 1577897700000,
    "duration": 20,
    "room": "K.3.201",
    "title": "Blender projects for 2020",
    "subtitle": "",
    "track": "Game Development",
    "abstract": "",
    "description": "",
    "persons": [
      "Ton Roosendaal"
    ]
  },
  {
    "start": 1577898900000,
    "duration": 30,
    "room": "K.3.201",
    "title": "Reloading Escoria",
    "subtitle": "Make point and click adventure games under Godot great again",
    "track": "Game Development",
    "abstract": "<p>Escoria is a Libre framework for the creation of point-and-click adventure games with MIT-Licenced Godot Engine. Since its release, Godot Engine changed a lot at fast pace while Escoria code was still based on old Godot 1.x features.</p>\n\n<p>In this presentation, I'll present the current state of Escoria and discuss the process of rewrite as a Godot Engine plugin. It'll cover architecture and design, allowing adventure game creators to use Godot Engine as a full-featured editor for their adventure game.</p>",
    "description": "<p>Escoria is a Libre framework for the creation of point-and-click adventure games with MIT-Licenced Godot Engine. It was initially developed for the adventure game The Interactive Adventures of Dog Mendonça and Pizzaboy® and later streamlined for broader usages and open sourced as promised to the backers of the Dog Mendonça Kickstarter campaign. Since its release, Godot Engine changed a lot at fast pace while Escoria code was still based on old Godot 1.x features. It is more a collection of dedicated scripts than a proper Godot Engine layer designed for point and click adventure games. Evolving Escoria is quite a big task, so it might be better to actually start it over with a new project architecture while keeping its most interesting features in the process.</p>\n\n<p>In this presentation, I'll present the current state of Escoria and discuss the process of rewrite as a Godot Engine plugin. It'll cover architecture and design, allowing them to use Godot Engine as a full-featured editor for their adventure game.</p>",
    "persons": [
      "Julian Murgia"
    ]
  },
  {
    "start": 1577901000000,
    "duration": 40,
    "room": "K.3.201",
    "title": "Spring & Steam, an Odyssey",
    "subtitle": "",
    "track": "Game Development",
    "abstract": "<p>The Spring RTS Engine has been in active development since 2005. In the past few years, two of its games, Evolution RTS and Zero-K, have been released on Steam. The journey to these releases was long and difficult. Let's regale ourselves with tales of the adventures of the devs, learn many things that you shouldn't do, and also learn what to do in case you've already done what you shouldn't have.</p>",
    "description": "",
    "persons": [
      "Eshed Shaham"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 20,
    "room": "K.3.401",
    "title": "How lowRISC made its Ibex RISC-V CPU core faster",
    "subtitle": "Using open source tools to improve an open source core",
    "track": "RISC-V",
    "abstract": "<p>Ibex implements RISC-V 32-bit I/E MC M-Mode, U-Mode and PMP. It uses an in order 2 stage pipe and is best suited for area and power sensitive rather than high performance applications. However there is scope for meaningful performance gains without major impact to power or area. This talk describes work done at lowRISC to analyse and improve the performance of Ibex. The RTL of an Ibex system is simulated using Verilator to run CoreMark and Embench and the traces analysed to identify the major sources of stalls within them. This informs where improvements should be targeted. The open source implementation tools Yosys and openSTA are used to assess potential timing and area impacts of these improvements. In this talk you’ll learn about the pipeline of Ibex, methods to analyse the performance of CPU microarchitecture and how to use Yosys and openSTA to analyse what limits clock frequency in a design.</p>",
    "description": "",
    "persons": [
      "Greg Chadwick"
    ]
  },
  {
    "start": 1577875800000,
    "duration": 20,
    "room": "K.3.401",
    "title": "BlackParrot",
    "subtitle": "An Open Source RISC-V Multicore For and By the World",
    "track": "RISC-V",
    "abstract": "<p>BlackParrot is a Linux-capable, cache-coherent RISC-V multicore, designed for efficiency and ease of use.  In this talk, we will provide an architectural overview of BlackParrot, focusing on the design principles and development process as well as the software and hardware ecosystems surrounding the core. We will also discuss the project roadmap and our plans to engage the open-source community.  Last, we will demonstrate a multithreaded RISC-V program running on top of Linux on a multicore BlackParrot FPGA implementation.</p>",
    "description": "<p>BlackParrot aims to be the default open-source, Linux-capable, cache-coherent, RV64GC multicore used by the world. Although originally developed by the University of Washington and Boston University, BlackParrot strives to be community-driven and infrastructure agnostic, a core which is Pareto optimal in terms of power, performance, area and complexity. In order to ensure BlackParrot is easy to use, integrate, modify and most importantly trust, development is guided by three core principles: Be Tiny, Be Modular, and Be Friendly. Development efforts have prioritized ease of use and silicon validation as first order design metrics, so that users can quickly get started and trust that their results will be representative of state-of-the-art ASIC designs. BlackParrot is ideal as the basis for a research platform, a lightweight accelerator host or as a standalone Linux core.</p>",
    "persons": [
      "Dan Petrisko"
    ]
  },
  {
    "start": 1577877000000,
    "duration": 20,
    "room": "K.3.401",
    "title": "The HammerBlade RISC-V Manycore",
    "subtitle": "A programmable, scalable RISC-V fabric",
    "track": "RISC-V",
    "abstract": "<p>HammerBlade is an open source RISC-V manycore that has been under development since 2015 and has already been silicon validated with a 511-core chip in 16nm TSMC. It features extensions to the RISC-V ISA that target GPU-competitive performance for parallel programs (i.e. GPGPU) including graphs and ML workloads. In this talk we will overview the components of the HW and software ecosystem in the latest version, and show you how to get up and running as an open source user or contributor in either SW or HW on Amazon F1 cloud FPGAs.</p>",
    "description": "<p>HammerBlade is an open source RISC-V manycore that has been under development since 2015 and has already been silicon validated with a 511-core chip in 16nm TSMC. It features extensions to the RISC-V ISA that target GPU-competitive performance for parallel programs (i.e. GPGPU) including graphs and ML workloads. In this talk we will overview the components of the HW and software ecosystem in the latest version, and show you how to get up and running as an open source user or contributor in either SW or HW on Amazon F1 cloud FPGAs. We will overview the HW-architecture, the CUDA-like programming environment, the runtime software, the HW architecture, and our Amazon F1 cloud emulation and cosimulation environments, and our suite of performance analysis tools.</p>",
    "persons": [
      "Michael Taylor"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 20,
    "room": "K.3.401",
    "title": "Open ESP",
    "subtitle": "The Heterogeneous Open-Source Platform for Developing RISC-V Systems",
    "track": "RISC-V",
    "abstract": "<p>ESP is an open-source research platform for RISC-V systems-on-chip that integrate many hardware accelerators.<br/>\nESP provides a vertically integrated design flow from software development and hardware integration to full-system prototyping on FPGA.  For application developers, ESP offers domain-specific automated solutions to synthesize new accelerators for their software and map it onto the heterogeneous SoC architecture.  For hardware engineers, ESP offers automated solutions to integrate their accelerator designs into the complete SoC.<br/>\nThe participants in this FOSDEM20 event will learn how to use ESP from the viewpoints of both application developers and hardware engineers by following a series of short hands-on tutorials embedded in the lecture.\nConceived as a heterogeneous integration platform and tested through years of teaching at Columbia University, ESP is intrinsically suited to foster collaborative engineering of RISC-V based SoCs across the open-source community.</p>",
    "description": "<ul>\n<li>What is ESP?</li>\n</ul>\n\n\n<p>ESP is an open-source research platform to design and program heterogeneous systems-on-chip (SoCs).  A heterogeneous SoC combines multiple general-purpose processor cores and many specialized hardware accelerators.</p>\n\n<ul>\n<li>What does ESP offer?</li>\n</ul>\n\n\n<p>ESP provides automated solutions to (a) synthesize new accelerators, (b) integrate them with RISC-V processors and other third party accelerators into a complete SoC, (c) rapidly prototype the SoC on an FPGA board, and (d) run software applications that take advantage of these accelerators.\nESP contributes to the open-source movement by supporting the realization of more scalable architectures for SoCs that integrate more heterogeneous components, thanks to a more flexible design methodology that accommodates different specification languages and design flows.</p>\n\n<ul>\n<li>What is an example of an SoC prototype realized with ESP?</li>\n</ul>\n\n\n<p>With ESP's automation capabilities, it is easy to realize FPGA-based prototypes of complete SoCs.  For example, an SoC may feature the Ariane RISC-V processor core booting Linux, a multi-plane network-on-chip supporting a partitioned memory hierarchy with multiple DRAM controllers, and tens of loosely-coupled accelerators that execute coarse-grained tasks exchanging large amount of data with DRAM through direct-memory access (DMA).  These accelerators can be third-party open-source hardware components that “speak” the AXI protocol (e.g. the NVIDIA NVDLA accelerator for deep learning) or new accelerators that can be synthesized with different design flows from specifications written in different languages, including: C with Xilinx Vivado HLS, SystemC with Cadence Stratus HLS, Keras TensorFlow and PyTorch with hls4ml, Chisel, SystemVerilog, Verilog, and VHDL.</p>\n\n<ul>\n<li>Why is ESP relevant now?</li>\n</ul>\n\n\n<p>Information technology has entered the age of heterogeneous computing.  Across a variety of application domains, computing systems rely on highly heterogeneous architectures that combine multiple general-purpose processors with specialized hardware accelerators.  The complexity of these systems, however, threatens to widen the gap between the capabilities provided by semiconductor technologies and the productivity of computer engineers.  ESP tackles this challenge by raising the level of abstraction in the design process, simplifying the domain-specific programming of heterogeneous architectures, and leveraging the potential of the emerging open-source hardware movement.</p>\n\n<ul>\n<li>What are some key ingredients of the ESP approach?</li>\n</ul>\n\n\n<p>Building on years of research on communication-based system-level design at Columbia University, ESP combines an architecture and a methodology.  The flexible tile-based architecture simplifies the integration of heterogeneous components by balancing regularity and specialization.  The companion methodology raises the level of abstraction to system-level design, thus promoting closer collaboration among software programmers and hardware engineers.  Through the automatic generation of device drivers from pre-designed templates, ESP simplifies the invocation of accelerators from user-level applications executing on top of Linux.  Through the automatic generation of a multi-plane network-on-chip from a parameterized model, the ESP architecture can scale to accommodate many processors, tens of accelerators, and a distributed memory hierarchy.  A set of ESP Platform Services provides pre-validated solutions for accelerators configuration, memory management, sharing of system resources, and dynamic frequency scaling, among others.</p>\n\n<ul>\n<li>How does ESP differ from other RISC-V based platforms?</li>\n</ul>\n\n\n<p>To date, the majority of the open-source hardware (OSH) efforts related to RISC-V have focused on the development of processor cores that implement the RISC-V ISA and small-scale SoCs that connect these cores with tightly-coupled functional units and coprocessors, typically through bus-based interconnects. Meanwhile, there have been less efforts in developing solutions for large-scale SoCs that combine RISC-V cores with many loosely-coupled components, such as coarse-grain accelerators, interconnected with a network-on-chip (NoC).  Compared to other RISC-V related projects, ESP is focused on scalability (with the NoC-based architecture), heterogeneity (with emphasis on loosely-coupled accelerators), and flexibility (with support of different design flows).  Just like the ESP architecture simplifies the integration of heterogeneous components developed by different teams, the ESP methodology embraces the use of heterogeneous design flows for component development.</p>\n\n<ul>\n<li>Who are the authors of ESP?</li>\n</ul>\n\n\n<p>ESP has been developed by the System-Level Design (SLD) group in the Department of Computer Science at Columbia University during the past seven years.  The SLD group has published over a dozen scientific papers in peer-reviewed conferences and journals to describe the most innovative aspects of ESP.  ESP has been released as an open-source project via GitHub in the summer 2019.</p>",
    "persons": [
      "Luca Carloni"
    ]
  },
  {
    "start": 1577879400000,
    "duration": 20,
    "room": "K.3.401",
    "title": "Building Loosely-coupled RISC-V Accelerators",
    "subtitle": "Using Chisel/FIRRTL to build accelerator templates and collateral for the ESP SoC platform",
    "track": "RISC-V",
    "abstract": "<p>The burgeoning RISC-V <em>hardware</em> ecosystem includes a number of microprocessor implementations [1, 3] and SoC generation frameworks [1, 2, 7]. However, while accelerator “sockets” have been defined and used (e.g., Rocket Chip’s custom coprocessor/RoCC), accelerators require <em>additional</em> collateral to be generated like structured metadata descriptions, hardware wrappers, and device drivers. Requiring manual effort to generate this collateral proves both time consuming and error prone and is at odds with an agile approach to hardware design. However, the existence and use of hardware construction languages and hardware compilers provides a means to automate this process. Through the use of the Chisel hardware description language [4] and the FIRRTL hardware compiler [5], we provide a definition of an abstract accelerator template which users then implement to integrate an accelerator with the Embedded Scalable Platform (ESP) System-on-Chip platform [2, 8]. Through the use of this template, we are able to automatically generate XML metadata necessary to integrate accelerators with the ESP platform and work on generating all collateral is in progress. Our accelerator template is open source software provided under an Apache 2.0 license [6].</p>\n\n<p>[1] CHIPS alliance Rocket-chip. <em>GitHub Repository</em>. Online: https://github.com/chipsalliance/rocket-chpi.\n[2] Columbia University Embedded scalable platform. <em>git repository</em>. Online: https://github.com/sld-columbia/esp.\n[3] ETH Zurich Ariane. <em>GitHub Repository</em>. Online: https://github.com/pulp-platform/ariane.\n[4] Freechips Project Chisel3. <em>GitHub Repository</em>. Online: https://github.com/freechipsproject/chisel3.\n[5] Freechips Project FIRRTL. <em>GitHub Repository</em>. Online: https://github.com/freechipsproject/firrtl.\n[6] IBM ESP chisel acclerators. <em>GitHub Repository</em>. Online: https://github.com/ibm/esp-chisel-accelerators.\n[7] Princeton University OpenPiton. <em>GitHub Repository</em>. Online: https://github.com/PrincetonUniversity/openpiton.\n[8] ESP: The open-source heterogeneous system-on-chip platform. Online: https://www.esp.cs.columbia.edu/.</p>",
    "description": "",
    "persons": [
      "Schuyler Eldridge"
    ]
  },
  {
    "start": 1577880600000,
    "duration": 20,
    "room": "K.3.401",
    "title": "ERASER: Early-stage Reliability And Security Estimation for RISC-V",
    "subtitle": "An open source framework for resilience/security evaluation and validation in RISC-V processors",
    "track": "RISC-V",
    "abstract": "<p>RISC-V processors have gained acceptance across a wide range of computing domains, from IoT to embedded/mobile class and even in server-class processing systems. In processing systems ranging from connected cars and autonomous vehicles, to those on-board satellites and spacecrafts, these processors are targeted to function in safety-critical systems, where Reliability, Availability and Serviceability (RAS)-based considerations are of paramount importance. Along with potential system vulnerabilities caused primarily due to random errors, these processors may also be sensitive to targeted errors, possibly from malicious entities, which raises serious concerns regarding the security and safety of the processing system. Consequently, such systems necessitate the incorporation of RAS-based considerations right from an early stage of processor design.</p>\n\n<p>While the hardware and software ecosystem around RISC-V has been steadily maturing, there have, however, been limited developments in early stage reliability-aware design and verification. The Early-stage Reliability And Security Estimation for RISC-V (ERASER) tool attempts to address this shortcoming. It consists of an open source framework aimed at providing directions to incorporate such reliability and security features at an early, pre-silicon stage of design. These features may include what kind of protection to be applied and which components within the processor should they be applied to. The proposed infrastructure comprises of an open source toolchain for early stage modeling of latch vulnerability in a RISC-V core (SERMiner [1]), a tool for automated generation of stress marks that maximize the likelihood of a transient-failure induced error (Microprobe (RISC-V) [2]), and verification by means of statistical and/or targeted fault injection (Chiffre [3]). While the infrastructure is targeted towards any core that uses the RISC-V ISA, the repository provides an end-to-end flow for the Rocket core [4].</p>\n\n<p>ERASER thus evaluates “RAS-readiness”, or the effectiveness of protection techniques in processor design such that processor vulnerability in terms of Failures-In-time (FIT) rate is minimized, for a specified power/performance overhead. FIT rate is defined as the number of failures in one billion hours of operation and is a standard vulnerability metric used in industry.</p>\n\n<p>ERASER is an open source tool available for download at https://github.com/IBM/eraser. The tool currently supports analysis of all latches in the design across a single Rocket core and the generation of stressmarks that can be used to evaluate the vulnerability of these latches. In addition to radiation-induced soft errors, we plan to extend ERASER to also model errors due to voltage noise, thermal and aging-induced failures, both in memory and logic, and generate representative stressmarks.</p>\n\n<p>ERASER is an initial effort to devise a comprehensive methodology for RAS analysis, particularly for open-source hardware, with the hope that it spurs further research and development into reliability-aware design both in industry and academia.</p>\n\n<p>References:</p>\n\n<ol>\n<li><p>K. Swaminathan, R. Bertran, H. Jacobson, P. Kudva, P. Bose, ‘Generation of Stressmarks for Early-stage Soft-error Modeling’, International Conference on Dependable Systems and Networks (DSN) 2019</p></li>\n<li><p>S. Eldridge R. Bertran, A. Buyuktosunoglu, P. Bose, ‘MicroProbe: An Open Source Microbenchmark Generator, ported to the RISC-V ISA, the 7th RISC-V workshop, 2017</p></li>\n<li><p>S. Eldridge, A. Buyuktosunoglu and P. Bose, ‘Chiffre A Configurable Hardware Fault Injection Framework for RISC-V Systems’ 2nd Workshop on Computer Architecture Research with RISC-V (CARRV), 2018</p></li>\n<li><p>Krste Asanović, Rimas Avižienis, Jonathan Bachrach, Scott Beamer, David Biancolin, Christopher Celio, Henry Cook, Palmer Dabbelt, John Hauser, Adam Izraelevitz, Sagar Karandikar, Benjamin Keller, Donggyu Kim, John Koenig, Yunsup Lee, Eric Love, Martin Maas, Albert Magyar, Howard Mao, Miquel Moreto, Albert Ou, David Patterson, Brian Richards, Colin Schmidt, Stephen Twigg, Huy Vo, and Andrew Waterman, The Rocket Chip Generator, Technical Report UCB/EECS-2016-17, EECS Department, University of California, Berkeley, April 2016</p></li>\n</ol>",
    "description": "<p>The attached figure shows a representative flow for the RAS estimation methodology. An initial characterization of all instructions in the RISC-V ISA is carried out via RTL simulation using an existing core model (eg. the Rocket core). The simulation is configured to generate VCD (Value- Change Dump) files for every single instruction testcase. The SERMiner tool parses these VCD files to determine latch activities across the core, aggregated at a macro (or RTL module) level. Based on these per-instruction latch activities, SERMiner outputs an instruction sequence, which forms the basis of the SER stressmark to be generated by Microprobe (RISC-V). Microprobe (RISC-V) is a microbenchmark generation tool that is capable of generating microbenchmarks geared towards specific architecture and micro-architecture level characterization. One of its key applications is in the generation of stressmarks, or viruses, that target various worst-case corners of processor operation. These stressmarks may be targeted at maximizing power, voltage noise, temperature, or soft-error vulnerability as in case of this tool. The generated stressmark is then used to generate a list of latches that show a high residency and hence a high SER vulnerability. These latches are the focus of fault injection-based validation experiments using the Chiffre tool. Chiffre provides a framework for automatically instrumenting a hardware design with run-time configurable fault injectors. The vulnerable latches obtained from running the generated stressmarks through the Rocket core model, and then through SERMiner, are earmarked for targeted fault injection experiments using Chiffre. The objective of these experiments is to further prune the list of vulnerable latches by eliminating those that are derated, that is, they do not affect the overall output even when a fault is injected in them. Focusing any and all protection strategies on this final list of latches would maximize RAS coverage across the entire core.</p>\n\n<p>Ongoing and future work:</p>\n\n<p>ERASER currently only supports analysis of all latches in the design across a single Rocket core and the generated stressmarks can be used to evaluate the vulnerability of these latches. Most on-chip memory structures such as register files and caches, are equipped with parity/ECC protection and are as such protected against most radiation-induced soft errors. However, they are still vulnerable to supply voltage noise, thermal and aging-induced failures, and other hard or permanent errors. We plan to extend ERASER to model such errors, both in memory and logic, and generate stressmarks representative of worst-case thermal emergencies and voltage noise, in addition to soft errors.</p>",
    "persons": [
      "Karthik Swaminathan"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 20,
    "room": "K.3.401",
    "title": "RISC-V Software and Firmware Development in the Cloud Using OpenPiton+Ariane on Amazon F1",
    "subtitle": "",
    "track": "RISC-V",
    "abstract": "<p>RISC-V application, OS, and firmware development has been slowed by the lack of \"real hardware\" available for developers to work with. With the rise of FPGAs in the cloud and the recent release of the OpenPiton+Ariane manycore platform on Amazon's F1 cloud FPGA platform, we propose using 1-12 core OpenPiton+Ariane processors emulated on F1 to develop RISC-V software and firmware. In this talk, we will give an accelerated tutorial on how to get started with OpenPiton+Ariane, the spec-compliant RISC-V platform it offers, and how the firmware and OS can be modified and run on top. We will show a number of applications built and running for our present Debian distribution and the software development environment that this offers. We will then highlight how hardware and software can be co-designed on OpenPiton+Ariane with the ability to recompile the hardware underlying the cloud FPGA image and deploy it for use by others. This platform is serving as a basis for software and hardware development for the DECADES project, a project investigating heterogenous manycore and hardware accelerator based designs with support for orchestrated data movement.</p>",
    "description": "<p>RISC-V Software and Firmware Development in the Cloud Using OpenPiton+Ariane on Amazon F1</p>\n\n<p>RISC-V application, OS, and firmware development has been slowed by the lack of \"real hardware\" available for developers to work with. With the rise of FPGAs in the cloud and the recent release of the OpenPiton+Ariane manycore platform on Amazon's F1 cloud FPGA platform, we propose using 1-12 core OpenPiton+Ariane processors emulated on F1 to develop RISC-V software and firmware. In this talk, we will give an accelerated tutorial on how to get started with OpenPiton+Ariane, the spec-compliant RISC-V platform it offers, and how the firmware and OS can be modified and run on top. We will show a number of applications built and running for our present Debian distribution and the software development environment that this offers. We will then highlight how hardware and software can be co-designed on OpenPiton+Ariane with the ability to recompile the hardware underlying the cloud FPGA image and deploy it for use by others. This platform is serving as a basis for software and hardware development for the DECADES project, a project investigating heterogenous manycore and hardware accelerator based designs with support for orchestrated data movement.</p>\n\n<p>http://openpiton.org\nhttps://openpiton-blog.princeton.edu/2019/10/bringing-openpiton-to-amazon-ec2-f1-fpgas/</p>\n\n<p>OpenPiton+Ariane contributors include:\nJonathan Balkind, Grigory Chirkov, Yaosheng Fu, Adi Fuchs, Fei Gao, Alexey Lavrov, Ang Li, Xiaohua Liang, Katie Lim, Matthew Matl, Michael McKeown, Tri Nguyen, Samuel Payne, Michael Schaffner, Mohammad Shahrad, Jinzheng Tu, Florian Zaruba, Yanqi Zhou, Georgios Tziantzioulis, Luca Benini, David Wentzlaff</p>\n\n<p>DECADES is a large collaboration from three academic groups: Margaret Martonosi (PI Princeton), David Wentzlaff (PI Princeton), Luca Carloni (PI Columbia) with students/researchers: Jonathan Balkind, Ting-Jung Chang, Fei Gao, Davide Giri, Paul Jackson, Paolo Mantovani, Luwa Matthews, Aninda Manocha, Tyler Sorensen, Jinzheng Tu, Esin Türeci, Georgios Tziantzioulis, and Marcelo Orenes Vera. In addition to the submission author, portions of the talk may be offered by others in the collaboration.</p>",
    "persons": [
      "David Wentzlaff"
    ]
  },
  {
    "start": 1577883000000,
    "duration": 20,
    "room": "K.3.401",
    "title": "Cacheable Overlay Manager RISC-V",
    "subtitle": "",
    "track": "RISC-V",
    "abstract": "<p>We would like to present and overlay technique for RISCV which will be open source by WD.\nThis FW feature acts as and “catchable” manager. It is to be threaded with the Real-Time code and to the toolchain.\nCacheable Overlay Manager RISC-V (ComRV), a technique which fits small devices (as IoT’s), and does not need any HW support.</p>",
    "description": "",
    "persons": [
      "Ofer Shinaar"
    ]
  },
  {
    "start": 1577884200000,
    "duration": 20,
    "room": "K.3.401",
    "title": "RISC-V Boot flow: What's next ?",
    "subtitle": "",
    "track": "RISC-V",
    "abstract": "<p>RISC-V boot flow has come a long way since in recent times by leveraging the various opensource boot loaders/firmware projects. This also helped in achieving a well-supported and standard boot flow for RISC-V. As a result, developers can use the same boot loaders to boot Linux on RISC-V as they do in other architectures. Currently, U-Boot is used as the last stage boot loader and OpenSBI as the machine mode run time service provider, but there's more work to be done. A few of such future works includes U-boot SPL support, UEFI boot in RISC-V Linux and booting protocol improvements. This talk will focus on some of these ongoing works which are necessary to declare that RISC-V is truly ready for world domination.</p>",
    "description": "",
    "persons": [
      "Atish Patra"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 20,
    "room": "K.3.401",
    "title": "Oreboot",
    "subtitle": "RISC-V Firmware in Rust",
    "track": "RISC-V",
    "abstract": "<p>Oreboot = Coreboot - C. Oreboot is a fully open-source power-on-reset and romstage firmware written in Rust. Oreboot can boot a HiFive RISC-V processor to Linux with a Go user-mode.</p>\n\n<p>Oreboot rethinks the firmware driver models. Each driver is distilled to four basic functions: init, pread, pwrite and shutdown. This interface allows us to make convenient higher-level drivers such as a \"union driver\" which duplicates a single write operation to multiple drivers. This makes consoles which have multiple underlying UART drivers elegant.</p>\n\n<p>By using the Rust programming language, Oreboot has a leg-up in terms of security and reliability compared to contemporary firmware written in C or assembly. Rust's borrow-checker ensures pointers are not used after freed and proves that coroutines are thread-safe at compile time.</p>\n\n<p>In this talk, we will also present a short overview of the basics of Rust, how our driver model incorporates coroutines and the bootflow of Oreboot.</p>",
    "description": "",
    "persons": [
      "Ryan O'Leary"
    ]
  },
  {
    "start": 1577886600000,
    "duration": 20,
    "room": "K.3.401",
    "title": "RISC-V Hypervisors",
    "subtitle": "Where are we ? What next ?",
    "track": "RISC-V",
    "abstract": "<p>The RISC-V H-extension (aka hypervisor extension) is suitable for both Type1 and Type2 hypervisor. We have ported two hypervisors for RISC-V: Xvisor (Type1) and KVM (Type2). We show the current state and furture work for both hypervisors.</p>",
    "description": "",
    "persons": [
      "Anup Patel"
    ]
  },
  {
    "start": 1577887800000,
    "duration": 20,
    "room": "K.3.401",
    "title": "Port luajit to RISC-V",
    "subtitle": "Motivation, first steps and perspectives",
    "track": "RISC-V",
    "abstract": "<p>There is a need for a lightweight tools for experiments with RISC-V\ncustom extensions. Adding support for custom instructions in\nbinutils/gcc/llvm is out of range for many hardware architects. LuaJIT\nincludes a small and powerful assembler: dynasm, accessible from\nwithin Lua interpreter. Currently dynasm supports following 32 and 64-bit\ninstruction sets: x86, x64, ARM, PowerPC, and MIPS, and it is just\nreasonable to extend this support to RISC-V.</p>\n\n<p>Lua itself is a very compact and simple yet powerful dynamic language,\nits JIT compiler (luajit) makes it one of the fastest, if not the\nfastest, interpreted language, and it is used in many projects, so\nhaving it running on RISC-V would have use besides the mere internal\nneed for experimental platform.</p>",
    "description": "<h1>Outline</h1>\n\n<h2>Project scope</h2>\n\n<ul>\n<li>Lua 5.1, luajit 2.1 overview</li>\n<li>rv32/rv64</li>\n<li>dynasm</li>\n<li>interpreter/virtual machine</li>\n<li>jit</li>\n<li>gc</li>\n<li>bit manipulation ('B' extention and bitop in Lua 5.3)</li>\n</ul>\n\n\n<h2>Develpment platforms</h2>\n\n<ul>\n<li>spike (ISA simulator, 32 and 64-bit)</li>\n<li>rv64: SiFive Unleashed</li>\n<li>rv32: softcore CPU on FPGA</li>\n</ul>\n\n\n<h2>Benchmarks and baseline</h2>\n\n<h2>Deviation (side project)</h2>\n\n<p>Yet another Forth and yet another assembler</p>",
    "persons": [
      "Anton Kuzmin"
    ]
  },
  {
    "start": 1577890500000,
    "duration": 5,
    "room": "K.3.401",
    "title": "Welcome to the Retrocomputing DevRoom 2020",
    "subtitle": "",
    "track": "Retrocomputing",
    "abstract": "<p>A quick introduction to the 3rd edition of the retro-computing devroom.</p>",
    "description": "",
    "persons": [
      "Pau Garcia Quiles (pgquiles)",
      "François Revol (mmu_man)"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 30,
    "room": "K.3.401",
    "title": "Alpha Waves, the first 3D platformer ever",
    "subtitle": "How 3D graphics worked when there were no graphic cards",
    "track": "Retrocomputing",
    "abstract": "<p>Alpha Waves is the first 3D platform game ever.\nInitially developed on Atari ST, it was then ported on Atari ST and on the IBM PC.\nThe technology later gave rise to Alone in the Dark, a major game that launched Infogrames in the big league.\nThis is the history of that game.</p>",
    "description": "<p>Alpha Waves is the first 3D platform game ever, according to the Guiness Book of Records.</p>\n\n<p>The game was initially developed on Atari ST, representing 17000 lines of 68K assembly code.\nIt was later ported on Atari ST and on the IBM PC, and was the first and only assembly program game that Infogrames ever ported to another CPU.\nThe technology developed for that game later inspired Frederick Raynal to develop Alone in the Dark, a major game that launched Infogrames in the big league.</p>\n\n<p>This talk is the history of that game by its developer. It will cover:\n* General principles of paleo-3D\n* How to draw polygons in software. Fast.\n* Computing 3D transforms using mostly additions\n* Music, graphics and other stuff\n* Funny stories and trivia around the game, including the stints of Infogrames in Artificial Intelligcence and the arch-genesis of Alone in the Dark</p>",
    "persons": [
      "Christophe de Dinechin"
    ]
  },
  {
    "start": 1577892900000,
    "duration": 30,
    "room": "K.3.401",
    "title": "BASICODE: the 8-bit programming API that crossed the Berlin Wall",
    "subtitle": "",
    "track": "Retrocomputing",
    "abstract": "<p>At the height of the cold war, BASIC programs exchanged by radio and cassette tape provided young people of socialist Eastern and capitalist Western Europe a rare insight into each other's worlds. BASICODE was a transmission format and an API developed by the Dutch public broadcasting service to overcome the challenge of exchanging open source hobby programs in the highly fragmented 8-bit computing landscape of the early 1980s, which was dominated by mutually incompatible versions of BASIC. Somewhat improbably, the format was picked up across the iron curtain in the German Democratic Republic, where it experienced its age of greatest popularity. The need for programs to work on platforms with widely different capabilities and incompatible syntaxes forced it to be simply structured, highly regulated and relatively well documented. This makes it ideally suited for implementation in a web browser.</p>",
    "description": "<ul>\n<li>An early age of open source</li>\n<li>Modulating source code for radio transmission</li>\n<li>An API based on GOSUBs in BASIC</li>\n<li>East and West in the 1980s</li>\n<li>Implementing BASICODE in a web browser</li>\n</ul>",
    "persons": [
      "Rob Hagemans"
    ]
  },
  {
    "start": 1577895000000,
    "duration": 30,
    "room": "K.3.401",
    "title": "Retro music - Open Cubic Player",
    "subtitle": "",
    "track": "Retrocomputing",
    "abstract": "<p>This presentation includes multiple topics mixed together\n * Porting a DOS multimedia program into Linux/BSD\n * How music was composed/stored in old type of games from the 8bit era, up into the amiga (and partly the DOS world)\n * How does audio/music hardware work. C64 SID, ZX-Spectrum and alike, Amiga, Adlib FM-synth OPL2/3 and General Midi</p>",
    "description": "<p>As a child, I was exposed to Cubic Player. This program was a text-based music player. All the colors mesmerized me and it gave a Direct visual feedback of how the music was generated.</p>\n\n<p><img src=\"http://content.pouet.net/files/screenshots/00015/00015501.gif\" alt=\"Cubic Player screenshot\" /></p>\n\n<p>During teenage years I learned programming and got curiousity for Linux. All the sourcecode for everything was available. The kernel, multimedia libraries, multimedia tools. If there are anything you wonder how works, you can inspect it. You are unable to sort how a specific detail can be done in your own project, try to find another project that has done something similiar! But for playing this amiga style modules, there was no programs that had the same charm as Cubic Player. Programs like mikmod, XMMS, Audacious only displayed playlist, maybe instrument-names and maybe an spectrum analyzer.\n<img src=\"http://mikmod.sourceforge.net/images/mikmod-3.2.2-dynsamp.png\" alt=\"http://mikmod.sourceforge.net/images/mikmod-3.2.2-dynsamp.png\" />\n<img src=\"http://soft.softoogle.com/pimg/1955.png\" alt=\"XMMS screenshot\" /></p>\n\n<p>Then I discovered that Cubic Player had been released as Open Cubic Player - but nobody had touched it. Releasing a project does not automatically make it work in other systems. But it makes it possible. I grabbed the source code and started to study it. All of it was based on direct hardware access, as it was written for DOS. Direct access to video-memory/hardware, raw keyboard scan codes, hardware interrupts for timers allowing background rendering of audio if needed etc. A natural candidate for a novice programmer to port?</p>\n\n<p>Slowly I went through one and one single file of the original source code.\n* Lots of logic could be kept (just needed type fixing like replacing byte with uint8_t, a char is not guaranteed to be unsigned)\n* Some could be thrown away like direct audio hardware drivers\n* Some needed heavy rewrites like video output. The code already was abstracted with having functions for mixing colors and text rendering.\n* Assembler inlines took many weeks - months (watcom c++ and gcc has VERY different syntax and integration)\n* The timer interrupt was solved using SIGTIMER. But porting the code caused a zero-day kernel bug to be discovered <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2004-0554\">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2004-0554</a>\n* Text output was originally solved using ncurses\n* Filenames are not 8.3 format in Linux/BSD</p>\n\n<p>When text appears on the screen, and audio (highly distorted in the beginning) appeared, was a magical feeling. Progress was faster when the goal felt to be well within the reach.</p>\n\n<p>When you the make your code public, people start to use it... and you start to get feedback like:\n* Program is unable to compile on other platforms - assembler inlines is not portable - even when compiling for x86_64 - got to make C-replacement for all of them - A person submitted the full rework of the floating point version of the mixer from assembler into C!!! WOW\n* Nothing works when compiling for MacOS - Wait, you tried what? Endian is important when parsing binary file formats\n* Specific crashes that only happens for some few people - compiler bugs (these were more common back in the days)</p>\n\n<hr />\n\n<p>Music formats time, and their integration!</p>\n\n<p>So, what is a amiga style module?</p>\n\n<p>Let's take a look at the Amiga hardware. The sound-chip there is a 4 channel wave-mixer. Each channel is assigned a PCM audio source, volume and a wide range of possible samplerates. These three parameters can be adjusted anytime.\nAn Amiga style module is then a set of audio PCM (8bit) samples - a sound font, and then 4 tracks recipe of how these 3 parameters should be manipulated. These can be small adjustements, big adjustments, slow or fast - All imitating different kind of effects from playing real life instruments.\nPortamento - sliding between seminotes - sliding the finger on the strings along the neck of the guitar\nTremolo - small vibration of the pitch - tremolo arm on an electric guitar\n...\nWhen this later moved into the computer world, the entire mixer had to be done in software, the number of channels was no longer limited by the hardware mixer.\n....\nVarious editors, players, file-format tools etc. all do things slighly different. The exact result for a given file will have variations, but the overall impression should be the same. The most important reason why music was done in this format was file sizes. A typical song would be in the range 10-100 kilobyte only, and the same mixer/playback-routines can be used for playing the game sound-effects. The complete code needed for rendering a given fileformat is not very large.</p>\n\n<p>So, what if we can ditch the soundfont in the files to save space. Welcome to General Midi. General Midi defines a set of instruments - actually a full orchestra and a bunch of synth style sounds. But it only defines what they are, not the exact sounds. An electric guitar is not just a guitar. And a MIDI file contains a set of \"note-on including attack\", \"note-off\" and \"select instrument\" events on a timeline.</p>\n\n<p>Open Cubic Player had support for General Midi, and parsing of soundfont in a single file format: UltraSound DOS driver fileformat. The few ones you found online were mostly propetiary - not very compatible with open source. So people kept demanding support for more formats. So I made a parser for the timidity config files; so far good enough. Then it kept a forever race and need for new features discovered in these configuration files - and even worse - depending on parsing binary fileformats that might contain the needed samples. Eventually - as the sole maintainer of the project - throw out the entire midi parser and renderer, and just use the entire Timidity+ project almost as-is. There is no point in re-inventing the wheel if you do not intend to improve it. So currently the MIDI renderer in Open Cubic Player is (a fork of) Timidity+.</p>\n\n<p>Same for the MP2/MP3 renderer that was using the original AMP engine. Instead of trying to fix it - use libmad!</p>\n\n<p>Ogg Vorbis files - libogg</p>\n\n<p>FLAC files - libflac</p>\n\n<p>And then users starts to request support for other fileformats - luckily, integration of them are not so hard when there is open source libraries and players that can be used.</p>\n\n<p>C64 .SID files - libsidplay</p>\n\n<p>ZX-Spectrum 128 .AY files - Code chopped out from \"aylet\"</p>\n\n<p>Atari ST .YM files - Using a fork of STYMulator</p>\n\n<p>OPL2/OPL3 style - libadplug</p>\n\n<p>AHX Tracker / Hively Tracker - Hively Tracker source contains a .wav file renderer</p>",
    "persons": [
      "Stian Sebastian Skjelstad"
    ]
  },
  {
    "start": 1577897100000,
    "duration": 30,
    "room": "K.3.401",
    "title": "Reviving Minitel",
    "subtitle": "How web technologies make it easy to emulate Minitel",
    "track": "Retrocomputing",
    "abstract": "<p>Using web technologies, one can easily emulate the Minitel ecosystem, not only the iconic french terminal but also its servers. This easiness has been made possible due to ES6 and websockets.</p>",
    "description": "<ul>\n<li>Brief history of the Minitel</li>\n<li>Minitel + VideoTex + X25 vs Browser + HTML + TCP/IP</li>\n<li>Technical aspects of Minitel emulation (ES6 + websockets)</li>\n<li>Creation of a VideoTex page (live demo)</li>\n<li>Surfing on Minitel (live demo)</li>\n</ul>",
    "persons": [
      "Frédéric Bisson"
    ]
  },
  {
    "start": 1577899200000,
    "duration": 30,
    "room": "K.3.401",
    "title": "Reverse engineering a VIC-20 expansion cartridge",
    "subtitle": "",
    "track": "Retrocomputing",
    "abstract": "<p>Going from seeing an image of a cartridge that I would have loved to have had for my VIC-20 to working out how it was built and then making my own.</p>",
    "description": "<p>Starting with a brief overview of the VIC-20 and the capabilities, to then looking at the \"vixen\" 16KiB RAM expansion, the initial view of the internals made available on the internet to going to a full implementation.</p>\n\n<p>The initial overview will go through some of the identification of the components, the technologies involved (such as SRAM and DRAM) and the VIC-20 expansion bus. It will show what sort of technologies could go into implementing this sort of expansion and then how these can be discounted either by the age of the technology or the component complexity.</p>\n\n<p>Techniques for further reversing a circuit from the physical device and how the actual device circuit was then discovered, followed by the implementation and testing will be shown.</p>",
    "persons": [
      "Ben Dooks"
    ]
  },
  {
    "start": 1577901300000,
    "duration": 30,
    "room": "K.3.401",
    "title": "Running a mainframe on your laptop (for fun and profit)",
    "subtitle": "",
    "track": "Retrocomputing",
    "abstract": "<p>Yes, this talk is about running your own mainframe on your own hardware. Mainframes are old, yes, but they are still very much alive. New hardware is still being developed and there are a lot of fresh jobs in this area too. A lot of mainframes run COBOL workloads. COBOL is far from a dead language. It processes an estimated 85% of all business transactions, and 5 billion lines of new COBOL code are written every year. In this session the speaker will help you in take your first steps towards running your own mainframe. If you like then after this session you can continue to build your knowledge of mainframe systems using the links provided during the talk. Come on in and learn the basics of a completely different computer system! And it will take you less than an hour to do that!</p>",
    "description": "",
    "persons": [
      "Jeroen Baten"
    ]
  },
  {
    "start": 1577903400000,
    "duration": 30,
    "room": "K.3.401",
    "title": "Arcade game port to ZX Spectrum",
    "subtitle": "A reverse engineering exercise",
    "track": "Retrocomputing",
    "abstract": "<p>Reverse engineering an Arcade game and re-implementing it into an 8 bit system is an engineering exercise, where compromises have to be made in order to accomplish the goal, since the capabilities of the target machine are severely under powered when compared with the source machine.</p>\n\n<p>The processes of accomplishing this and all it involves, will be presented.</p>",
    "description": "<p>Making an Arcade Game remake (reverse engineering) into an 8 bit system is an engineering exercise, where compromises have to be made in order to accomplish the goal, since the capabilities of the target machine are severely under powered when compared with the source machine.</p>\n\n<p>Starting with graphic capabilities, passing through CPU limitations (clock speed and architecture) and ending with multimedia capabilities, every single one, needs to be addressed with a suitable compromise.\nSome ingenious \"hacks\" and extreme optimization need to be applied, to use the 8 bit hardware capabilities in a convenient way to overcome the huge handy cap between architectures.</p>\n\n<p>For practical and example purposes, the reverse engineering of the Arcade game \"Magical Drop II\" will be presented, and how it became \"Extruder\" ZX Spectrum game.</p>\n\n<p>The software was developed in Zilog Z80 Assembly, and several tips and tricks will be shown, that facilitate and help the conversion process.</p>",
    "persons": [
      "Rui Martins"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 40,
    "room": "K.4.201",
    "title": "Debugging with LLVM",
    "subtitle": "A quick introduction to LLDB and LLVM sanitizers",
    "track": "LLVM",
    "abstract": "<p>The aim of this presentation is to showcase the technologies available in LLVM that aid debugging. We will focus on LLDB, the debugger, and sanitisers (e.g. AddressSanitizer and LeakSanitizer). No prior knowledge is required, but if you're familiar with GDB or Valgrind then this talk will introduce you to alternatives available within LLVM.</p>\n\n<p>LLDB is a very powerful and extensible command line debugger available on Linux, Mac OS, FreeBSD, Windows and Android. It is used internally in XCode and Android Studio and available on various hardware platforms (e.g. X86, ARM, AArch64, PowerPC, Mips). LLDB is built as a set of reusable components which highly leverage existing libraries in LLVM. It has a very powerful expression evaluation engine, intuitive CL interface (with tab-completion), easy to navigate help pages and a \"graphical\" user interface. In this presentation we will explore basic usage as well as some lesser known features. LLDB has come a long way and we want to present how intuitive, helpful and powerful it can be when used pragmatically.</p>\n\n<p>While LLDB will let you easily examine and debug a program at the point of failure, it can be harder to diagnose the underlying problem if it occurred before the program crashed or printed an incorrect result. LLVM provides some extra features in the form of 'sanitizers' to help find the root cause of some extra problems, like accessing a wrong-but-still-valid memory address or unintentionally wrapping a signed integer value. This presentation will explore how to use the sanitizers to debug programs and some examples of bugs they can catch.</p>",
    "description": "<p>TITLE: <em>Debugging with LLVM</em></p>\n\n<p>The aim of this presentation is to showcase the technologies available in LLVM that aid debugging. We will focus on LLDB, the debugger, and sanitisers (e.g. AddressSanitizer and LeakSanitizer). No prior knowledge is required, but if you're familiar with GDB or Valgrind then this talk will introduce you to alternatives available within LLVM.</p>\n\n<p>LLDB is a very powerful and extensible command line debugger available on Linux, Mac OS, FreeBSD, Windows and Android. It is used internally in XCode and Android Studio and available on various hardware platforms (e.g. X86, ARM, AArch64, PowerPC, Mips). LLDB is built as a set of reusable components which highly leverage existing libraries in LLVM. It has a very powerful expression evaluation engine, intuitive CL interface (with tab-completion), easy to navigate help pages and a \"graphical\" user interface. In this presentation we will explore basic usage as well as some lesser known features. LLDB has come a long way and we want to present how intuitive, helpful and powerful it can be when used pragmatically.</p>\n\n<p>While LLDB will let you easily examine and debug a program at the point of failure, it can be harder to diagnose the underlying problem if it occurred before the program crashed or printed an incorrect result. LLVM provides some extra features in the form of 'sanitizers' to help find the root cause of some extra problems, like accessing a wrong-but-still-valid memory address or unintentionally wrapping a signed integer value. This presentation will explore how to use the sanitizers to debug programs and some examples of bugs they can catch.</p>",
    "persons": [
      "Andrzej Warzynski"
    ]
  },
  {
    "start": 1577877300000,
    "duration": 40,
    "room": "K.4.201",
    "title": "Benchmarking LLVM using Embench",
    "subtitle": "",
    "track": "LLVM",
    "abstract": "<p>Dhrystone and Coremark have been the defacto standard microcontroller benchmark suites for the last thirty years, but these benchmarks no longer reflect the needs of modern embedded systems. Embench™ was explicitly designed to meet the requirements of modern connected embedded systems. The benchmarks are free, relevant, portable, and well implemented.</p>\n\n<p>In this talk we will present the results of benchmarking Clang/LLVM for various IoT class architectures using Embench. We shall look at\n- how code size and speed varies across architectures when compiling with Clang/LLVM.\n- how Clang/LLVM performance has evolved over time\n- how Clang/LLVM compares against other compilers, notably GCC\n- the effectiveness of various compilation techniques (LTO, Combined Elimination, Profile Guided Optimization)</p>\n\n<p>The aim is not to show which architecture or compiler is best, but to gain insight into the detail of the compilation process, so that all compilers and architectures can learn from each other.</p>",
    "description": "",
    "persons": [
      "Jeremy Bennett"
    ]
  },
  {
    "start": 1577880600000,
    "duration": 40,
    "room": "K.4.201",
    "title": "Confronting Clang and Fedora",
    "subtitle": "",
    "track": "LLVM",
    "abstract": "<p>GCC is the default toolchain to build C/C++ packages in Fedora. Meanwhile OpenMandrivia already builds most of its package with the LLVM toolchain, the Debian archive is regularly rebuilt with with a recent verion of clang... So could we try that for Fedora?</p>\n\n<p>This talk describes an on-going effort to achieve that goal while keeping the same compiler feature set as GCC.</p>",
    "description": "<p>Subtopics of the talk include:</p>\n\n<ul>\n<li>Creating a clang-based buildroot and use it to rebuild the fedora user-land (and more?)</li>\n<li>Support for missing security features such as -fstack-clash-protection or -D<em>FORTIFY</em>SOURCE=2</li>\n<li>Add missing features for the annobin tool within lld</li>\n<li>Python3 compatibity</li>\n<li>Package size</li>\n</ul>",
    "persons": [
      "Serge Guelton (serge-sans-paille)"
    ]
  },
  {
    "start": 1577883300000,
    "duration": 40,
    "room": "K.4.201",
    "title": "LLVM and GCC",
    "subtitle": "Learning to work together",
    "track": "LLVM",
    "abstract": "<p>At the GNU Tools Cauldron we held a panel discussion on how GCC and LLVM can work together. The video of that discussion can be seen at https://www.youtube.com/watch?v=PnbJOSZXynA.</p>\n\n<p>We proposed a similar discussion to be held at the LLVM Developers Meeting, but the reviewers suggested that such a discussion would be better held as part of the FOSDEM LLVM Devroom, since that was more likely to attract GNU developers as well.</p>\n\n<p>We wish to explore how Clang/LLVM and the GCC can work together effectively.</p>\n\n<p>The participants will explore opportunities for co-operation between the projects. Areas to be covered include:\n- collaboration on issues related to language standards, changes to existing standards or implementing new ones;\n- maintaining ABI compatibility between the compilers;\n- interoperability between tools and libraries e.g. building with clang and libstdc++ or building with gcc and linking with lld; and\n- communication channels for developers via bugzilla or mailing lists.</p>\n\n<p>We hope the output of the discussion will inform future work between the two communities.</p>\n\n<p>The panelists are yet to be confirmed, but I have invited those who offered to participate at the LLVM Developer Room: Tom Stellard of Red Hat, Nathan Sidwell of Facebook, Iain Sandoe, now independent.  All have been major contributors to GCC and/or LLVM for many years.  I shall act as moderator.</p>",
    "description": "",
    "persons": [
      "Jeremy Bennett"
    ]
  },
  {
    "start": 1577886600000,
    "duration": 40,
    "room": "K.4.201",
    "title": "LLVM meets Code Property Graphs",
    "subtitle": "",
    "track": "LLVM",
    "abstract": "<p>The security of computer systems fundamentally depends on the quality of its underlying software. Despite a long series of research in academia and industry, security vulnerabilities regularly manifest in program code. Consequently, they remain one of the primary causes of security breaches today.\nThe discovery of software vulnerabilities is a classic yet challenging problem of the security domain. In the last decade, there appeared several production-graded solutions with a favorable outcome.</p>\n\n<p>Code Property Graph[1] (or CPG) is one such solution. CPG is a representation of a program that combines properties of abstract syntax trees, control flow graphs, and program dependence graphs in a joint data structure.\nThere exist two counterparts[2][3] that allow traversals over code property graphs in order to find vulnerabilities and to extract any other interesting properties.</p>\n\n<p>In this talk, we want to cover the following topics:</p>\n\n<ul>\n<li>an intro to the code property graphs</li>\n<li>how we built llvm2cpg, a tool that converts LLVM Bitcode to the CPG representation</li>\n<li>how we teach the tool to reason about properties of high-level languages (C/C++/ObjC) based on the low-level representation only</li>\n<li>interesting findings and some results</li>\n</ul>\n\n\n<p>[1] https://ieeexplore.ieee.org/document/6956589\n[2] https://github.com/ShiftLeftSecurity/codepropertygraph\n[3] https://ocular.shiftleft.io</p>",
    "description": "",
    "persons": [
      "Alex Denisov",
      "Fabian Yamaguchi"
    ]
  },
  {
    "start": 1577889300000,
    "duration": 40,
    "room": "K.4.201",
    "title": "LLVM and Python",
    "subtitle": "Past, Present, Future",
    "track": "LLVM",
    "abstract": "<p>Python with LLVM has at least one decade of history. This session will be going to cover-up how python implementations tried to use LLVM such as CPython's Unladen Swallow branch (PEP 3146) or attempts from PyPy and why they failed. After that it will show what are the current python projects that use LLVM for speed, such as numba and python libraries for working with LLVM IR. In the end, it will mention about new ideas that would unite the powers of both LLVM and Python.</p>",
    "description": "<p>This talk is about LLVM's influence over Python's ecosystem. It is targeted an audience of language developers who want to integrate LLVM and developers who are curious about why dont dynamic languages can unite their power with LLVM to speed-up. It will start with python's implementations and the approaches they take. The Unladen Swallow branch which basically tries to integrate LLVM to CPython (under google sponsored) is a good example of these approaches. There are attempts from the PyPy side but they are also failed because of the dynamic nature of Python. After this fails, we'll swap back to our current time and show projects that are benefiting from LLVM to speed up python especially on the scientific side such as numba (which offers JITting via LLVM). Besides these projects, there are also a few projects that offer an interface to LLVM. Such as llvmpy and llvmlite. I've been using llvmlite about 1 year in my side projects and toy languages so these projects has the potential to inspire developers to work with LLVM and build languages a-top on it. In the end, it will show what is the future of these two big projects (LLVM &amp; Python) and how we can participate.</p>",
    "persons": [
      "Batuhan Taşkaya"
    ]
  },
  {
    "start": 1577892000000,
    "duration": 40,
    "room": "K.4.201",
    "title": "Flang : The Fortran frontend of LLVM",
    "subtitle": "This technical talk introduces the new Fortran fronted of LLVM.",
    "track": "LLVM",
    "abstract": "<p>This talk introduces Flang (F18), the new Fortran frontend of LLVM being written in modern C++. The talk will provide a brief introduction to Flang, motivation for writing this new compiler, design principles, architecture, status, and an invitation to contribute.</p>",
    "description": "<p>F18 project started at PGI/Nvidia as a new Fortran frontend designed to work with LLVM. The aim of the project is to create a modern Fortran frontend (Fortran 2018 standard) in modern C++. In April of this year, it was accepted as an LLVM project (https://lists.llvm.org/pipermail/llvm-dev/2019-April/131703.html).</p>\n\n<p>The parser and semantic analysis are implemented in a way that provides a strong correspondence to the standards document. It is hoped that this correspondence will help in the development of new features and will become the testbed for deciding future Fortran standard features. The frontend also embraces the newly open-source MLIR framework for language-specific optimisations. This will be through a new dialect call FIR (https://www.youtube.com/watch?v=ff3ngdvUang). MLIR will also be used for creating an OpenMP dialect. The project also hopes to share code with the Clang frontend. While the parser/AST will not be shared, code will be shared in the Driver, OpenMP codegen etc.</p>\n\n<p>In this presentation, we hope to cover the technical details mentioned in the paragraph above, the status of implementation and also give an invitation to contribute.</p>",
    "persons": [
      "Kiran Chandramohan"
    ]
  },
  {
    "start": 1577894700000,
    "duration": 40,
    "room": "K.4.201",
    "title": "Ask LLVM developers Anything Panel",
    "subtitle": "",
    "track": "LLVM",
    "abstract": "<p>Ever wondered how the LLVM project and community works?\nWant to get some advice on how to most effectively contribute?\nNow is your chance at FOSDEM to ask experienced developers directly.  This\npanel will host a number of experienced LLVM developers, answering any\nrelevant questions from the audience.</p>",
    "description": "<p>In case questions from the audience do not fill the entire slot, a set of\nprepared questions focussed on how to get started working with and contributing\nto LLVM will be raised for the panelists to answer.</p>\n\n<p>The panel will consist of at least the following 2 experienced contributors to\nLLVM, and will be expanded with other experienced contributors who are planning\nto come to the dev room:\n- Kristof Beyls\n- Peter Smith</p>",
    "persons": [
      "Kristof Beyls",
      "Peter Smith",
      "Nick Desaulniers"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 40,
    "room": "K.4.201",
    "title": "Automating Programming and Development of Heterogeneous SoCs with LLVM Tools",
    "subtitle": "",
    "track": "LLVM",
    "abstract": "<p>Historically, programming heterogeneous systems has been quite a challenge. While programming support for basic general-purpose accelerators such as GPUs has become quite mature in many ways, general heterogeneous SoCs in particular can feature a much broader range of accelerators in their efforts to minimize power consumption while maximizing performance. Many SoCs, though, are designed with accelerators tailored for the domain -- such as signal processing --  in which they’ll be used: Domain-Specific SoCs. As SoC platforms become ever-more heterogeneous, we think that application developers shouldn’t need to waste time reading datasheets or APIs for SoC-specific kernel extensions just to take full advantage of their hardware. With this in mind, in this talk we will discuss strategies we are using to automate mapping of LLVM-compatible languages to heterogeneous platforms with no intervention (not even #pragmas) from the programmer.</p>\n\n<p>To this end, we present our prototype of a software stack that seeks to address both of these needs. To meet the first need, we developed an LLVM-based hybrid compile/run-time toolchain to extract the semantic operations being performed in a given application. With these semantic operations extracted, we can link in additional libraries that enable dispatch of certain kernels (such as a Fast Fourier Transform) to accelerators on the SoC without user intervention. To evaluate the functionality of this toolchain, we developed a runtime system built on top of QEMU+Linux that includes scheduling and task dispatch capabilities targeting hypothetical SoC configurations. This enables behavioral modeling of these accelerators before silicon (or even FPGA) implementations are available.  The focus here will be on the LLVM-mapping aspects, but a brief overview of our SoC simulation environment will be presented as well.</p>",
    "description": "",
    "persons": [
      "Joshua Mack",
      "Nirmal Kumbhare"
    ]
  },
  {
    "start": 1577900700000,
    "duration": 40,
    "room": "K.4.201",
    "title": "HPVM: Extending LLVM For Compiling to Heterogeneous Parallel Systems",
    "subtitle": "",
    "track": "LLVM",
    "abstract": "<p>TITLE: HPVM: Extending LLVM For Compiling to Heterogeneous Parallel Systems</p>\n\n<p>SPEAKER: Vikram Adve, University of Illinois at Urbana-Champaign</p>\n\n<p>Abstract:</p>\n\n<p>We will present a detailed description of HPVM, an extension to LLVM for\ncompiling to heterogeneous parallel systems.  HPVM aims to make it much\neasier to develop compilers for diverse parallel hardware, and to implement\nparallel languages (including domain-specific languages) for such hardware.\nWe will briefly describe at a high-level the key parallel abstraction of\nhierarchical dataflow graphs used in HPVM, and then focus on on how HPVM is\nintegrated on top of LLVM.  A second part of the talk will briefly describe\nhow we are extending HPVM to enable greater energy efficiency and\nperformance by taking advantage of <em>approximation</em> opportunities in\napplication domains such as machine learning and image processing.  To\nconclude, I will briefly discuss how HPVM might be added as a dialect in\nMLIR so that other MLIR dialects and MLIR-based compilers can use HPVM for\ncode generation to diverse heterogeneous hardware targets, including GPUs,\nFPGAs, and custom accelerators.</p>",
    "description": "<p>TITLE: HPVM: Extending LLVM For Compiling to Heterogeneous Parallel Systems</p>\n\n<p>SPEAKER: Vikram Adve, University of Illinois at Urbana-Champaign</p>\n\n<p>Background</p>\n\n<p>LLVM has been extraordinarily successful as a compiler infrastructure for\nenabling a wide range of compilers and compiler-based tools for scalar and\nvector processors, and for supporting GPU compilers for OpenCL and CUDA.\nLLVM has seen only limited use, however, for other classes of target\narchitectures, such as reconfigurable hardware (FPGAs) and domain-specific\naccelerators such as for machine learning, image processing, signal\nprocessing, graph processing, and other emerging domains.  More generally,\nheterogeneous system-on-chip (SoC) architectures are becoming increasingly\nimportant, especially in \"edge computing,\" but LLVM has largely been\nlimited to the host CPU and GPU on such SoCs, even though the number of\nother programmable components on these systems has been steadily increasing.</p>\n\n<p>Overview</p>\n\n<p>In this talk, I will describe an extension of LLVM for developing a compiler\ninfrastructure -- Heterogeneous Parallel Virtual Machine, or HPVM -- for\nheterogeneous parallel systems [1].  I will briefly describe at a high-level\nthe key parallel abstraction of hierarchical dataflow graphs used in HPVM to\ndescribe heterogeneous parallelism, where ordinary LLVM code is used to\nrepresent the computatational tasks.  The main focus of this part of the\ntalk is how HPVM is integrated on top of LLVM.  First, HPVM has been\nimplemented as a set of intrinsic functions that extend the LLVM\ninfrastructure.  Second, the HPVM code generation framework reuses existing\nLLVM (and other) back-ends, in order to leverage existing (often well-tuned)\ncode generators for individual programmable hardware elements, such as NVPTX\nfor NVIDIA GPUs, Intel's SPIR-V code generator for Intel SSE and AVX vector\nhardware, and Altera's AOCL compiler for targeting Altera's FPGAs.</p>\n\n<p>A second part of the talk will briefly describe how we are extending\nHPVM to enable greater energy efficiency and performance by taking\nadvantage of <em>approximation</em> opportunities in application domains such\nas machine learning and image processing.  In particular, we are\ncurrently developing ApproxHPVM, an extension of HPVM that supports a\nrange of algorithmic and hardware-level approximation mechanisms [2].\nMoreover, ApproxHPVM only requires application programmers to specify\nhigh-level, \"end-to-end\" design goals such as the maximum allowable\naccuracy loss in a neural network or loss of image quality (e.g.,\nPSNR) and the system automatically selects, optimizes and maps\napproximation choices for individual coarse-grain tensor operations in\nthe application.  The goal is to make sophisticated and well-tested\napproximation techniques widely accessible to application developers.</p>\n\n<p>To conclude, I will briefly discuss how HPVM and ApproxHPVM might be added\nas a dialect in MLIR so that other MLIR dialects and MLIR-based compilers\ncan use HPVM for diverse heterogeneous hardware targets, including GPUs,\nFPGAs, and custom accelerators.</p>\n\n<p>Target Audience</p>\n\n<p>The intended target audience for this talk falls into broadly two classes.\nThe first includes compiler practitioners and researchers interested in\ncompiling to heterogeneous systems, such as SoCs, FPGAs, and other\n\"edge-compute\" hardware.  The second includes language implementers\ninterested in implementing or porting domain-specific languages such as\nTensorFlow, Halide, SPIRAL, and others to heterogeneous parallel systems.</p>\n\n<p>Takeaways</p>\n\n<p>We envision several takeaways for the audience: (1) Understand how to\ndevelop an extension of LLVM that makes it easier to target emerging\nhardware platforms not sufficiently well-supported by the existing LLVM IR\nand code generation framework. (2) Expose attendees to the opportunities and\nchallenges in supporting and reasoning about approximate computations in a\ncompiler framework. (3) Discuss the opportunities and limitations of using\nHPVM for supporting heterogeneous parallel systems in the context of MLIR.</p>\n\n<p>Web Site and Software Availability</p>\n\n<p>More information about HPVM is available at http://hpvm.cs.illinois.edu/.\nThe HPVM infrastructure is implemented as an extension to LLVM.  To date,\nthe software is being developed using an internal Git repository at Illinois\nand has been shared with collaborators at IBM and at Harvard University.\nWe will make it available publicly in open-source form on Github before the\nFOSDEM conference.</p>\n\n<p>REFERENCES</p>\n\n<p>[1] Maria Kotsifakou, Prakalp Srivastava, Matthew D. Sinclair,\nRakesh Komuravelli, Vikram S. Adve and Sarita V. Adve, “HPVM:\nHeterogeneous Parallel Virtual Machine.” Proceedings of Principles and\nPractice of Parallel Programming (PPoPP), Feb 2018, Vösendorf / Wien,\nAustria.</p>\n\n<p>[2] Hashim Sharif, Prakalp Srivastava, Mohammed Huzaifa, Maria\nKotsifakou, Yasmin Sarita, Nathan Zhou, Keyur Joshi, Vikram S. Adve,\nSasa Misailovic and Sarita V. Adve, “ApproxHPVM: A Portable Compiler\nIR for Accuracy-aware Optimizations,” OOPSLA 2019, October 2019,\nAthens, Greece.</p>",
    "persons": [
      "Vikram Adve"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 25,
    "room": "K.4.401",
    "title": "libliftoff status update",
    "subtitle": "Taking advantage of KMS planes",
    "track": "Graphics",
    "abstract": "<p>This talk will explain some basics about KMS, introduce libliftoff, describe the library's current status and outline the next steps.</p>",
    "description": "<p>Many DRM drivers have been exposing overlay planes for quite some time. Overlay planes can improve battery consumption by scanning out directly client buffers, skipping composition. While Wayland compositors and the X server usually take advantage of the cursor plane (and sometimes are able to use the primary plane to directly scan out a client's buffer), overlay planes are under-used. The exception is Weston, which tries to use overlay planes. Other compositors ignore them.</p>\n\n<p>The main challenge is to figure out how to assign buffers coming from clients to hardware planes. The only API exposed by KMS is atomic test commits, so user-space needs to try different combinations. It would be nice to have a common library shared between compositors to de-duplicate the work.</p>\n\n<p>During the XDC 2019 conference we discussed about <a href=\"https://github.com/emersion/libliftoff\">libliftoff</a>, an attempt at designing such a library. Feedback was positive from both compositor writers and driver developers. We discussed about the API, the potential pitfalls and future goals. The scope of the library has been expanded: libliftoff could also provide some feedback to clients so that they allocate buffers suitable for hardware planes. Additionally, because the KMS API makes it tricky to find the best way to make use of hardware planes, libliftoff could grow some vendor-specific plugins.</p>",
    "persons": [
      "Simon Ser"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 25,
    "room": "K.4.401",
    "title": "Zink Update",
    "subtitle": "OpenGL on Vulkan upstream in mesa",
    "track": "Graphics",
    "abstract": "<p>A short update on the state of Zink, and OpenGL implementation on top of vulkan, now that it's upstream in Mesa.</p>",
    "description": "",
    "persons": [
      "Erik Faye-Lund"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 55,
    "room": "K.4.401",
    "title": "Modernizing mesa3d.org",
    "subtitle": "Let's bring mesa3d.org past web 1.0",
    "track": "Graphics",
    "abstract": "<p>mesa3d.org is stuck on web 1.0 technology, but let's see what we can do about it.</p>",
    "description": "<p>This is a Birds-Of-a-Feather session, which starts off with a short presentation as an introduction about the current state of affairs.</p>",
    "persons": [
      "Erik Faye-Lund"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 55,
    "room": "K.4.401",
    "title": "Enabling 8K displays",
    "subtitle": "A story of 33M pixels, 2 CRTCs and no Tears!",
    "track": "Graphics",
    "abstract": "<p>This talk will cover enabling 8k display by tying 2 display pipelines together over 2 displayport connections.</p>",
    "description": "<p>Ever seen a true 33 million pixel 8K display? The maximum display link bandwidth available with DisplayPort’s highest bit rate of 8.1 Gbps/lane limits the resolution to 5K@60 over a single DP connector. Hence the only true 8K displays allowing up to full 60 frames per second are the tiled displays enabled using 2 DP connectors running at their highest bit rate across 2 CRTCs in the display graphics pipeline. Enabling tiled displays across dual CRTC dual connector configuration has always resulted in screen tearing artifacts due to synchronization issues between the two tiles and their vertical blanking interrupts.</p>\n\n<p>Transcoder port synchronization is a new feature supported on Intel’s Linux Graphics kernel driver for platforms starting Gen 11 that fixes the tearing issue on tiled displays. In this talk Manasi will explain how port synchronization is plumbed into the existing atomic KMS implementation. She will deep dive into the DRM and i915 code changes required to handle tiled atomic modesets through master and slave CRTCs lockstep mode operation to enable tearfree 8K display output across 2 CRTCs and 2 ports in the graphics pipeline. She will conclude by showing the 8K display results using Intel GPU Tools test suite.</p>",
    "persons": [
      "Manasi Navare"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 55,
    "room": "K.4.401",
    "title": "FOSS Virtual & Augmented Reality",
    "subtitle": "The Monado project & OpenXR",
    "track": "Graphics",
    "abstract": "<p>In this talk will cover Monado and Khronos' OpenXR standard, and give an overview about the current state of open source VR and what lies ahead. Also go into some details of how tracking is done inside of Monado and show of the current state.</p>",
    "description": "<p>VR took off for the consumer with the release of Oculus consumer hardware. But the hardware lacked open source drivers and Linux support in general. The consumer VR space has now grown from a kickstarter campaign into a large industry. But this growth has its down sides, multiple companies have their own APIs competing. Luckily these companies have agreed to work on a single API under the Khronos umbrella. Now that OpenXR has been released and and the Monado project has been getting more stable it is now possible to do good VR on a completely open stack.</p>",
    "persons": [
      "Jakob Bornecrantz"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 55,
    "room": "K.4.401",
    "title": "Back to the Linux Framebuffer!",
    "subtitle": "Linux Framebuffer support in free software",
    "track": "Graphics",
    "abstract": "<p>Although KMS/DRM can replace the Linux Framebuffer, there are a number of programs and libraries that can be built on top of the Linux Framebuffer (without X11 or Wayland dependencies) and that might still be worth considering. The Linux Framebuffer allows direct access to pixels: we will illustrate it with various rendering tools (Fbpad, Fbi, NetSurf, MPlayer, ...), but also with drawing libraries such as Cairo or Evas, and multimedia frameworks like FFmpeg or GStreamer.\nThe Mesa 3D project makes OpenGL rendering possible using only the Linux Framebuffer with GLFBDev or EGL: mesa-demos and yagears programs will be shown.\nWe will then cover graphics libraries (GLUT, SDL, EFL, GTK, Qt) that allow to integrate high level applications running directly on top of the Linux Framebuffer with no compositor. An example will be described using either WebKitGTK or QtWebKit for the rendering of a HTML5 media player and a WebGL sample, using the Linux Framebuffer port of those libraries and toolkits.\nThis talk is inspired by the HiGFXback project which aims at preserving historical backends used for graphics on GNU/Linux systems.</p>",
    "description": "",
    "persons": [
      "Nicolas Caramelli"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 55,
    "room": "K.4.401",
    "title": "The TTM memory manager",
    "subtitle": "A general overview and an update on graphics memory  management in the kernel",
    "track": "Graphics",
    "abstract": "<p>TTM is the memory manager in the Linux kernel used by graphics drivers with dedicated VRAM.</p>\n\n<p>It was added to the mainline kernel in June 2009 and has seen numerous changes and we are now more or less running into a dead-end with it's design.</p>\n\n<p>This talk outlines TTMs current functionality, what design problems we ran into and what can we do to fix this.</p>",
    "description": "",
    "persons": [
      "Christian König"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 55,
    "room": "K.4.401",
    "title": "Pattern Based Code Generation for GPUs",
    "subtitle": "",
    "track": "Graphics",
    "abstract": "<p>Automatic, pattern-based code generation for Mesa's compiler infrastructure has been a long standing dream. Nearly a decade ago experiments were conducted using systems like BURS and lburg. Each of these attempts encountered various insurmountable road blocks. In the intervening years, both software and GPU architectures have changed significantly. These changes have enabled a code-generator generator to be a reality. The design and implementation of one system will be presented. In addition to the successes, various difficulties and rough edges will be detailed.</p>",
    "description": "",
    "persons": [
      "Ian Romanick"
    ]
  },
  {
    "start": 1577901600000,
    "duration": 25,
    "room": "K.4.401",
    "title": "A Vulkan driver for the RPI4",
    "subtitle": "A lesson in futility",
    "track": "Graphics",
    "abstract": "<p>For the Raspberry PI 4 I started implementing a Vulkan driver. This talk will give a guide of how to approach such a task, what my expectations are and what I learned so far.</p>",
    "description": "<p>With the release of Raspberry PI 4 it becomes theoretically more viable to use it in GPU heavy scenarios. Even ordinary software like Gnome Shell, Chromium and games fall into that category.\nSadly, neither Broadcom nor Raspberry PI Foundation currently provide a Vulkan driver. Since I want as much performance (and little overheating) as possible, I started writing a Vulkan driver.\nThis entails learning kernel and mesa internals as well as trying to understand Gallium. All that I have learned so far, I will try to share in this talk.</p>",
    "persons": [
      "Andreas Bergmeier"
    ]
  },
  {
    "start": 1577903400000,
    "duration": 25,
    "room": "K.4.401",
    "title": "libratbag",
    "subtitle": "A way to configure your input devices",
    "track": "Graphics",
    "abstract": "<p>This talk will give an update on the progress being done in libratbag as well as present the new projects we have planned. If there's time I will also show how you should be able to write your own driver and debug existing drivers. This is a talk about libratbag updates, planned projects and a code demo.</p>",
    "description": "",
    "persons": [
      "Filipe Laíns"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 10,
    "room": "K.4.601",
    "title": "Openning",
    "subtitle": "",
    "track": "Hardware-aided Trusted Computing",
    "abstract": "<p>abstract</p>",
    "description": "<p>description</p>",
    "persons": [
      "Vasily A. Sartakov"
    ]
  },
  {
    "start": 1577875200000,
    "duration": 30,
    "room": "K.4.601",
    "title": "Introduction to the CoSMIX Compiler",
    "subtitle": "Compiler-based techniques for secure memory instrumentation in enclaves",
    "track": "Hardware-aided Trusted Computing",
    "abstract": "<p>Hardware secure enclaves are increasingly used to run complex applications. Unfortunately, existing and emerging en- clave architectures do not allow secure and efficient implementation of custom page fault handlers. This limitation impedes in-enclave use of secure memory-mapped files and prevents extensions of the application memory layer commonly used in untrusted systems, such as transparent memory compression or access to remote memory.\nCoSMIX is a Compiler-based system for Secure Memory Instrumentation and eXecution of applications in secure enclaves. A novel memory store abstraction allows the implementation of application-level secure page fault handlers that are invoked by a lightweight enclave runtime. The CoSMIX compiler instruments the application memory accesses to use one or more memory stores, guided by a global instrumentation policy or code annotations without changing application code.\nThe CoSMIX prototype runs on Intel SGX and is compatible with popular SGX execution environments, including SCONE and Graphene. Our evaluation of several production applications shows how CoSMIX improves their security and performance by recompiling them with appropriate memory stores. For example, unmodified Redis and Memcached key-value stores achieve about 2× speedup by using a self-paging memory store while working on datasets up to 6× larger than the enclave’s secure memory. Similarly, annotating a single line of code in a biometric verification server changes it to store its sensitive data in Oblivious RAM and makes it resilient against SGX side-channel attacks.</p>",
    "description": "",
    "persons": [
      "Yan Michalevsky"
    ]
  },
  {
    "start": 1577877300000,
    "duration": 30,
    "room": "K.4.601",
    "title": "Be secure with Rust & Intel SGX",
    "subtitle": "",
    "track": "Hardware-aided Trusted Computing",
    "abstract": "<p>Intel Software Guard Extensions (SGX) makes software secure from the outside. Rust makes it secure from the inside. This workshop will introduce you to Rust and the Fortanix® Enclave Development Platform (EDP) for Rust: how it works, what you can do with it, and why Rust is such a good fit for SGX.</p>",
    "description": "",
    "persons": [
      "Jethro G. Beekman"
    ]
  },
  {
    "start": 1577879400000,
    "duration": 30,
    "room": "K.4.601",
    "title": "The Confidential Consortium Framework",
    "subtitle": "A framework to build secure, highly available, and performant applications that focus on multi-party compute and data",
    "track": "Hardware-aided Trusted Computing",
    "abstract": "<p>The Confidential Consortium Framework is an open-source framework for building permissioned confidential multi-party services. It leverages hardware trusted execution environments to provide strong confidentiality, integrity, and high performance. CCF implements consortium-based programmable and auditable governance mechanism.</p>",
    "description": "",
    "persons": [
      "Amaury Chamayou"
    ]
  },
  {
    "start": 1577881500000,
    "duration": 30,
    "room": "K.4.601",
    "title": "EActors: an actor-based programming framework for Intel SGX",
    "subtitle": "",
    "track": "Hardware-aided Trusted Computing",
    "abstract": "<p>In this talk I will present EActors, an actor framework that is tailored to SGX and offers a more seamless, flexible and efficient use of trusted execution – especially for applications demanding multiple enclaves. EActors disentangles the interaction with enclaves and, among them, from costly execution mode transitions. It features lightweight fine-grained parallelism based on the concept of actors, thereby avoiding costly SGX SDK provided synchronisation constructs. Finally, EActors offers a high degree of freedom to execute actors, either untrusted or trusted, depending on security requirements and performance demands.</p>",
    "description": "",
    "persons": [
      "Vasily A. Sartakov"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 30,
    "room": "K.4.601",
    "title": "A Tale of Two Worlds: Assessing the Vulnerability of Enclave Shielding Runtimes",
    "subtitle": "",
    "track": "Hardware-aided Trusted Computing",
    "abstract": "<p>This talk analyzes the vulnerability space arising in Trusted\nExecution Environments (TEEs) when interfacing a trusted enclave\napplication with untrusted, potentially malicious code. Considerable\nresearch and industry effort has gone into developing TEE runtime\nlibraries with the purpose of transparently shielding enclave\napplication code from an adversarial environment. However, our analysis\nreveals that shielding requirements are generally not well-understood in\nreal-world TEE runtime implementations. We expose several sanitization\nvulnerabilities at the level of the Application Binary Interface (ABI)\nand the Application Programming Interface (API) that can lead to\nexploitable memory safety and side-channel vulnerabilities in the\ncompiled enclave. Mitigation of these vulnerabilities is not as simple\nas ensuring that pointers are outside enclave memory. In fact, we\ndemonstrate that state-of-the-art mitigation techniques such as Intel’s\nedger8r, Microsoft’s “deep copy marshalling”, or even memory-safe\nlanguages like Rust fail to fully eliminate this attack surface. Our\nanalysis reveals 35 enclave interface sanitization vulnerabilities in 8\nmajor open-source shielding frameworks for Intel SGX, RISC-V, and Sancus\nTEEs. We practically exploit these vulnerabilities in several attack\nscenarios to leak secret keys from the enclave or enable remote code\nreuse. We have responsibly disclosed our findings, leading to 5\ndesignated CVE records and numerous security patches in the vulnerable\nopen-source projects, including the Intel SGX-SDK, Microsoft Open\nEnclave, Google Asylo, and the Rust compiler.</p>",
    "description": "",
    "persons": [
      "Jo Van Bulck"
    ]
  },
  {
    "start": 1577885700000,
    "duration": 30,
    "room": "K.4.601",
    "title": "HOWTO build a product with OP-TEE",
    "subtitle": "",
    "track": "Hardware-aided Trusted Computing",
    "abstract": "<p>OP-TEE is an open source implementation of the GPD TEE specifications. However deploying OP-TEE inside\na real world product requires more than just the integration into the system, since the integrator needs\nto ensure that all security requirements are met. This talk will outline a common set of these requirements\nand show the necessary changes based on NXP i.MX6 platforms.</p>",
    "description": "",
    "persons": [
      "Rouven Czerwinski"
    ]
  },
  {
    "start": 1577887800000,
    "duration": 20,
    "room": "K.4.601",
    "title": "Demo: SGX-LKL",
    "subtitle": "Running unmodified Linux applications inside Intel SGX's enclaves",
    "track": "Hardware-aided Trusted Computing",
    "abstract": "<p>SGX-LKL is a library OS designed to run unmodified Linux binaries inside SGX enclaves. It uses the Linux Kernel Library (LKL) and a modified version of musl to provide system support for complex applications within the enclave.  SGX-LKL has support for in-enclave user-level threading, signal handling, and paging. This demo presents an overview of SGX-LKL and demonstrates how popular applications can be ported and executed within SGX-LKL.</p>",
    "description": "",
    "persons": [
      "Thiago Zagatti"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 25,
    "room": "K.4.601",
    "title": "Open source UEFI and TianoCore",
    "subtitle": "",
    "track": "Open Source Firmware, BMC and Bootloader",
    "abstract": "<p>Historically, the UEFI forum has been a bit rubbish at interacting with open source development, but this is improving.</p>\n\n<p>This talk gives a background on <em>why</em> (both the rubbish and the improvement) and what is being done.</p>\n\n<p>Also, a brief update on news for the TianoCore/EDK2 project.</p>",
    "description": "<p>After much lawyerly fun, the UEFI Self-Certification Testsuite (SCT) was released under an OSI license (BSD2) at the end of 2018.\nWe will explain why this is useful, and how this has helped with the addition of UEFI support in U-Boot, as well as helped improving EDK2 code quality.</p>\n\n<p>We have a new process, referred to as \"code first\" for drafting changes to the UEFI spefication in public.\nThis will give an overview of the process, as well as one of the first exercises of it - the definition of audio APIs (for accessibility or, you know, DOOM).</p>\n\n<p>Also, an update on changes in the EDK2 reference implementation, including dropping the CLA and upcoming RISC-V support.</p>",
    "persons": [
      "Leif Lindholm"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 25,
    "room": "K.4.601",
    "title": "Discover UEFI with U-Boot",
    "subtitle": "",
    "track": "Open Source Firmware, BMC and Bootloader",
    "abstract": "<p>The Unified Extensible Firmware Interface (UEFI) is the default for booting most Linux and BSD distributions. But the complexity of the UEFI standard does not offer an easy entry point for new developers. The U-Boot firmware provides a lightweight UEFI implementation. Using booting from iSCSI with U-Boot and iPXE as an example let's delve into the UEFI API.</p>\n\n<p>The UEFI sub-system in U-Boot has developed from barely starting GRUB to supporting complex UEFI applications like iPXE and the EFI shell and passing most of the UEFI compliance tests for the implemented protocols and services.</p>\n\n<p>The session gives an overview of the boottime and runtime services of UEFI with a focus on driver binding. The challenges of integrating the UEFI subsystem with U-Boot's infrastructure are described and an outlook is provided.</p>\n\n<p>Questions this talk should answer:\n- How does the UEFI driver model work?\n- How does this integrate with U-Boot?\n- What to expect next in U-Boot's UEFI implementation?</p>",
    "description": "",
    "persons": [
      "Heinrich Schuchardt"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 25,
    "room": "K.4.601",
    "title": "Heads OEM device ownership/reownership : A tamper evident approach to remote integrity attestation",
    "subtitle": "Current status and future plan : A call for collaboration",
    "track": "Open Source Firmware, BMC and Bootloader",
    "abstract": "<p>Insurgo had engaged itself in the adventure of facilitating security accessibility and received NlNet funding to do exactly that. Now it wants to get developers involved and expand funding.</p>\n\n<p>The goal of this is to bridge the gap between reasonably secure OS (QubesOS) and slightly more secure hardware (Heads) to help privacy-focused users and those that are vulnerable. But we need to prepare for the future now!</p>\n\n<p>Insurgo has challenged the status quo that has been prevalent since 2015 and has made it possible for OEMs to preinstall QubesOS, thanks to the Heads Open Source Firmware (OSF) and his own PrivacyBeast QubesOS certified branch, not yet merged upstream, due to the lack of time and resources of a single man effort needing additional collaboration.</p>\n\n<p>The integrity of the firmware and boot files is already remotely sealed and can be attested over smartphone (TPMTOTP) and from the bundled Librem Keys/Nitrokey Pro 2 (HOTP), prior to shipping. Thanks to HOTP-enabled USB security dongles bounded to shipped products, the user can visually validate that the hardware they've received is in OEM attested state, prior to complete reownership which is regenerating all required secrets from a trustable recovery environment (Heads OSF) thanks to a re-ownership wizard that guides the user until completion.</p>\n\n<p>This is just the beginning of the adventure and the road ahead requires your help. Insurgo wants to propel this movement forward.</p>\n\n<p>Today's secure hardware (REAL open source initialized hardware, eg. the RYF KGPE-D16, replicant supported phones, Sandy bridge/Ivy bridge based boards, eg. x230) struggle to stay current with upstream code and compliance requirements. LineageOS dropped support of the i9300. Coreboot dropped support of the KGPE-D16 platform. And the list will expand if no measures are taken to support maintainership of privacy focused projects that are taken for granted until support is finally dropped. This is a real problem requiring real solutions.</p>\n\n<p>New efforts to support future, REAL Open Source Hardware (newly Respect Your Freedom [RYF] certified hardware, eg. Talos II from RaptorEngineering, future Power10 based hardware) are neither currently under active development nor currently supported by QubesOS. This needs to change. Now.</p>\n\n<p>There is an opportunity for transition. This requires leadership, developers and funding.\nThis is why we've created the Insurgo Initiative on the OpenCollective platform.</p>\n\n<p>This is where transparent funding will be available to the public for open source R&amp;D. Please consider participating through code contributions!</p>",
    "description": "<p>Insurgo is making today's most trustworthy hardware available (TRUELY Neutered+Deactivated Intel ME, no FSP, no binary blobs whatsoever but EC firmware in the Root of Trust) to the masses through remote attestation over Heads OSF.</p>\n\n<p>NlNet is helping Heads to be compatible on the T530, T430, T420 and X220, which are widely available, binary blob-free hardware platforms, thanks to a partnership with 9element under NlNet grant.\nNlNet funds is also permitting development of remote administration of QubesOS over tor hidden services when needed, thanks to an ongoing partnership with both the Qubes OS Project &amp; Whonix.</p>\n\n<p>But what about other work needed to ease accessibility of tomorrow's secure hardware and technologies?</p>\n\n<p>Insurgo decided to give back to Open Source Firmware (OSF) related communities and will publicly announce novel approach to support required open source projects.\nIn premiere, we plan to give back 25% of Insurgo's net profit on sales to the Insurgo Initiative, hosted on OpenCollective.</p>\n\n<p>Those funds will be available to Open Source projects in the form of bounties, to be paid out upon proof of work of agreed contributions.</p>\n\n<p>The idea here is that open source tickets (issues) can be used as bounties and if knowledgeable people knew funds were available for needed work, they'd be more incentivized to address them.\nDevelopers could then be rewarded for their efforts and paid for completing tasks similiar to how Open Source Funds (OpenTech, NlNet, etc) provides funds for larger projects.</p>\n\n<p>The Insurgo Initiative will be self funded and potentially expanded through international partnerships, while the goal stays the same: supporting a future where security is more accessible to the public.</p>\n\n<p>Here are some projects needing additional funding and more developer awareness, right now. Big funds and grant application are great. But the funding process has issues.\nNot every developer wants to go through the application process, which requires management skills and requires a process that is not just about coding.\nThere are awesome developers out there whose help would be greatly needed.</p>\n\n<p>How do we appropriately match developers with pertinent issues? We can fix this with the right mission and funding.\nInsurgo's mission is for accessible security.</p>\n\n<p>Bounty tags are being added to projects that lack the funding and to help address the current problems they face for completion:\n-   PPC64le QubesOS support for upcoming Power10 laptop and Talos II RYF hardware: https://github.com/QubesOS/qubes-issues/issues/4318#issuecomment-547487583\n-   Heads needs more community developers and maintainers: https://github.com/osresearch/heads/issues\n-   QubesOS bounty tagged issues: https://github.com/QubesOS/qubes-issues/labels/bounty\n-   Whonix needs more collaborators or it might die: https://forums.whonix.org/t/focus-on-whonix-core-development/5036</p>\n\n<p>The main problem we seem to face with many projects can be seen over and over again: a lack of maintainership.</p>\n\n<p>No one can carry on a project for too long without becoming overwhelmed/drained by it.\nWe need to fairly distribute this work and make sure contributions are incentivized and fairly paid.</p>\n\n<p>In this talk, I will go quickly over past work. The current situation. And where Insurgo wants to go.</p>\n\n<p>Welcome aboard!</p>",
    "persons": [
      "Thierry Laurion"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 25,
    "room": "K.4.601",
    "title": "Improving the Security of Edge Computing Services",
    "subtitle": "Update status of the support for AMD processors",
    "track": "Open Source Firmware, BMC and Bootloader",
    "abstract": "<p>For the last several years, hypervisors have played a key role in platform\nsecurity by reducing the possible attack surface. At the same time, the hype\nsurrounding computing and Internet of Things Gateways has led to an increase in\nnetwork appliance devices. Our target was to create a less-insecure virtual\nnetwork appliance using TrenchBoot, Trusted Platform Module 2.0 and AMD SKINIT\nDynamic Root of Trust for Measurement to establish a Xen hypervisor with a\nmeta-virtualized pfSense firewall. We are going to present it with an update\nof the status of support of TrenchBoot for AMD processors.\nThis appliance is supported by are supported by apu2, a reliable low-SWaP x86\ndevice from Swiss OEM PC Engines. It can be used as a Single Office / Home\nOffice firewall or an industrial edge device and has mostly open-source\nhardware, coreboot firmware, mPCIe extensibility and an extended support\nlifecycle for the embedded Central Processing Unit and motherboard.\nIn this talk, we will show how to create a system, which enables a significant\nportion of computations to the edge devices while maintaining security. Using\na simple, well-known platform, we will conduct a secure boot using the Static\nRoot of Trust for Measurement with coreboot, move to the Dynamic Root of Trust\nfor Measurement by SKINIT in TrenchBoot and use all of this to provide a\ncomplete chain of trust for the Xen hypervisor, a virtual firewall appliance\nisolated by an input–output memory management unit (IOMMU) from the physical\nnetwork interface controller (NIC) devices. We will present benchmark data\non virtualization overhead, explain how this complexity can still be practical\nand outline the value of this stack.</p>",
    "description": "",
    "persons": [
      "Daniel Kiper",
      "Piotr Król"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 25,
    "room": "K.4.601",
    "title": "Introducing AUTOREV",
    "subtitle": "An automatic reverse-engineering framework for firmware BLOBs",
    "track": "Open Source Firmware, BMC and Bootloader",
    "abstract": "<p>Modern Open Source boot firmware ships with an increasing amount of BLOBs. While it's often claimed that it eases the integration,\nit makes life of Open Source developers harder, as it's not documented what is done inside BLOBs and what should be done outside of\nthe same.</p>\n\n<p>We will show how to trace the MMIO access of BLOBs in firmware by using Open Source tools. As analysing the traces for possible\nbranches and loops is hard and stressful work, we created our own framework for automatic reverse engineering.\nOur framework allows to capture and analyse MMIO traces, fuzz the BLOB under test and finally generates readable code in a high level language,\nlike C, for easy analysing.</p>\n\n<p>During this talk, we will discuss the legal side, the motivation behind reverse engineering, and the benefit for the Open Source community.\nWe will explain the problems we faced, and explain the basic concept used, with examples from the real world.</p>",
    "description": "",
    "persons": [
      "Patrick Rudolph"
    ]
  },
  {
    "start": 1577899800000,
    "duration": 25,
    "room": "K.4.601",
    "title": "Look at ME!",
    "subtitle": "Intel ME firmware investigation",
    "track": "Open Source Firmware, BMC and Bootloader",
    "abstract": "<p>With Intel's Firmware Support Package (FSP) and the recent release of a\n<a href=\"https://edk2.groups.io/g/devel/message/50920\">redistributable firmware binary</a>\nfor the Management Engine, it has become possible to share full firmware images\nfor modern x86 platforms and potentially audit the binaries. Yet, reverse\nengineering, decompilation and disassembly are still not permitted. However,\nthanks to previous research, we can have a closer look at the binary data and\ncome to a few conclusions. This talk briefly summarizes the fundamentals of\ndeveloping custom and open source firmware, followed by a quick guide through\nthe process of analyzing the binaries without actually violating the terms to\nunderstand a few bits, and finally poses a statement on the political issues\nthat researchers, repair technicians and software developers are facing\nnowadays, taking into account how consumers are affected and how they perceive\nthe situtation eventually.</p>",
    "description": "",
    "persons": [
      "Daniel Maslowski (CyReVolt)"
    ]
  },
  {
    "start": 1577901600000,
    "duration": 25,
    "room": "K.4.601",
    "title": "Capsule Update & LVFS: Improving system firmware updates",
    "subtitle": "Improving reliability and security by simplifying distribution of firmware updates",
    "track": "Open Source Firmware, BMC and Bootloader",
    "abstract": "<p>As the rich capabilities of platforms increase, so does their complexity. As hypervisors and operating systems harden their attack surfaces, malware has been moving deeper into the platform. For example, a modern laptop may have over 15 updatable firmware elements, each with low-level access to a specific hardware domain. From the early days of proprietary BIOS in the 1980’s and 1990’s, to the world of standards in the 2000’s, to the post-PC world of the last few years, the nature of firmware has changed. In order to provide security guarantees for platform firmware, the servicing model of the platform takes center stage.</p>\n\n<p>This session discusses the evolution of platform servicing using examples based on device firmware, non-host/system on a chip (SOC) firmware, and implementation of the Unified Extensible Firmware Interface (UEFI). A modern servicing model features elements for component-based update, resiliency in case unexpected conditions, a more seamless user experience, lowering the friction of update integration, and telemetry for a view into platform health and firmware inventory.</p>\n\n<p>This talk will discuss current trends in standards such as UEFI and associated EDK II firmware, and how the Linux Vendor Firmware System (LVFS) used these components as part of a holistic, open source approach to seamless firmware updates.</p>",
    "description": "",
    "persons": [
      "Brian Richardson"
    ]
  },
  {
    "start": 1577903400000,
    "duration": 25,
    "room": "K.4.601",
    "title": "Opening Intel Server firmware based on OpenBMC example",
    "subtitle": "",
    "track": "Open Source Firmware, BMC and Bootloader",
    "abstract": "<p>Have you ever heard of Board Management Controller? It has been black box firmware to manage servers since last century … now it’s open. OpenBMC is a  Linux Foundation project with a goal to produce an open source implementation of BMC firmware stack. It is a vendor independent Linux distribution created using Yocto project that provides complete set of manageability features. Backbone technologies in OpenBMC include D-Bus and systemd. With embedded web server it provides user friendly WebUI and Redfish interface for easy server management using modern RESTful APIs. Intel as one of the founding  companies offers additional functionalities on top of OpenBMC implementation which will be presented as a part of this presentation.</p>\n\n<p>In this talk we will:\n- tell you a short history and overview of OpenBMC\n- have a quick view on OpenBMC architecture (Yocto, Dbus, systemd)\n- show what’s new in latest 2.7 releases and what is planned for 2.8 (Feb 2020)\n- talk about Intel specific features available in OpenBMC\n- tell you how to contribute to OpenBMC project\n- give you a guide on how to modify, build and run the project on target BMC on Intel server</p>\n\n<p>Audience: software engineers, validation engineer, embedded software architects, data center administrators</p>",
    "description": "",
    "persons": [
      "Maciej Lawniczak",
      "Przemyslaw Czarnowski"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Threat Modelling for Developers",
    "subtitle": "",
    "track": "Security",
    "abstract": "<p>What threats do we need to take into account when building a system? A key method for answering this question is an approach called threat modelling, whereby security problems can be anticipated during the design phase. This talk discusses major threat-modelling approaches, and includes concrete examples of how to apply them to software-intensive systems.</p>",
    "description": "",
    "persons": [
      "Arne Padmos"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "OSINT",
    "subtitle": "do you really know what data you are leaking to the public?",
    "track": "Security",
    "abstract": "<p>Open source intelligence. That almost sounds like a technique described in a movie plot – purely fictional right ?\nAs it turns out, it is alarmingly much more simple than you may think and in some cases we walk the fine line between intelligence and creepy stalker-like activity.\nIn this talk we'll look at some examples, and discuss practical applications from an adversarial point of view. Hopefully you'll leave with an increased appreciation for the data you may be leaking to the world.</p>",
    "description": "",
    "persons": [
      "David Busby"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Securing Existing Software using Formally Verified Libraries",
    "subtitle": "",
    "track": "Security",
    "abstract": "<p>Security vulnerabilities are still very common in todays software. Formal methods could improve the situation, but program verification remains a complex and time-consuming task. Often, the verification of existing software is infeasible and a complete rewrite can be prohibitively expensive. Both, however, is not necessarily required to improve on the current state. By replacing critical parts of an existing software by verified code, security can be strengthened significantly with moderate effort.</p>\n\n<p>We show the feasibility of this approach by the example of a FLOSS TLS implementation. The basis of our PoC is the TLS 1.3 library Fizz [1] which is written in C++. The existing message parser was replaced by a verified version implemented in the SPARK language [2]. Our RecordFlux toolkit [3] was used to automatically generate the parser based on a formal message specification. With the SPARK tools we can prove automatically that an attacker cannot cause any overflows, runtime errors or undefined state by sending malformed messages to the modified library. Because of mismatches in the data structures used in C++ and SPARK, some glue code had to be written manually to integrate the verified parser into Fizz. Still, the modified TLS implementation shows only a slight performance loss while providing higher security.</p>\n\n<p>[1] https://github.com/facebookincubator/fizz\n[2] https://www.adacore.com/about-spark\n[3] https://github.com/Componolit/RecordFlux</p>",
    "description": "",
    "persons": [
      "Tobias Reiher"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "SpecFuzz: Bringing Spectre-type vulnerabilities to the surface",
    "subtitle": "",
    "track": "Security",
    "abstract": "<p>Spectre-type attacks are a real threat to secure systems because a successful attack can undermine even an application that would be traditionally considered safe.\nSpecFuzz is the first tool that enables fuzzing for such vulnerabilities.</p>",
    "description": "<p>The key is a novel concept of speculation exposure:\nThe program is instrumented to simulate speculative execution in software by forcefully executing the code paths that could be triggered due to mispredictions, thereby making the speculative memory accesses visible to integrity checkers (e.g., AddressSanitizer).\nCombined with the conventional fuzzing techniques, speculation exposure enables more precise identification of potential vulnerabilities compared to state-of-the-art static analyzers.</p>\n\n<p>Technical report: https://arxiv.org/abs/1905.10311</p>",
    "persons": [
      "Oleksii Oleksenko"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Falco Internals 101 : Syscalls processing for security analysis",
    "subtitle": "What happens when you have: syscalls, a kernel module, an eBPF probe and a Ring Buffer?",
    "track": "Security",
    "abstract": "<p>Linux Syscalls can be used as an entrypoint to do security analysis on Linux. However reading and processing every system call in userspace creates a very unique set of challenges.\nIn this talk we are going to see exactly what those challenges are and how we solved them in the Falco project.</p>",
    "description": "<p>One of the ways to have broad visibility into our systems, when doing security analysis is to go and ask our questions directly to the Linux kernel.\nFor this purpose, at a very foundational level, in every Linux system we find the syscalls interface. It’s certain that every user space process goes through this part of the kernel.</p>\n\n<p>Starting with this assumption, the immediate conclusion is that we can just go and ask the syscalls “Yo syscalls! What’s happening in my system?”. While this reasoning might seem very simple, reading and processing every single syscall in userspace can result in a set of very unique challenges to this domain.</p>\n\n<p>In this talk we are going to see exactly what those challenges are and how we solved them in the Falco project.</p>\n\n<p>Part of the solution for Falco is to have two alternative drivers, a Kernel module and an eBPF driver talking to userspace using a Ring buffer but you have to come to this talk to hear the rest!</p>",
    "persons": [
      "Lorenzo Fontana"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Docker Security considerations & Incident Analysis",
    "subtitle": "",
    "track": "Security",
    "abstract": "<p>In this presentation we take under consideration the increased use of Docker in corporate environments.\nIt is a fact that Docker has found wide spread of use during the past years, mostly because of it\nbeing very easy to use , economic w.r.t resources used, fast and easy to deploy when compared with\na full blown virtual machine. More and more servers are being operated as Docker hosts on which\nmicro-services run in containers. From a security point of view, two aspects of it arise in the\ncontext of this talk and the inherent time-limitations it has. Firstly, the aspect of the already\nquite talked-through question, “is it secure ?”.Secondly the less analyzed aspect of incident analysis\nand the changes introduced with respect  to known methods and evidence.In this presentation we will\nbriefly outline some security considerations about Docker and the average user and then we will try\nto examine how Docker introduces changes to the workflow related to incident analysis and forensics in its environment.</p>",
    "description": "",
    "persons": [
      "John Lionis"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Incrementality and deck functions",
    "subtitle": "Simple protocols and efficient constructions in symmetric cryptography",
    "track": "Security",
    "abstract": "<p>Protocols in symmetric cryptography are often built from block ciphers, with a fixed input and output size, while variable sizes are handled through their modes of use. Incrementality, namely, the ability to efficiently compute the output for increasing inputs, or to request longer outputs, is often a property of the implementation rather than an explicit feature of a mode.</p>\n\n<p>A doubly-extendable cryptographic keyed (or deck) function is a new kind of object that makes incrementality an integral part of its definition. Writing modes for various applications, such as authenticated encryption of a network channel or disk encryption with a wide block cipher, on top of a deck function turns out to be a simple exercise and leads to less error-prone implementations than on top of a block cipher. We illustrate this with the session-supporting authenticated encryption modes SANE and SANSE. (Sessions naturally protect a continuous flow of messages or a client-server dialog.)</p>\n\n<p>While a deck function can be constructed from existing primitives, like a block cipher, we show two more natural ways of making a deck function in practice.</p>\n\n<ul>\n<li><p>The first one is based on the well-known permutation-based duplex construction, of which a nice instantiation is the <a href=\"https://strobe.sourceforge.io/\">Strobe protocol framework</a>. Strobe was showcased in <a href=\"https://www.discocrypto.com/\">Noise+Strobe=Disco</a> as an advantageous replacement to all kinds of primitives in the <a href=\"https://noiseprotocol.org/\">Noise protocol framework</a>, resulting in <a href=\"https://permutationbasedcrypto.org/2018/slides/David_Wong.pdf\">much simpler specifications and a lighter implementation</a>. Xoodyak, our candidate to the NIST Lightweight Cryptography competition, is another example.</p></li>\n<li><p>The second one is based on the recent Farfalle construction, which relies on the parallel application of a permutation. Farfalle's inherent parallelism yields deck functions that are at the same time simple and efficient on a wide range of platforms. In particular, we point out the nice performance of Kravatte and Xoofff, two deck functions based on the Keccak-p and the Xoodoo permutation, respectively. It is worth noting that Kravatte and Xoofff are much faster than AES-128 in software, and at least competitive with and often faster than AES-128 using dedicated AES instructions <a href=\"https://keccak.team/sw_performance.html\">on the more recent Intel and AMD processors</a>!</p></li>\n</ul>",
    "description": "",
    "persons": [
      "Gilles Van Assche"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "How Transparent Data Encryption is built in MySQL and Percona Server ?",
    "subtitle": "",
    "track": "Security",
    "abstract": "<p>How Transparent Data Encryption is built in MySQL and Percona Server ?\n- keyrings – what are they used for ? What is the difference between using a server back-end (keyring<em>vault) versus file back-end (keyring</em>file). How it affects server startup and why? Why per server separation is needed in Vault Server?\n- How Master Key encryption works ? How it is build on page level ? How do we know which key we should fetch to decrypt a table ? How do we know that used key is the correct one ? How do we make sure that we can decrypt a table when we need it ?\n- What crypto algorithms are used ?\n- How Master Key rotation works ? Why is it needed ?\n- What is KEYRING encryption and what are encryption threads?\n- How binlog encryption works in 5.7 and how it works in 8.0 ?\n- How undo log/redo log encryption works ?</p>",
    "description": "<p>How Transparent Data Encryption is Built in MySQL and Percona Server ?</p>\n\n<p>In this presentation, we'll take a deep dive into the world of transparent data encryption for open source databases. We'll be looking at how transparent data encryption is implemented in MySQL and Percona Server for MySQL:\n- keyrings – what are they used for ? What is the difference between using a server back-end (keyring<em>vault) versus file back-end (keyring</em>file). How it affects server startup and why? Why per server separation is needed in Vault Server?\n- How Master Key encryption works ? How it is build on page level ? How do we know which key we should fetch to decrypt a table ? How do we know that used key is the correct one ? How do we make sure that we can decrypt a table when we need it ?\n- How Master Key rotation works ? Why is it needed ?\nBy the end of the talk, you'll have a better understanding of the transparent data encryption and will be aware of things to take into account when interacting with encrypted databases in your applications.</p>",
    "persons": [
      "Robert Golebiowski"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Secure logging with syslog-ng",
    "subtitle": "Forward integrity and confidentiality of system logs",
    "track": "Security",
    "abstract": "<p>The design, implementation, and configuration of the secure logging service. Its aim is to provide tamper evident logging, i.e., to adequately protect log records of an information system against tampering and to provide a sensor indicating attack attempts. The secure logging service achieves this by authentically encrypting each log record with an individual cryptographic key used only once and protects integrity of the whole log archive by a cipher{based message authentication\ncode. Each attempt to tamper with either an individual log record or the log archive itself will be immediately detected during log archive verification. Therefore, an attacker can no longer tamper with log records without being detected which greatly enhances the use of log archives in forensic investigations.</p>",
    "description": "<p>Log records are normally produced by any information system in order to perform monitoring during normal operations and for troubleshooting in case of technical problems. Log information is equally important for retaining the security of an information system, as security relevant events are recorded and can later be monitored for unusual patterns which may indicate an attack attempt. Examples include log on and log off, startup and shutdown, network service access, network filter rule application, storage access, etc. Log records may also contain valuable information about a system that a potential attacker intends to compromise. If an attacker is able to successfully compromise a system, they are also able to tamper with log records, potentially hiding their traces. This makes forensic analysis extremely difficult, as no reliable data source about system behavior immediately before the attack is available to a security analyst performing incident investigation. Therefore, log information should be appropriately protected. The aim of the secure logging service is to provide tamper evident logging, i.e., to adequately protect log records of an information system and to provide a sensor indicating attack attempts. The secure logging service achieves this by authentically encrypting each log record with an individual cryptographic key used only once and protects integrity of the whole log archive by a cryptographic authentication code. Each attempt to tamper with either an individual log record or the log archive itself will be immediately detected during log archive verification. Therefore, an attacker can no longer tamper with log records\nwithout being detected. ost information systems rely on standards in order to provide logging services. One of the most widely adopted standards is the syslog protocol which is specified in RFC 5424. Many implementations of this\nprotocol are available. A popular extensible implementation with additional features is syslog-ng, which is an enhanced logging daemon with advanced features for input and output. Furthermore, it features capabilities for log message filtering, rewriting, and routing. It can be used as a drop-in replacement for existing log daemons on UNIX systems. The implementation of the secure logging service providing tamper evidence and confidentiality of system logs based on the template mechanism of syslog-ng is presented together with an application example.</p>",
    "persons": [
      "Stephan Marwedel"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Protecting plaintext secrets in configuration files",
    "subtitle": "",
    "track": "Security",
    "abstract": "<p>Applications and services rely on configuration data in order to be customized and we will talk about how to keep them in a safer place other than plaintext configuration files.</p>",
    "description": "<p>The configparser module is Python's standard configuration file parser and many projects rely on it to achieve easy configuration with plaintext files. OpenStack Common Libraries (Oslo) has an alternative called oslo.config with additional sources of input like command line arguments or environment variables. With the addition of a feature called source drivers last year, we are now able to increase the security of configuration values storing them in a safer place.</p>\n\n<p>This talk focuses on the new source driver that integrates Oslo.Config and Castellan, another Olso module specialized in talking to secret managers, and how we can store our sensitive configuration data using HashiCorp Vault.</p>",
    "persons": [
      "Moisés Guimarães"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Application Whitelisting in Linux Environment",
    "subtitle": "",
    "track": "Security",
    "abstract": "<p>Are you a sysadmin and feeling paranoid? Let's promote security hardening to another level.\nPerhaps, with the concept of Application Whitelisting you will be able to sleep again.</p>",
    "description": "<p>In this session we are going to explain the Application Whitelisting idea and its implementation, what benefits are there from a security point of view and how it differs from competitors.\nWe are going to show how to create a new set of rules based on distribution default for given examples.\nAs a result, an attendee should be able to setup the Application Whitelisting framework on his server or workstation.</p>\n\n<p>This presentation is based on Red Hat/Fedora Linux environment.</p>",
    "persons": [
      "Radovan Sroka"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "seccomp — Your Next Layer of Defense",
    "subtitle": "",
    "track": "Security",
    "abstract": "<p>Why should you allow all possible system calls from your application when you know that you only need some? If you have ever wondered the same then this is the right talk for you. We are covering:</p>\n\n<ul>\n<li>What is seccomp in a nutshell and where could you use it.</li>\n<li>Practical example with Elasticsearch and Beats.</li>\n<li>How to collect seccomp violations with Auditd.</li>\n</ul>\n\n\n<p>Because your security approach can always use an additional layer of protection.</p>",
    "description": "",
    "persons": [
      "Philipp Krenn"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Kernel Runtime Security Instrumentation",
    "subtitle": "LSM+BPF=KRSI",
    "track": "Security",
    "abstract": "<p>KRSI (Kernel Runtime Security Instrumentation) is an ongoing effort at Google to upstream an LSM (Linux Security Module) instrumentable using eBPF (extended Berkeley Packet Filter) to the Linux kernel.</p>\n\n<p>KRSI allows system owners to dynamically attach eBPF programs to security hooks and write MAC and audit policies without having to reboot or patch the kernel thereby enabling a new class of system security and auditing software.</p>\n\n<p>This talk presents the main concepts behind KRSI: it introduces the technologies leveraged and presents the API exposed to users.</p>",
    "description": "",
    "persons": [
      "Florent Revest"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Using SELinux with container runtimes",
    "subtitle": "",
    "track": "Security",
    "abstract": "<p>Lukas Vrabec built a new standalone tool, udica, for generating SELinux policy profiles for containers based on automatic inspecting these containers. We will focus on why udica is needed in the container world and how it can make SELinux and containers work better together.  We will show real examples where SELinux separation for containers had to be turned off because the generic SELinux type container_t was too tight. With a tool like “udica”, users and developers can easily customize the policy with limited SELnux policy writing skills. Come to see how easy also you can create custom SELinux policy for your containers!</p>",
    "description": "<p>This talk will explain how SELinux works with containers.  We will show how to enable/disable SElinux using multiple different container runtimes and define the default types.  One issue with these types is that they are tough to customize.  The two default types for running containers are container<em>t which is a fully confined domain, which eliminates any use of the host files unless they are relabeled.  Or spc</em>t, which is the type containers run with when SELinux is disabled for container separation, --privileged mode.  As an example, If you had a container that you wanted to be able to gather the logs from /var/log on the host and send them to a centralized server, you have to disable SELinux separation.</p>\n\n<p>Writing custom policy for each container that needed additional access would be very difficult and require a container policy writer.</p>",
    "persons": [
      "Lukas Vrabec"
    ]
  },
  {
    "start": 1577899800000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "The hairy issue of e2e encryption in instant messaging",
    "subtitle": "",
    "track": "Security",
    "abstract": "<p>End-to-end encryption is often regarded as the holy grail of security. But when you start implementing it soon becomes a security hell. Does it really protect against the threats it should protect against? And watch out for the pitfalls when implementing it: almost everybody fails there!</p>",
    "description": "<p>Lets start with the conclusion of this talk: after twenty years of designing and analyzing high security instant messaging systems, I came to the conclusion that end-to-end encryption (e2ee) in instant messaging is snake-oil. It creates a false sense of security.</p>\n\n<p>First of all the threat model underneath e2ee has fundamental flaws, it doesn’t deliver protection against the threats commonly named to justify it. And if that isn’t enough, there a lot of issues that make a proper implementation very hard to get right. To name a few: key verification, one-to-many messages, store and forward and archiving.</p>\n\n<p>But lets not end this talk all in black. Though we aren’t there yet, there are some developments that may solve these issues. I will name those too.</p>",
    "persons": [
      "Winfried Tilanus"
    ]
  },
  {
    "start": 1577901600000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "What you most likely did not know about sudo…",
    "subtitle": "",
    "track": "Security",
    "abstract": "<p>Everybody knows sudo, right? Sudo allows a system administrator to give certain users  the ability to run some commands as root, while logging the executed commands and their arguments. It is installed by default on almost all Linux systems, and is available for most commercial UNIX systems. Still, even system administrators often only know it is the “prefix” to use before entering a command requiring root privileges. Learn how much more this simple looking tool can do!</p>",
    "description": "<p>Everybody knows sudo, right? Sudo allows a system administrator to give certain users  the ability to run some commands as root, while logging the executed commands and their arguments. It is installed by default on almost all Linux systems, and is available for most commercial UNIX systems. Still, even system administrators often only know it is the “prefix” to use before entering a command requiring root privileges. Learn how much more this simple looking tool can do!\nMost of the times the default configuration allows a group of users to run any commands:\n%wheel  ALL=(ALL)   ALL\nIt’s a good first step, better than using the root account directly. This way you can trace who ran what commands on the system. But there are a lot more possibilities when configuring sudo, making your system more secure.\nLife is simple when when you have to give access a single user to a single command. But as soon as  you have multiple users with the same access rights, it is not just shorter but also easier to maintain, if you use aliases.\nFor added security, you can add a hash of binaries to sudo. This way if the binary changes for any reasons, like modifying it through a successful exploit, you can prevent it from being used.</p>\n\n<p>Using sudo does not make much sense without proper logging and alerting. There are three major possibilities:\n- syslog: all events are logged to syslog. For additional security, collect sudo logs  centrally, so a malicious user cannot delete them easily.\n- e-mail: sudo can send e-mail alerts on different kinds of failures\n- debug: in depth logging of subsystems, mostly useful for developers\nSession recording is a fourth possibility. The terminal output can be saved in a local file and played back. You can play back what happened, even if the user started up an interactive shell.</p>\n\n<p>Instead of maintaining the sudoers file on each of your systems, you can use LDAP to configure sudo. It has some differences compared to a sudoers file, but also many advantages:\n- local users cannot manipulate the rules,\n- is easier to maintain,\n- goes live immediately.</p>\n\n<p>Starting with version 1.8, sudo has a plugin-based architecture. You can replace or extend sudo functionality using plugins. This way users can keep starting applications the usual way using sudo, but have a different configuration or policy engine behind the sudo command. There are both open source and commercial plugins available. For example the sudo_pair plugin – developed in Rust – enables monitoring and makes it possible to require interactive approval of sessions.</p>\n\n<p>As you can see, sudo has several lesser-known features that can make monitoring and access management easier for large organizations.</p>",
    "persons": [
      "Peter Czanik"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 5,
    "room": "UA2.220 (Guillissen)",
    "title": "Welcome to the Legal & Policy Issues DevRoom",
    "subtitle": "",
    "track": "Legal and Policy Issues",
    "abstract": "<p>Welcome to the Legal &amp; Policy Issues DevRoom including and overview of how the new Collaboration and Debate sessions will work.</p>",
    "description": "",
    "persons": [
      "Tom Marble"
    ]
  },
  {
    "start": 1577874900000,
    "duration": 25,
    "room": "UA2.220 (Guillissen)",
    "title": "Technology challenges for privacy: the case of decentralized social media",
    "subtitle": "",
    "track": "Legal and Policy Issues",
    "abstract": "<p>As decentralized social media gathers more users, the privacy by design and default principles from the GDPR are in accordance to the design model it proposes. This talk is going to tackle the main advantages and challenges this approach brings, from the perspective of the data protection legislation and privacy architectural strategies.</p>",
    "description": "<p>Social media platforms have been a central feature in our generation and as we grow more toward understanding their power and taking part in their evolution, we realize the challenges they impose. One of those is how to protect personal data of users, and ensure that the processing is done in accordance with legislation such as GDPR.\nDecentralized social media has developed as a space where personal data ownership is a priority, coming as an alternative to centralized platforms. Not coincidentally, they are mostly open source software, as transparency and offering control of the data to the users go hand in hand with this ambition. Blockchain based social media networks, and projects built on top of the ActivityPub protocol are some of the most popular examples of alternatives which have gathered significant numbers of users or data subjects, under the GDPR.\nOne of the main architectural strategies in building software which is privacy by default and design is data separation. It states that the processing of personal data should be performed whenever possible in a distributed manner. As the GDPR lists privacy by design and default as core principles, decentralized social networking poses a significant advantage compared to centralized solutions. One heuristic to take from this is if the future is privacy-oriented, then social media will be decentralized.\nThis talk is going to offer an analysis of the main benefits and challenges that decentralized social medial pose, from the points of view of personal data protection legislation and privacy design patterns for software architecture.</p>",
    "persons": [
      "Cristina DeLisle"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 50,
    "room": "UA2.220 (Guillissen)",
    "title": "DEBATE: Should FOSS licenses be enforced at all?  What means are acceptable if so?",
    "subtitle": "",
    "track": "Legal and Policy Issues",
    "abstract": "<p>In a perfect world, lawyers (and the entire legal system) should not be necessary. And in a perfect FOSS world, everyone respects each and every provision of every license. The reality is, however, very different, and enforcement may be a necessary evil. This need does not mean we have to open the gates to be flooded by \"copyleft trolls\", but to establish a sound enforcement policy, in order to unleash the lawyers only for the most blatant and repeated violations</p>",
    "description": "<p>Affirmative position: FOSS licenses should not be enforced.</p>\n\n<ol>\n<li>First Affirmative Constructive (1AC) = 7 minutes\na. Cross-examination of First Affirmative by Second Negative = 3 minutes</li>\n<li>First Negative Constructive (1NC)  = 7 minutes\na. Cross-examination of First Negative by First Affirmative = 3 minutes</li>\n<li>Second Affirmative Constructive (2AC)  = 7 minutes\na. Cross-examination of Second Affirmative by First Negative = 3 minutes</li>\n<li>Second Negative Constructive (2NC)  = 7 minutes\na. Cross-examination of Second Negative by Second Affirmative = 3 minutes</li>\n<li>First Negative Rebuttal (1NR)  = 3 minutes</li>\n<li>First Affirmative Rebuttal (1AR)  = 3 minutes</li>\n<li>Second Negative Rebuttal (2NR)  = 3 minutes</li>\n<li>Second Affirmative Rebuttal (2AR)  = 3 minutes</li>\n</ol>",
    "persons": [
      "Pamela Chestek",
      "Giovanni Battista Gallus",
      "Marc Jones",
      "McCoy Smith"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 50,
    "room": "UA2.220 (Guillissen)",
    "title": "DEBATE: Does Careful Inventory of Licensing Bill of Materials Have Real Impact on FOSS License Compliance?",
    "subtitle": "",
    "track": "Legal and Policy Issues",
    "abstract": "<p>Projects today often have thousands of FOSS dependencies. Since risk\nflows downstream in the supply chain; projects inherit and pass on the\nrisks of all their dependencies. In response, licensing bill of\nmaterials tools often seek to push well-formed licensing inventory\ndata upstream in an effort to ease downstream compliance\nchallenges. At the same time, there has been a stark increase in\nlicense violations, especially, though not exclusively, on copyleft\nlicenses. Is this approach to improving compliance working?</p>",
    "description": "<p>Affirmative position: Compliance at scale through tool-driven assembly of bills of materials is essential for FOSS</p>\n\n<ol>\n<li>First Affirmative Constructive (1AC) = 7 minutes\na. Cross-examination of First Affirmative by Second Negative = 3 minutes</li>\n<li>First Negative Constructive (1NC)  = 7 minutes\na. Cross-examination of First Negative by First Affirmative = 3 minutes</li>\n<li>Second Affirmative Constructive (2AC)  = 7 minutes\na. Cross-examination of Second Affirmative by First Negative = 3 minutes</li>\n<li>Second Negative Constructive (2NC)  = 7 minutes\na. Cross-examination of Second Negative by Second Affirmative = 3 minutes</li>\n<li>First Negative Rebuttal (1NR)  = 3 minutes</li>\n<li>First Affirmative Rebuttal (1AR)  = 3 minutes</li>\n<li>Second Negative Rebuttal (2NR)  = 3 minutes</li>\n<li>Second Affirmative Rebuttal (2AR)  = 3 minutes</li>\n</ol>",
    "persons": [
      "Bradley M. Kuhn",
      "Carol Smith",
      "Jeff McAffer"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 25,
    "room": "UA2.220 (Guillissen)",
    "title": "COLLAB: How can we give users standing in free/open software/hardware?",
    "subtitle": "",
    "track": "Legal and Policy Issues",
    "abstract": "<p>How can we give users standing in free/open software/hardware?\nHow can we motivate end users to care about FOSS if\nthey can't express their preference? What tools do we have beyond\nthe \"court of public opinion\"? Can we invent a NEW legal hack?</p>",
    "description": "",
    "persons": [
      "Italo Vignoli"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 25,
    "room": "UA2.220 (Guillissen)",
    "title": "COLLAB: The optics of the policy",
    "subtitle": "And vice-versa",
    "track": "Legal and Policy Issues",
    "abstract": "<p>Photography policies have begun to appear at free-software events in recent years. These policies typically seek to address personal privacy concerns for event attendees, but they sometimes conflict with the event's desire to record talks, Q&amp;A periods, and social gatherings in public spaces. If not drafted with care, photo policies also run the risk of creating ambiguities for journalists, other attendees making personal photo or video recordings, and members of event-hosting organizations or the public. This session will be an open discussion about photo and video-recording policies, online tagging policies, and related personal-privacy policies, with the goal of clarifying the requirements, needs, and intents of all stakeholders in the FOSS community, so that future event organizers have a solid framework from which to draft clear policies that fit their situations.</p>",
    "description": "<p>Free-software events, like free-software projects, have to maintain a delicate balance between openness as a broad principle and privacy as an individual concern. In the past few years, more and more free-software events and community projects have developed \"photo policies\" that are intended to define when and how individuals and groups should be captured in media from the event and when and how those same people should be identified in the media. But a haphazard approach to policy writing can create unintentional ambiguities, such as how to define when an individual is the \"subject' of a photograph or merely in the background. And free-software communities must also take care to write policies that do not come into conflict with local law, especially when events take place in public spaces. Finally, event organizers need to ensure that their photo policies, real-name policies, press policies, and session-recording consent policies work in concern with one another, not in conflict.</p>\n\n<p>This session will be a broad discussion of photography policies and how they interact with other policy concerns. The intent will be to enumerate the concerns of all stakeholders, identify potential areas of confusion, note best practices, and — most importantly — establish resources and spaces for further discussion for project and community members creating photo policies in the future.</p>",
    "persons": [
      "Nathan Willis"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 50,
    "room": "UA2.220 (Guillissen)",
    "title": "DEBATE: The 4 Freedoms and OSD are outdated and no longer relevant in 2020",
    "subtitle": "",
    "track": "Legal and Policy Issues",
    "abstract": "<p>Are the FSF's 4 Freedoms and the OSI's Open Source Definition out\nof date in 2020 and should be replaced.</p>",
    "description": "<p>Affirmative position: OSD/FSD is now irrelevant</p>\n\n<ol>\n<li>First Affirmative Constructive (1AC) = 7 minutes\na. Cross-examination of First Affirmative by Second Negative = 3 minutes</li>\n<li>First Negative Constructive (1NC)  = 7 minutes\na. Cross-examination of First Negative by First Affirmative = 3 minutes</li>\n<li>Second Affirmative Constructive (2AC)  = 7 minutes\na. Cross-examination of Second Affirmative by First Negative = 3 minutes</li>\n<li>Second Negative Constructive (2NC)  = 7 minutes\na. Cross-examination of Second Negative by Second Affirmative = 3 minutes</li>\n<li>First Negative Rebuttal (1NR)  = 3 minutes</li>\n<li>First Affirmative Rebuttal (1AR)  = 3 minutes</li>\n<li>Second Negative Rebuttal (2NR)  = 3 minutes</li>\n<li>Second Affirmative Rebuttal (2AR)  = 3 minutes</li>\n</ol>",
    "persons": [
      "Neil McGovern",
      "Andrew Katz",
      "Matt Jarvis",
      "Luis Villa",
      "Frank Karlitschek",
      "Amanda Brock"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 25,
    "room": "UA2.220 (Guillissen)",
    "title": "DEBATE: Should licenses be designed to advance general social goals?",
    "subtitle": "",
    "track": "Legal and Policy Issues",
    "abstract": "<p>We have seen several licenses proposed as \"open source\" that\ncarry some obligation or restriction related to ethics or\nother social goals. Is this a good direction for FOSS license drafting?</p>",
    "description": "<p>Affirmative position: FOSS licenses should advance social goals</p>\n\n<ol>\n<li>First Affirmative Constructive (1AC) = 7 minutes\na. Cross-examination of First Affirmative by Second Negative = 3 minutes</li>\n<li>First Negative Constructive (1NC)  = 7 minutes\na. Cross-examination of First Negative by First Affirmative = 3 minutes</li>\n<li>Second Affirmative Constructive (2AC)  = 7 minutes\na. Cross-examination of Second Affirmative by First Negative = 3 minutes</li>\n<li>Second Negative Constructive (2NC)  = 7 minutes\na. Cross-examination of Second Negative by Second Affirmative = 3 minutes</li>\n<li>First Negative Rebuttal (1NR)  = 3 minutes</li>\n<li>First Affirmative Rebuttal (1AR)  = 3 minutes</li>\n<li>Second Negative Rebuttal (2NR)  = 3 minutes</li>\n<li>Second Affirmative Rebuttal (2AR)  = 3 minutes</li>\n</ol>",
    "persons": [
      "John Sullivan",
      "Molly de Blanc",
      "James Vasile",
      "Josh Simmons",
      "Dashiell Renaud"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 50,
    "room": "UA2.220 (Guillissen)",
    "title": "DEBATE: Does FOSS need sustainability?",
    "subtitle": "",
    "track": "Legal and Policy Issues",
    "abstract": "<p>Several prominent FOSS projects have changed their FOSS licenses to\nalternate licenses that make software available, but with additional\nrestrictions intended to help financially sustain FOSS development and\ncombat \"strip mining\" by software-as-a-service providers. Additionally,\nrecently several related organizations have jumped into the the role of\nhelping sustain open source by providing (for a fee) funding conduits,\nfundraising services, or other mechanisms to route money to maintainers.</p>",
    "description": "<p>Affirmative position: FOSS benefits from sustainability efforts</p>\n\n<ol>\n<li>First Affirmative Constructive (1AC) = 7 minutes\na. Cross-examination of First Affirmative by Second Negative = 3 minutes</li>\n<li>First Negative Constructive (1NC)  = 7 minutes\na. Cross-examination of First Negative by First Affirmative = 3 minutes</li>\n<li>Second Affirmative Constructive (2AC)  = 7 minutes\na. Cross-examination of Second Affirmative by First Negative = 3 minutes</li>\n<li>Second Negative Constructive (2NC)  = 7 minutes\na. Cross-examination of Second Negative by Second Affirmative = 3 minutes</li>\n<li>First Negative Rebuttal (1NR)  = 3 minutes</li>\n<li>First Affirmative Rebuttal (1AR)  = 3 minutes</li>\n<li>Second Negative Rebuttal (2NR)  = 3 minutes</li>\n<li>Second Affirmative Rebuttal (2AR)  = 3 minutes</li>\n</ol>",
    "persons": [
      "Tom Marble",
      "Philippe Ombredanne",
      "Luis Villa",
      "Mehdi Medjaoui"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 25,
    "room": "UA2.220 (Guillissen)",
    "title": "Oracle v. Google: What are the implications for FOSS?",
    "subtitle": "",
    "track": "Legal and Policy Issues",
    "abstract": "<p>All the merits briefs for Oracle v. Google will be filed a couple weeks before FOSDEM 2020. This will be a rundown of the positions argued by various groups - how are the parties positioning the questions presented? What are the various amici arguing? Are there any positions that will be particularly impactful for FOSS groups and users?</p>",
    "description": "<p>On Friday, Nov. 15, the U.S. Supreme Court agreed to hear Oracle v. Google. It is hard to overstate how impactful this decision will be on FOSS. For the first time in a generation, the Supreme Court will be evaluating how copyright and software interact - and they will be discussing it in the context of GPL-licensed Java.</p>\n\n<p>Right before FOSDEM, all the briefs by all parties will be due. As we sit in Brussels, the court clerks will be reading the various briefs and creating a \"bench memo\" for each justice, summarizing the arguments being advanced by both Oracle and Google, as well as the points raised by different amici.</p>\n\n<p>This presentation will be a verbal \"bench memo\" for those in the FOSS community. Rather than advance a particular view, we will try to understand the scope of issues being argued, and how they may affect Free and Open Source Software in the United States - and worldwide.</p>",
    "persons": [
      "Van Lindberg"
    ]
  },
  {
    "start": 1577899800000,
    "duration": 50,
    "room": "UA2.220 (Guillissen)",
    "title": "Legal Organizer's Panel",
    "subtitle": "",
    "track": "Legal and Policy Issues",
    "abstract": "<p>The Legal &amp; Policy Issues DevRoom Organizers gather to reflect on our DevRoom talks and FOSS issues of the day</p>",
    "description": "",
    "persons": [
      "Tom Marble",
      "Bradley M. Kuhn",
      "Karen Sandler",
      "Richard Fontana"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 30,
    "room": "UB2.147",
    "title": "Improving the culture of automated testing in FOSS",
    "subtitle": "",
    "track": "Testing and Automation",
    "abstract": "<p>In this talk we will explore some of the FOSS specific mentalities and\npractices that may discourage adoption of comprehensive automated testing, and\npresent advice for promoting and sustaining automated testing in FOSS projects.</p>",
    "description": "<p>Automated testing is on the rise in the FOSS world, but there is still ample\nroom for improvement when it comes to sufficiently comprehensive automated\ntests. The test suites of many FOSS projects leave a lot to be desired,\na result that's often affected by useful FOSS practices that are\ntaken too far. Identifying such practices, like placing excessive trust in code\nreviews because \"given enough eyeballs, all bugs are shallow\", or leaving tests\nfor later in the spirit of \"release often, release early\", is a first step in\nhaving a discussion that will hopefully convince more projects to embrace\nautomated testing, and improve the quality of FOSS overall.</p>",
    "persons": [
      "Alexandros Frantzis"
    ]
  },
  {
    "start": 1577876700000,
    "duration": 30,
    "room": "UB2.147",
    "title": "Welcome to KernelCI",
    "subtitle": "You're all welcome to the KernelCI project's new home",
    "track": "Testing and Automation",
    "abstract": "<p>KernelCI is a project dedicated to testing the upstream Linux kernel.\nOriginally created by Linaro in 2014, it started a new chapter by\nbecoming a Linux Foundation project in October 2019.  Its future looks\nbright, with plenty of opportunities for new contributors to join.</p>",
    "description": "<h2>The chosen one</h2>\n\n<p>The upstream kernel testing landscape is pretty wide, rich and\ndiverse, in the same ways that the Linux kernel is.  But as there is\nonly one upstream kernel, it became clear that there should also be\none main test system associated with it.  KernelCI was chosen to\nfulfil this role, being rather neutral, versatile and based on a\ndistributed architecture.</p>\n\n<h2>A welcoming place</h2>\n\n<p>While the project now has a governing board via the Linux Foundation\nmembership, its involvement with the kernel community is only getting\nstronger.  It is of utmost importance to keep the roadmap aligned with\nexpectations from maintainers and developers to preserve the integrity\nand overall purpose of project.  In fact, it now needs to become an\neasy tool to use by anyone who wants to add tests for their subsystem\nor their hardware and for anyone to reproduce those tests locally.</p>\n\n<h2>An exciting year ahead</h2>\n\n<p>This is a new beginning for KernelCI, with many of its prior\nlimitations now being removed thanks to the framework provided by the\nLinux Foundation.  Contributors to the code, tests, hardware labs and\nnew project members will all have a great influence by joining the\nproject at this very special point in time.  Now is the time to come\nand help shape it as a successful project for the years to come.</p>\n\n<p>Slides: https://people.collabora.com/~gtucker/kernelci/doc/gtucker-kernelci-fosdem-2020.pdf</p>",
    "persons": [
      "Guillaume Tucker"
    ]
  },
  {
    "start": 1577878800000,
    "duration": 40,
    "room": "UB2.147",
    "title": "Abusing GitLab CI to Test Kernel Patches",
    "subtitle": "",
    "track": "Testing and Automation",
    "abstract": "<p>See how Red Hat’s CKI project uses GitLab CI to test kernel patches as soon as they're posted to maillists.</p>",
    "description": "<p>Red Hat's CKI project uses GitLab CI to organize and track its pipelines, lint, patch, and build Linux kernels, and oversee testing. It also uses a number of supporting systems to discover kernel patches and commits, maintain hardware inventory, provision hardware and VMs, run tests, and finally record and report results.</p>\n\n<p>See which tricks the project is pulling to tie all these parts together, and test patches posted to several maillists, commits to 15+ git repos, builds done by other build systems, as well as weird things like stable kernel patch queue, and parts of its own software stack.</p>\n\n<p>Making such extensive use of a CI system inevitably uncovers its limitations, and a list of these will also be presented along with some possible solutions.</p>",
    "persons": [
      "Nikolai Kondrashov"
    ]
  },
  {
    "start": 1577881500000,
    "duration": 50,
    "room": "UB2.147",
    "title": "OpenQA with the JDP data analyses framework",
    "subtitle": "Bug tag propagation on 2M+ test results using Julia",
    "track": "Testing and Automation",
    "abstract": "<p>Overview of SUSE's Linux kernel testing in OpenQA, how we keep track of known issues, explore test results and other features of JDP. The JDP framework is written in Julia, uses Redis as a distributed data cache and Jupyter for interactive reporting. OpenQA is a large application used for testing operating systems and displaying the results.</p>",
    "description": "",
    "persons": [
      "Richard Palethorpe"
    ]
  },
  {
    "start": 1577884800000,
    "duration": 30,
    "room": "UB2.147",
    "title": "Automated Performance Testing for Virtualization with MMTests",
    "subtitle": "The Tools, the Challenges and also some War-Stories about Performance Testing Hypervisors and VMs",
    "track": "Testing and Automation",
    "abstract": "<p>What benchmark? How many VMs? How big each VM is? Are they all equal or are they different? What's the host OS? What are the guest OSes? I.e., when wanting to do virtualization performance testing, the matrix of test cases tends to explode pretty quickly. This talk will show how we enhanced an existing benchmarking suite, MMTests, in order to be able to deal a little bit better with such complexity. And what our further activities and plans are, for even more and better automation.</p>",
    "description": "<p>Functional testing is already hard enough, in virtualization. For instance, because we need to make sure that things work with different combinations of versions of the OSes in hosts and guests. Doing performance testing, even more so. In fact, there are much more things to consider, such as how many VMs we use, how big they are, whether or not they are equally big or different, what to run in them, how to partition the host resources for them... And this is true either in case you have a specific (virtualized) workload and some KPI to meet, in which case you need testing and benchmarking to figure out whether or not the tuning you have done has brought you there, or in case you wonder how good (or how bad) a certain configuration of both your host and your guests works, for a number of workloads,</p>\n\n<p>This talk will introduce the problem, showing how the size and the complexity of a typical 'virtualization performance testing matrix' really tend to explode. We will, as an example, show how some specific characteristics of a virtualized system were, despite tuning, causing us to not be able to achieve the desired performance levels. Then we illustrate how, at SUSE, we do automated performance benchmarking, how we enhanced the tool that was in use the most for baremetal benchmarks (the MMTests suite) in order for it to be much more useful in virtualized systems and how we are integrating it with other tools to bring the level of automation even further and achieve something that really resembles a Virtualization Performance CI system.</p>",
    "persons": [
      "Dario Faggioli"
    ]
  },
  {
    "start": 1577886900000,
    "duration": 25,
    "room": "UB2.147",
    "title": "Auto-healing cluster through negative testing",
    "subtitle": "",
    "track": "Testing and Automation",
    "abstract": "<p>OCS stands for Openshift Container storage. It provides container-based storage for OCP(Openshift container platform). It’s easily scalable to bare metal, VMs and cloud platforms.\nAuto healing is a property of OCS cluster that auto heals a cluster component automatically when passes through an unexpected condition. A component can be a node, a network interface, a service, etc. To make sure auto heals just fine, we introduced negative testing.\nNegative Testing is defined as, a testing type that checks a system for unexpected conditions. In this presentation, We’re going to talk, what role negative testing plays, how to negative test components like node by shutting it down, deploying a heavy workload, etc. Similarly, for the network component, we are going to see what happens when the public network is disconnected along with many more scenarios.</p>",
    "description": "",
    "persons": [
      "Rajat Singh"
    ]
  },
  {
    "start": 1577888700000,
    "duration": 30,
    "room": "UB2.147",
    "title": "Introducing OpenTAP - Open Test Automation Project",
    "subtitle": "A developer-first extensible test and measurement automation project",
    "track": "Testing and Automation",
    "abstract": "<p>OpenTAP is a project aimed at automation in the test and measurement space. It is designed for test and measurement of hardware in R&amp;D and manufacturing, but is moving more towards software testing e.g. with usage in cloud infrastructure testing. The project started as an internal product by Keysight Technologies and is used as the core of many products and solutions deployed around the world. As of 2019, we have released OpenTAP under the Mozilla Public License v2 and are working on building a community around it. The release was influenced by the team attending FOSDEM over the last few years, we will also welcome you for more detailed discussion at our booth on Saturday.</p>\n\n<p>What we want is to grow an environment for people to share, leverage and co-develop test system plugins and solutions.</p>\n\n<p>In this short talk, we will explain the basic concepts of OpenTAP, what it can be used for, and how to get started.</p>",
    "description": "",
    "persons": [
      "Rolf Madsen"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 30,
    "room": "UB2.147",
    "title": "One test output format to unite them all",
    "subtitle": "",
    "track": "Testing and Automation",
    "abstract": "<p>Since several years, software quality tools have evolved, CI systems are more and more scalable, there are more testing libraries than ever and they are more mature than ever and we have seen the rise of new tools to improve the quality of code we craft.</p>\n\n<p>Unfortunately, most of our CI system still launch a script and check the return code, most of the testing libraries don't allow to select finely which tests to launch and most of CI advanced innovations, parallel running, and remote execution, are not available to developers on their workstation.</p>\n\n<p>Each language community has its own set of tools, libraries, and command-line and visual interfaces increasing the effort for developers to learn or learn again how to write, run and debug tests in each language.</p>\n\n<p>How to improve the situation? In this talk, I will present one of my project LITF (https://github.com/Lothiraldan/litf) a new protocol for test running and test output as well as BALTO (https://github.com/lothiraldan/balto), a test orchestrator using this new format. Thanks to this new format, BALTO can execute several test suites in different languages, remotely on a Kubernetes cluster and all in parallel. In any case, this is the goal of the stable version.</p>",
    "description": "",
    "persons": [
      "Boris Feld"
    ]
  },
  {
    "start": 1577892900000,
    "duration": 30,
    "room": "UB2.147",
    "title": "Releasing Software with GitOps",
    "subtitle": "How OpenStack manages releases using Git based automation",
    "track": "Testing and Automation",
    "abstract": "<p>Overview of the process the OpenStack community uses to manage all software releases through automation around Git commits.</p>",
    "description": "<p>The OpenStack community has enabled a lot of automation around releasing their software. This presentation will give an overview of how code reviews are used to manage release activity. It will step through the use of Zuul CI jobs to perform validation of requests before they are accepted, and how commits are used to trigger jobs to tag and release the software, update documentation, trigger requirements updates, and other follow on work that needs to happen whenever new code is released.</p>",
    "persons": [
      "Sean McGinnis"
    ]
  },
  {
    "start": 1577895000000,
    "duration": 45,
    "room": "UB2.147",
    "title": "Writing Go(od) Tests",
    "subtitle": "Writing good tests in golang",
    "track": "Testing and Automation",
    "abstract": "<p>Every year we hear great content about how to <em>develop</em> in Go, but rarely do we focus on how to <em>test</em> in Go. Well written tests are critical to the success of a project, and more often than not, they can help drive developers to design features in more simple and concise ways.</p>",
    "description": "<p>In this talk, I'll explain the importance of test driven development and provide some tactics for how to implement the practice in your daily work and on your respective team. I'll dive into the <code>testing</code>, <code>require</code>, and <code>assert</code> packages to dissect which function calls are appropriate for different use cases, and present multiple different ways to write Go tests for each scope, including unit, integration and e2e. I'll also discuss how to refactor code to make it more testable (with examples), so you can optimize and simplify Go code for robust and reliable Go tests. Lastly, I will cover race conditions to help you debug concurrency related problems. Let's write Go(od) tests!</p>",
    "persons": [
      "Nikki Attea"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 10,
    "room": "UB2.147",
    "title": "Testing apps with third-party API integrations",
    "subtitle": "",
    "track": "Testing and Automation",
    "abstract": "<p>As reliance on third-party services becomes more prevalent in our ecosystem, developers need cost-effective, secure and reliable ways to mock these services. In this talk, we will briefly examine strategies and best practices for testing apps that make heavy use of third-party API integrations.</p>",
    "description": "<p>With such a short session, I want to focus on the bits that are the most useful for developers today. Whether you’re working in a hip startup or traditional company with legacy code, you’re likely going to need ways to mock services for both dev/staging environments and testing. So this will include a (very short) demo of Unmock, the fuzz testing library that I maintain, with references to other tools like Nock and PollyJS. After this session, audience members should walk away with practical ways to improve their testing practices for REST APIs and third-party integrations.</p>",
    "persons": [
      "Carolyn Stransky (carostran)"
    ]
  },
  {
    "start": 1577898900000,
    "duration": 30,
    "room": "UB2.147",
    "title": "Testing a large testing software",
    "subtitle": "",
    "track": "Testing and Automation",
    "abstract": "<p>LAVA is an automated validation architecture primarily aimed at testing deployments of systems based around the Linux kernel on ARM devices, specifically ARMv7 and later.\nLAVA is becoming the de facto standard to test software (bootloader, kernel, userspace) on development boards (rpi, juno, beagle, ...). It's used by many projects to build large testing systems like kernelci.</p>\n\n<p>Testing a testing system like LAVA is sometimes a tricky task. In order to test LAVA we had to develop some specific tools (meta-lava, DummySYS, lavafed, ...) that I will present during this talk.</p>",
    "description": "",
    "persons": [
      "Rémi Duraffort"
    ]
  },
  {
    "start": 1577901000000,
    "duration": 20,
    "room": "UB2.147",
    "title": "Correlation analysis in automated testing",
    "subtitle": "",
    "track": "Testing and Automation",
    "abstract": "<p>Correlation Analysis is a statistical method that is used to discover if there\nis a relationship between two variables, and how strong that relationship might\nbe. A correlation coefficient is a numerical measure of such correlation.\nAccording to the Cauchy–Schwarz inequality it has a value between +1 and −1,\nwhere 1 is total positive linear correlation, 0 is no linear correlation, and −1\nis total negative linear correlation. One of the axioms of automated testing is\nthat tests are independent and in spite of that correlation coefficient should\nbe equal to 0. But often it isn't. In this work, we are going to present\na method of evaluation of tests suites quality based on correlation coefficient\nand finding their weak points. Using PC Engines open-source firmware regression\ntest results, which are based on over 140 automated tests run with 2 flavors of\nsoftware on 4 different platforms, we will show how its quality can be described\nnumerically, and how that results can be used to optimize test criteria.</p>",
    "description": "<p>As far as automated testing is considered all the tests can have only two\nexpected output values - pass or fail. Originally Pearson's correlation\ncoefficient is the covariance of the two variables divided by the product of\ntheir standard deviations - the first question was how to do it for Boolean\nvariables. We assumed that the only value that matters can be a failure of a\ntest. During the lecture, we will present how mathematical analysis can reveal\npotential flaws in test criteria by targeting cases that have a large chance to\nfail simultaneously.</p>",
    "persons": [
      "Łukasz Wcisło"
    ]
  },
  {
    "start": 1577902500000,
    "duration": 45,
    "room": "UB2.147",
    "title": "How to fail successfully and reliably",
    "subtitle": "And look good while doing it",
    "track": "Testing and Automation",
    "abstract": "<ul>\n<li>Introduction</li>\n<li>Activity: what's \"failure\", anyway?</li>\n<li>Impact of failure\n  Resources spent before failure is made\n  Resources spent to detect failure\n  Resources spent after failure is detected</li>\n<li>So what is \"fail fast\"?\n*The art and science of satisficing</li>\n<li>Activity: how would you design to fail in these scenarios?</li>\n<li>Documentation - the power of story-telling</li>\n<li>Summary: the successful way to fail fast</li>\n<li>Q/A</li>\n</ul>",
    "description": "",
    "persons": [
      "Saleem Siddiqui"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "Extending sudo in Python",
    "subtitle": "Best of both worlds",
    "track": "Python",
    "abstract": "<p>From my talk you will learn about some lesser-known features of sudo, and how you can make your security more flexible by extending sudo using Python.</p>",
    "description": "<p>Everybody knows sudo, right? Sudo allows a system administrator to give certain users the ability to run some commands as root, while logging the executed commands and their arguments. It is installed by default on almost all Linux systems, and is available for most commercial UNIX systems. Sudo allows you to fine-tune access policies, record sessions, and do extensive logging. Still, even system administrators often only know it is the “prefix” to use before entering a command requiring root privileges, and don’t realize its true powers.</p>\n\n<p>Did you know that with version 1.8 sudo changed to a plugin-based architecture? You can extend or even replace basic functionality through plugins. While plugins provide the ultimate flexibility – and there are both open source and commercial plugins for sudo available – it is not easy to extend sudo. This is why the Python plugin is under way to sudo (and will be released before FOSDEM).</p>\n\n<p>Sudo has a number of well defined APIs for plugins (https://www.sudo.ws/man/sudo_plugin.man.html). The Python plugin builds on these APIs. For example, you can set your own policies using the policy API, or access what is happening on the screen using the I/O API (used by session recording).</p>\n\n<p>Using Python for extending sudo makes development not just easier (no development environment necessary), but opens up many new possibilities. For example, you can develop a plugin which analyzes on-screen activity in real-time, and breaks the session if the infamous “rm -fr /” command appears on screen. As multiple I/O plugins can work in parallel, you do not have to give up session recording to analyze sessions in real-time from Python.</p>\n\n<p>From my talk you will learn about some lesser-known features of sudo, and how you can make your security more flexible by extending sudo using Python.</p>",
    "persons": [
      "Peter Czanik"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "Boosting Python with Rust",
    "subtitle": "The case of Mercurial",
    "track": "Python",
    "abstract": "<p>While working on the Mercurial version control system, we hit our heads against the limits of Python's performance. In this talk we will see how Python and Rust can cohabit to play off of each other's strenghts to improve a big open-source project, and what advances have been made in bridging the two languages.</p>",
    "description": "",
    "persons": [
      "Raphaël Gomès"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "How to write a scikit-learn compatible estimator/transformer",
    "subtitle": "Tips and tricks, testing your estimator, and must-watch related current developments",
    "track": "Python",
    "abstract": "<p>This is a hands-on short tutorial on how to write your own estimator or transformer\nwhich can be used in a scikit-learn pipeline, and works seamlessly with the other\nmeta-estimators of the library.</p>\n\n<p>It also includes how they can be conveniently tested with a simple set of tests.</p>",
    "description": "<p>In many data science related tasks, the use-case specific requirements require us to\nslightly manipulate the behavior of some of the estimators or transformers present\nin scikit-learn. Some of the tips and requirements are not necessarily well documented\nby the library, and it can be cumbersome to find those details.</p>\n\n<p>In this short tutorial, we go through an example of writing our own estimator,\ntest it against the scikit-learn's common tests, and see how it behaves inside\na pipeline and a grid search.</p>\n\n<p>There has also been recent developments related to the general API of the estimators\nwhich require slight modifications by the third party developers. I will cover these\nchanges and point you to the activities to watch as well as some of the private utilities\nwhich you can use to improve your experience of developing an estimator.</p>\n\n<p>The materials of the talk will be available on github as a jupyter notebook.</p>",
    "persons": [
      "Adrin Jalali"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "Why is Django 3.0 a revolution for building websites with Python?",
    "subtitle": "From WSGI to ASGI and why it matters",
    "track": "Python",
    "abstract": "<p>For almost 20 years, we relied on a CGI based protocol called WSGI to use Python to handle HTTP requests and responses software.\nBecause Python is singled threaded we relied on a couple of hacks such as Gunicorn or uWSGI to share a socket through multiple processes.\nHowever the cost of all these multiple processes was a bit heavy and error prone.</p>\n\n<p>Through Django Channels Andrew Godwin paved the way for a better way of creating web services with Python. This work landed in Django 3.0.\nLet's explore how it works and why it worth it!</p>",
    "description": "",
    "persons": [
      "Rémy Hubscher"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "Will somebody *please* tell me what's going on?",
    "subtitle": "Managing change in Python projects",
    "track": "Python",
    "abstract": "<p>How does one manage and document change in Python projects, be that new features or deprecation or removal of a feature? Let's explore some of the tools a Python developer can keep in their toolbox for just this purpose.</p>",
    "description": "<p>Software rarely stands still (unless it's TeX). Things are added, things are removed, things break and are then hopefully fixed. Managing this, from both the developer and user perspective, can be tough. In this talk we examine and compare some of the tools that one can use to make this process easier, such as 'debtcollector', 'reno' and 'towncrier', and contrast these with alternatives used in other projects. This talk would mainly be of interest to developers of open source libraries, though the same tooling can be used for any Python library or application that wishes to maintain stable interfaces and/or document changes in their product.</p>",
    "persons": [
      "Stephen Finucane"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "Discover Static Code Analysis in Python with Coala Framework",
    "subtitle": "",
    "track": "Python",
    "abstract": "<p>We, as developer, aim to provide code that, almost matches our team code style, looks better and behaves right. Static code analysis (SCA) tools are one of the way to achieves that. But, with multi-programming languages projects and all kinds of code related needs, It's difficult to address all thoses usecases without dealing with a vast majority of SCA tools.</p>\n\n<p><strong>Coala</strong> is a — language agnostic — static code analysis framework that provides a common command-line interface for linting and fixing all your code.</p>\n\n<p>It is written in Python and supports way over 50 languages in addition to language independent routines. So, instead of building new analysis tools from scratch you can now build your own custom logic and let let coala deal with the rest.</p>\n\n<p>This talk introduces the audience to the Coala Framework and guides them through how the can use it to build routines to do almost anything you want with your code.</p>",
    "description": "<h6>AUDIENCE</h6>\n\n<p>Python Developers</p>\n\n<h6>LEVEL</h6>\n\n<p>Beginner / Intermediate / Advanced</p>\n\n<h3>Notes</h3>\n\n<p>This talk is for python developers with any level of experience.</p>\n\n<p>At the en of the talk, the attendees will learn :</p>\n\n<ul>\n<li>Some Basics concepts of static code analysis</li>\n<li>The purpose and usage of Coala Framework</li>\n<li>How to create custom routines either lint, or fix their code</li>\n</ul>\n\n\n<h3>Abstract</h3>\n\n<p>We, as developer, aim to provide code that, almost matches our team code style, looks better and behaves right. Static code analysis (SCA) tools are one of the way to achieves that. But, with multi-programming languages projects and all kinds of code related needs, It's difficult to address all thoses usecases without dealing with a vast majority of SCA tools.</p>\n\n<p><strong>Coala</strong> is a — language agnostic — static code analysis framework that provides a common command-line interface for linting and fixing all your code.</p>\n\n<p>It is written in Python and supports way over 50 languages in addition to language independent routines. So, instead of building new analysis tools from scratch you can now build your own custom logic and let let coala deal with the rest.</p>\n\n<p>This talk introduces the audience to the Coala Framework and guides them through how the can use it to build routines to do almost anything you want with your code.</p>\n\n<h3>Agenda</h3>\n\n<ul>\n<li>Static Code Analysis : Quick overview</li>\n<li>Introduction to Coala and Coala-bears</li>\n<li>Coala-bears : How to build your own coala routines</li>\n<li>Going Further : What's next ?</li>\n<li>Q &amp; A</li>\n</ul>",
    "persons": [
      "Lionel Lonkap Tsamba"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "When Python meets GraphQL: Managing contributors identities in your open source project",
    "subtitle": "",
    "track": "Python",
    "abstract": "<p>SortingHat is an open source Python tool that helps to manage the different contributor identities within an open source project. Under the hood SortingHat relies on a relational database, which can be queried via SQL, command line or directly via its Python interface. However, these ways of interacting with SortingHat hinder its integration with external tools, web interfaces and new web technologies (e.g., Django, REST services). To overcome these obstacles, we have evolved SortingHat's architecture using a GraphQL model based on the Graphene-Django implementation.</p>\n\n<p>This talk describes our experience in migrating to GraphQL, from adapting the SortingHat functionalities to refactoring the unit tests. Furthermore, we comment also on lesson learned, advantages and drawbacks of using this new approach</p>\n\n<p>SortingHat is one of the core tools of GrimoireLab, an open-source software analytics platform part of CHAOSS project (Community Health Analytics Open Source Software) under the umbrella of the Linux Foundation.</p>",
    "description": "",
    "persons": [
      "Miguel-Ángel Fernández"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "Follow Your Celery Tasks",
    "subtitle": "",
    "track": "Python",
    "abstract": "<p>All Python developer who want to run asynchronous tasks should know Celery. If you have already used it, you know how great it is ! But you also discovered how it can be complicated to follow the state of a complex workflow. Celery Director is a tool  we created at OVH to fix this problem : using some concepts of Event Sourcing, Celery Director helps us to follow the whole lifecycle of our workflows. It allows us to check when a problem occurred and relaunch the whole DAG (or just a subpart if tasks are not completely idempotent). During this talk we will introduce you the different concepts of Celery Director then we'll make a demonstration of it.</p>",
    "description": "<p>All Python developer who want to run asynchronous tasks should know Celery. If you have already used it, you know how great it is ! But you also discovered how it can be complicated to follow the state of a complex workflow. Celery Director is a tool  we created at OVH to fix this problem : using some concepts of Event Sourcing, Celery Director helps us to follow the whole lifecycle of our workflows. It allows us to check when a problem occurred and relaunch the whole DAG (or just a subpart if tasks are not completely idempotent). During this talk we will introduce you the different concepts of Celery Director then we'll make a demonstration of it.</p>",
    "persons": [
      "Nicolas Crocfer"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "Asyncio: understanding async and await in Python",
    "subtitle": "",
    "track": "Python",
    "abstract": "<p>Often when asyncio is discussed, people think of it as a high performance concurrency programming paradigm for Python. In this talk however, we approach asyncio from a different angle, one that will possibly help some of you to finally get what asyncio is about. it's not only about performance, but at least as much about correctness and readability of concurrent applications.</p>",
    "description": "<p>Concurrency is hard to get right.</p>\n\n<p>Often when asyncio is discussed, people think of it as a high performance concurrency programming paradigm for Python. In this talk however, we approach asyncio from a different angle, one that will possibly help some of you to finally get what asyncio is about. it's not only about performance, but at least as much about correctness and readability of concurrent applications.</p>\n\n<p>It is known that for multithreaded applications, synchronization is hard to get right. Doing it wrong can either lead to deadlocks or broken data structures.</p>\n\n<p>We will have a look at how using asyncio is different from using threads, when it's better and what pitfalls we have.</p>\n\n<p>This talk should be a good introduction for anyone just starting with asyncio, but can also clarify things for people that are using asyncio already. I expect people to have at least used some form of concurrency, either threads or an event loop like we have in JavaScript.</p>",
    "persons": [
      "Jonathan Slenders"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "Production-time Profiling for Python",
    "subtitle": "",
    "track": "Python",
    "abstract": "<p>Learn how to scrutinize your Python application in order to optimize them and make them run faster.</p>",
    "description": "<p>Getting inside knowledge of how your Python application runs is critical in order to achieve the best performance. Profiling is a mean to achieve this: by gathering all the runtime information available about the execution of your program, you might be able to understand how to optimize it. However, profiling running code in production might be a real challenge as it requires the profiler to be noninvasive and having low overhead.</p>\n\n<p>Therefore, to profile production services, statistical profiling is the favorite analysis method. By regularly checking your program activity, you’ll be able to find production code bottlenecks down to the line of code. Profiling services that are running with real workload makes sure that you are collecting valuable data and that you are not guessing what the performance barrier might be.</p>\n\n<p>This talk explains how it’s possible to build a statistical profiler that collects information about CPU time usage, memory allocation, and other information — all that while respecting the need for low overhead, data export format, and granularity. We’ll dig into some of the operating systems and CPython internals to understand how to build the best profiler possible.</p>",
    "persons": [
      "Julien Danjou"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "Introduction to Reactive Programming with RxPY",
    "subtitle": "",
    "track": "Python",
    "abstract": "<p>Reactive Programming is an event based programming method. ReactiveX is a cross-platform implementation of Reactive Programming. It is heavily inspired from functional programming and contains many operators that allow to create, modify, and combine streams of events. Moreover it is composable and extensible. This short introduction presents Reactive Programming through RxPY, the Python implementation of ReactiveX.</p>",
    "description": "<p>The aim of this talk is to present RxPY to people that never used it, or used RxPY v1:</p>\n\n<ul>\n<li>Principles of Reactive Programming</li>\n<li>What is ReactiveX and RxPY</li>\n<li>Short history of RxPY</li>\n<li>How to deal with errors</li>\n<li>how to deal with concurrency</li>\n<li>How to document your code with marble and reactivity diagrams</li>\n</ul>\n\n\n<p>The examples of the presentations are done with RxPY v3, that has been released this summer. This release contains major improvements over RxPY v1 (v2 has never been released).</p>",
    "persons": [
      "Romain Picard"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "Introducing HTTPX",
    "subtitle": "",
    "track": "Python",
    "abstract": "<p>HTTPX is a next generation HTTP client, that supports HTTP/2 and HTTP/1.1.</p>\n\n<p>It can be used in high-performance async web frameworks, using either asyncio or trio, and is able to support making large numbers of requests concurrently.</p>\n\n<p>This talk will introduce HTTPX, demonstrate some of its features, and talk through the motivation and aims for the project.</p>",
    "description": "<p>The talk will cover:</p>\n\n<ul>\n<li>Why HTTPX exists &amp; what new functionality HTTPX brings to the table.</li>\n<li>How and where HTTPX differs from the existing <code>Requests</code> package.</li>\n<li>Examples of making parallel HTTP requests.</li>\n<li>Examples of making HTTP/2 requests.</li>\n<li>Examples of making requests directly to a web application, rather than over the network.</li>\n<li>Using Asyncio vs using Trio.</li>\n<li>A quick architecture overview.</li>\n</ul>",
    "persons": [
      "Tom Christie"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "FoxDot and the Summer of 2019",
    "subtitle": "",
    "track": "Python",
    "abstract": "<p>Yeah, this is about my last summer. But I promise to focus on the story of how I was able to do four amazing lightning talks with Python and a harmonica.</p>\n\n<p>This one is not about technical stuff, it is about finding something that python overlaps with your hobbies and sharing it back to the community.</p>",
    "description": "<p>This talk is about writing songs and playing music with python. Back in 2017 I came to know FoxDot, a python wrapper around SuperCollider, which is a super popular open source synthesizer. Since then, I have been using it to create entertaining lighting talks and would like to cover a bit more than just a lightning talk this time.</p>",
    "persons": [
      "Moisés Guimarães"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "Monads in Python: why and how?",
    "subtitle": "",
    "track": "Python",
    "abstract": "<p>In this talk I would give some motivating examples behind the idea of monads in Python, and show some implementation examples. I'd also show how we can leverage AST transformations to make the Python syntax more amenable to the use of monads. I have already given a talk on this topic during Pycon France 2018 in Lille. Unfortunately, the video footage has been lost, but the original slides can be found here: https://slides.com/v-perez/pythonic-monads-in-real-life#/</p>\n\n<p>If this talk was selected, I'd probably update it a bit to account for the feedback I received, and new ideas I may have.</p>",
    "description": "<p>In this talk I would give some motivating examples behind the idea of monads in Python, and show some implementation examples. I'd also show how we can leverage AST transformations to make the Python syntax more amenable to the use of monads. I have already given a talk on this topic during Pycon France 2018 in Lille. Unfortunately, the video footage has been lost, but the original slides can be found here: https://slides.com/v-perez/pythonic-monads-in-real-life#/</p>\n\n<p>If this talk was selected, I'd probably update it a bit to account for the feedback I received, and new ideas I may have.</p>",
    "persons": [
      "Vincent Perez"
    ]
  },
  {
    "start": 1577899800000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "repcloud",
    "subtitle": "A repacker for PostgreSQL in cloud",
    "track": "Python",
    "abstract": "<p>repcloud is a tool for repacking postgresql databases in cloud written in python3.</p>",
    "description": "<p>repcloud is a tool for repacking postgresql databases hosted in cloud.\nas pgrepack is a far better approach for repacking tables online, there are situations where is not possible to install the extension on a postgresql if it's hosted in cloud (e.g. Heroku).\nHence comes repcloud which can help in rebuilding the tables on line with the use of simple SQL and some postgreSQL magic.\nThe author will explain how the project started, the functionalities and the limitations.</p>",
    "persons": [
      "Federico Campoli"
    ]
  },
  {
    "start": 1577901600000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "Thoth - a recommendation engine for Python applications",
    "subtitle": "",
    "track": "Python",
    "abstract": "<p>Project Thoth is a recommendation engine that collects information about software packages, container images such as installation, assembling issues, runtime crashes or information about performance. This information is subsequently used in a recommendation engine that searches large state space of libraries and recommends the best possible combination of libraries suitable for your application. Let’s have a look at how such information is collected and how the large state space is explored to resolve the best application stack for your Python application based on different aspects.</p>",
    "description": "<p>Python ecosystem is experiencing significant growth and popularity especially with the hype machine learning, data science and AI are creating. As the ecosystem grows its many times not straightforward and easy to decide which libraries in which versions are the most suitable ones for an application. Project Thoth is a recommendation engine which aggregates various characteristics of Python packages, called \"observations\", and uses them to recommend the best possible software stack (a fully pinned down list of dependencies) suitable for user's runtime environment and the application purpose. In this talk, we give an overview of the project Thoth, main ideas in data aggregation and its recommendation engine. We will also show how you can benefit from Thoth's recommendations.</p>",
    "persons": [
      "Fridolín Pokorný"
    ]
  },
  {
    "start": 1577903400000,
    "duration": 25,
    "room": "UB2.252A (Lameere)",
    "title": "The Pythran compiler, 7 years later",
    "subtitle": "",
    "track": "Python",
    "abstract": "<p>7 years ago, a first file was commited in the Pythran git repo in order to create a compiler from Python to C++. The project now has hundreds of downloads per day on PyPI and has moved to a cross-platform compiler for scientific programs. This talks walks through the initial ideas, sorting out the good and the bad ones and compares the approach with other major Pythran compilers for scientific programs, most notably Cython, Pypy and Numba.</p>",
    "description": "<p>Relevant topics include :</p>\n\n<ul>\n<li>Numpy compatibility</li>\n<li>Abstraction level</li>\n<li>Backward compatibility with Python</li>\n<li>Taking advantage of hardware</li>\n<li>Translating or optimizing</li>\n<li>Community interaction</li>\n<li>Multi-platform support</li>\n<li>Benchmarking</li>\n</ul>",
    "persons": [
      "Serge Guelton (serge-sans-paille)"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 120,
    "room": "UB4.132",
    "title": "LPI Exam Session 1",
    "subtitle": "",
    "track": "Certification",
    "abstract": "<h3>LPI offers discounted certification exams at FOSDEM</h3>",
    "description": "<p>As in previous years, the Linux Professional Institute (LPI) will offer discounted certification exams to FOSDEM attendees.\nLPI offers level 1, level 2 and level 3 certification exams at FOSDEM with an almost <strong>50% discount</strong>.</p>\n\n<p>For further information and instructions see <a href=\"https://fosdem.org/certification\">https://fosdem.org/certification</a>.</p>",
    "persons": [
      "LPI Team"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 120,
    "room": "UB4.132",
    "title": "LPI Exam Session 2",
    "subtitle": "",
    "track": "Certification",
    "abstract": "<h3>LPI offers discounted certification exams at FOSDEM</h3>",
    "description": "<p>As in previous years, the Linux Professional Institute (LPI) will offer discounted certification exams to FOSDEM attendees.\nLPI offers level 1, level 2 and level 3 certification exams at FOSDEM with an almost <strong>50% discount</strong>.</p>\n\n<p>For further information and instructions see <a href=\"https://fosdem.org/certification\">https://fosdem.org/certification</a>.</p>",
    "persons": [
      "LPI Team"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 25,
    "room": "UB4.136",
    "title": "Openoffice Build system",
    "subtitle": "A walk through building OpenOffice",
    "track": "Open Document Editors",
    "abstract": "<p>This talk will be about the OpenOffice Build system. We will talk about how it works today's, issues with it. And the talk will highlight current development in this field plus where it might moves in the future. (plans of development)</p>\n\n<p>Sheduled length will be 20 min +question</p>",
    "description": "<p>Currently the build system is a mixture of dmake, gmake, ant and other tools. After a short going through I like to describe the vision we have for the future build system we are working towards.</p>",
    "persons": [
      "Peter Kovacs"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 25,
    "room": "UB4.136",
    "title": "Contributing to LibreOffice without C++ knowledge",
    "subtitle": "",
    "track": "Open Document Editors",
    "abstract": "<p>A good grasp of C++ is rather useful when it comes to improving LibreOffice. However, in the project there are vital roles and tasks that do not involve writing C++. This talk explores these other ways of contributing.</p>",
    "description": "",
    "persons": [
      "Ilmari Lauhakangas"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 25,
    "room": "UB4.136",
    "title": "coverity and oss-fuzz issue solving",
    "subtitle": "common patterns for solving reported issues",
    "track": "Open Document Editors",
    "abstract": "",
    "description": "",
    "persons": [
      "Caolán McNamara"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 25,
    "room": "UB4.136",
    "title": "LibreOffice lockdown and encryption improvements",
    "subtitle": "",
    "track": "Open Document Editors",
    "abstract": "<p>LibreOffice has builtin support for working with encrypted documents since a long time (with some recent improvements adding OpenPGP support). Further support for more fine-grained control of <em>what</em> a user can do with access-restricted documents was though missing.\nCome and see what recent improvements we implemented for LibreOffice 6.4 and 6.5, to permit fine-grained access controls to individual LibreOffice documents, matching the feature set of MS Rights Management Solution.</p>",
    "description": "",
    "persons": [
      "Thorsten Behrens"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 25,
    "room": "UB4.136",
    "title": "Prioritizing is key",
    "subtitle": "How to prioritize thousands of bugs without dying in the attempt",
    "track": "Open Document Editors",
    "abstract": "",
    "description": "",
    "persons": [
      "Xisco Fauli"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 25,
    "room": "UB4.136",
    "title": "Proposal to inspect and highlight styles in Writer",
    "subtitle": "",
    "track": "Open Document Editors",
    "abstract": "<p>Styles are the essence of a text processor. And while experts love to unleash the power of LibreOffice Writer, it’s at the same time a major source of nuisance. In particular when you receive documents from other people, it can be quite difficult to understand the applied formatting and to fix issues around. This talk presents two ideas for an improved feedback.</p>",
    "description": "",
    "persons": [
      "Heiko Tietze"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 25,
    "room": "UB4.136",
    "title": "LibreOffice Theme Changer",
    "subtitle": "An Extension for Customize LibreOffice Appearance in Easy Way",
    "track": "Open Document Editors",
    "abstract": "<p>LibreOffice is free and open source office suite software that is very popular today. LibreOffice is almost used in various user segments, ranging from personal, community, education, and even companies. It would be very interesting to be able to have LibreOffice specific themes for each segment. For this reason, we (LibreOffice Indonesia Community) took the initiative to create a special extension to manage themes in LibreOffice, we call it LO-TC (read: Lotis) LibreOffice Theme Changer.</p>",
    "description": "<p>LibreOffice is free and open source office suite software that is very popular today. LibreOffice is almost used in various user segments, ranging from personal, community, education, and even companies. It would be very interesting to be able to have LibreOffice specific themes for each segment. This of course will also further strengthen the fact LibreOffice is truly free software.\nUnfortunately, the features to set themes in LibreOffice are currently limited. As of now, LibreOffice only provides 6 theme choices for users. In the previous version, although there were many bug and problems, there was a persona theme menu that was quite interesting to me. For this reason, I and my friends in the LibreOffice Indonesia Community took the initiative to create a special extension to manage themes in LibreOffice, we call it LO-TC (read: Lotis) LibreOffice Theme Changer.\nLO-TC was originally just a simple bash script that allows users to change some visual components in LibreOffice, some of which are:</p>\n\n<pre><code>• Images in headers and footers\n• Intro or splash screen\n• Colors in the application, and\n• Icons (optional)\n</code></pre>\n\n<p>Because it is only based on bash scripts, LO-TC can only be used for Linux and Mac (with a few adjustments). Because of this limitation, we finally decided to rewrite LO-TC in the form of extensions so that it could later be used on many operating systems.\nThe various LibreOffice installation models (via repositories of distributions, snap, flatpak, etc.) pose quite daunting challenges for developers. In addition, this is our first extension project, so we need a lot of new things that we must understand in the process of working on this latest LO-TC. Current status of LO-TC development can be found here: https://github.com/libreofficeid/LO-TC-GUI</p>",
    "persons": [
      "Rania Amina"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 25,
    "room": "UB4.136",
    "title": "Creating Word Clouds with OpenOffice",
    "subtitle": "Text mining and visualization in Writer",
    "track": "Open Document Editors",
    "abstract": "<p>OpenOffice Writer offers all features needed for basic frequency analysis and visualization. We'll see how to do automated text analysis and simple word clouds without using specialized external tools.</p>",
    "description": "",
    "persons": [
      "Andrea Pescetti"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 25,
    "room": "UB4.136",
    "title": "Crowdfunding to advance open document editors",
    "subtitle": "A status report",
    "track": "Open Document Editors",
    "abstract": "<p>Productivity software like LibreOffice has long been sustained by the commercial activities of community members as well as the contributions of countless volunteers. That's also driven standards engagement, like the work around Open Document Format (ODF). But the cloud is slowly strangling the desktop support business, and spare-time volunteers may not be enough for complex, mature software. The Document Foundation has been innovating to sustain LibreOffice and ODF; this talk will describe the COSM and TDC projects, and ask whether similar approaches might sustain other open source desktop software.</p>",
    "description": "",
    "persons": [
      "Simon Phipps"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 25,
    "room": "UB4.136",
    "title": "Online Open Document Editing New Possibilities",
    "subtitle": "",
    "track": "Open Document Editors",
    "abstract": "<p>Open Document editing is, as many things in life, more and more an online action. Collabora introduced the important first steps in 2015. Since then much work has been done and LibreOffice and Collabora Online grew enormously in possibilities. This presentation will guide you trough the various areas. And in the Q&amp;A, lets talk about expectations for the future.</p>",
    "description": "",
    "persons": [
      "Cor Nouws"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 25,
    "room": "UB4.136",
    "title": "Make Online yours",
    "subtitle": "How to customize Collabora Online",
    "track": "Open Document Editors",
    "abstract": "<p>Collabora Online - The driving force behind putting LibreOffice in the cloud -is quite flexible in the means that you can alter to your personal taste without the need to change other core components.</p>",
    "description": "<p>Collabora Online - The driving force behind putting LibreOffice in the cloud - is quite flexible in the means that you can alter to your personal taste without the need to change other core components.\nTag along and see how can you customize Online's look and feel without a sweat and using mainly CSS, SVG! Don't know much about web technologies? No problem! There is no requirement to be eligible to attend, as I'll be talking in a casual fashion and with examples and hopefully illustrate each step of the way.</p>",
    "persons": [
      "Pedro Pinto Silva"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 25,
    "room": "UB4.136",
    "title": "Online: wrestling web Copy/Paste to usability",
    "subtitle": "Defeating the API and implementation disasters in copy/paste",
    "track": "Open Document Editors",
    "abstract": "<p>Overcoming the synchronous web copy/paste API's limitations in real\nbrowsers is extremely non-trivial. Come &amp; hear how we provide rich\ncopy/paste support across browsers, inside our app and hear a bit\nabout how this mess should be fixed.</p>",
    "description": "<p>Collabora has been working to provide a good user-experience for Collabora\nOnline - bringing LibreOffice to the web, and a particularly\nchallenging aspect of this has been copy/paste. One of our challenges\nis that by design we keep our document data on the server, which is at\nthe end of an asynchronous web-socket. Another challenge is the\nimpossibly baroque and arguably mis-designed set of clipboard APIs\nthat we have to work with.</p>\n\n<p>Hear a story of how we defeated the issues, as well as the somewhat\nsad UX compromises we were forced to make for the hard cases.</p>",
    "persons": [
      "Michael Meeks"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 25,
    "room": "UB4.136",
    "title": "Integrate Collabora Online with web applications",
    "subtitle": "",
    "track": "Open Document Editors",
    "abstract": "<p>Come and hear how to integrate Collabora Online – a powerful online office suite based on LibreOffice code – with web applications. Learn about how Collabora developers helped to develop solutions by extending the WOPI-like API and PostMessage API of Collabora Online.</p>",
    "description": "",
    "persons": [
      "Andras Timar"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 25,
    "room": "UB4.136",
    "title": "LibreOffice Online adoption into 1&1 Mail&Media ecosystem",
    "subtitle": "Brief overview of the open-source adoption of project LibreOffice Online into 1&1 Mail&Media ecosystem: WEB.DE, GMX, mail.com brands",
    "track": "Open Document Editors",
    "abstract": "",
    "description": "",
    "persons": [
      "Eduard Ardeleanu"
    ]
  },
  {
    "start": 1577899800000,
    "duration": 25,
    "room": "UB4.136",
    "title": "Collabora Office Android app gory details",
    "subtitle": "How we tweaked LibreOffice & the Online to get an Android app",
    "track": "Open Document Editors",
    "abstract": "<p>The LibreOffice Android app consists of the LibreOffice core as the native code and Java part that takes care of compositing of the tiles, input handling, etc.  It is hard to maintain, because everything that has been implemented in LibreOfficeKit for the Online has to be ported to Java - which is a huge amount of work.</p>\n\n<p>For the Collabora Office Android app, we have tried a new approach - to build on top of work pioneered by Tor Lillqvist for iOS: Using the native code for the rendering, the Online JavaScript for the composition of tiles, input handling, etc. and only a thin Java layer to instantiate a WebView where the JS lives.</p>\n\n<p>Come and see the current state!  And don't worry, all the work is contributed back to the LibreOffice code too :-)</p>",
    "description": "",
    "persons": [
      "Jan Holesovsky"
    ]
  },
  {
    "start": 1577901600000,
    "duration": 55,
    "room": "UB4.136",
    "title": "Lightning talk session",
    "subtitle": "",
    "track": "Open Document Editors",
    "abstract": "",
    "description": "",
    "persons": [
      "Thorsten Behrens"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 510,
    "room": "UB4.228",
    "title": "Open Source Hardware and Soldering Workshop",
    "subtitle": "",
    "track": "Workshops",
    "abstract": "<p>Open Source Hardware room with two day soldering workshops.\nDay 1 soldering workshop will be dedicated to Through Hole Technology and is good for beginners which has no experience with component soldering.</p>\n\n<p>Beside the soldering workshop we will show our latest OSHW boards we work on, you are welcome to join and show your own OSHW projects too.</p>",
    "description": "<p>With this soldering workshop we will show that assembling printed circuit boards is not hard to learn.</p>\n\n<p>We designed special board with through holes components for FOSDEM - the FOSDEM MUSIC BOX which is Arduino programmable and can play music.</p>\n\n<p>During the soldering workshop we will introduce the electronic components used in the PCB and how to identify them and how components with polarity is to be recognized.</p>\n\n<p>We will teach you the basics of soldering, how good and bad solder joints look like and what is cold solder joint.\nAt the end of the workshop you will build your own Music Box and could program it with Arduino IDE to play music.</p>",
    "persons": [
      "Tsvetan Usunov"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 30,
    "room": "UD2.119",
    "title": "Past, Present and Future of DRLM project",
    "subtitle": "",
    "track": "Backup and Recovery",
    "abstract": "<p>Brief introduction to DRLM project, it's features and news in the 2.3.x release and the presentation of the new DRLM version 3 architecture and its development state.</p>",
    "description": "<p>This talk is going to explain our vision on the future of the DRLM project, that will continue evolving in DRLM 2.x while the new DRLMv3 is being developed.</p>\n\n<p>For DRLMv3 we've prepared a complete development environment on Docker to provide an easy and fast way to contribute to the project.</p>\n\n<p>In this session we'll show all interesting new features on DRLMv2, a DRLMv3 preview of what is developed at time of this presentation, and how easy is to have a complete DRLMv3 development\nenvironment with a couple of commands.</p>\n\n<p>We don't want to spoil anything, but this talk may be of interest for developers looking for a FLOSS project to contribute to ;).</p>",
    "persons": [
      "Didac Oliveira"
    ]
  },
  {
    "start": 1577876700000,
    "duration": 30,
    "room": "UD2.119",
    "title": "Relax-and-Recover (ReaR) Basics",
    "subtitle": "with Demo on Real Hardware",
    "track": "Backup and Recovery",
    "abstract": "<p>Introducing Relax-and-Recover (ReaR) for the novice users. What is it and what can it mean for you? Is Disaster Recovery useful to consider it or not? How can ReaR assist you with DR?\nReaR can store the details about your systems on disks (NAS, USB, SAN,...) or network (PXE, NFS, CIFS,...) including the complete backup. It also creates a bootable image which you need to recreate your system from scratch.\nFurthermore, thanks to the modular concept, ReaR integrates perfectly with external backup solutions (be commercial and/or open source ones) to do the backup and restore part which makes ReaR very scalable in big enterprises. ReaR scales even with Cloud solutions and is the heart of another great project (DRLM or Disaster Recovery Linux Manager).</p>",
    "description": "<p>Relax-and-Recover (ReaR) is the de facto standard generic (bare metal) disaster recovery framework for all kind of Linux systems.\nReaR is in common use by admins for disaster recovery on thousands and thousands of Linux server systems.\nThe first part of the \"ReaR Basics\" talk will be presented by Gratien D'haese (one of the co-founders of ReaR).</p>\n\n<p>In the second part of the talk Johannes Meixner (one of the main developers of ReaR) will use his own laptop to demonstrate the ReaR disaster recovery framework.\nTherefore, Johannes will show a real live demo without safety net how ReaR is used to recover his own laptop from soft errors like deleted essential files\n(rm -r /lib...) and/or destroyed partitioning/bootloader (dd if=/dev/zero of=/dev/sdX). Come and see for yourself how ReaR can save your day!</p>",
    "persons": [
      "Gratien D'haese"
    ]
  },
  {
    "start": 1577878800000,
    "duration": 25,
    "room": "UD2.119",
    "title": "Relax-and-Recover (ReaR) Mass Deployment",
    "subtitle": "",
    "track": "Backup and Recovery",
    "abstract": "<p>Installing and configuring ReaR on thousands of Linux systems might become a nightmare to keep track what/where was done with success. Luckily using configuration management software we can do this quite easily.\nIn this talk we will guide you through a recipe on how we have done this for a multi-national company.</p>",
    "description": "",
    "persons": [
      "Gratien D'haese"
    ]
  },
  {
    "start": 1577880600000,
    "duration": 25,
    "room": "UD2.119",
    "title": "Overview of Bareos",
    "subtitle": "What is Bareos and what is new in 19.2?",
    "track": "Backup and Recovery",
    "abstract": "<p>This talk will give quick overview of Bareos and the new features in Bareos 19.2.</p>",
    "description": "<p>This talk will give quick overview of Bareos and the new features in Bareos 19.2.</p>",
    "persons": []
  },
  {
    "start": 1577882400000,
    "duration": 15,
    "room": "UD2.119",
    "title": "oVirt-Plugin for Bareos",
    "subtitle": "Backing up oVirt using Bareos",
    "track": "Backup and Recovery",
    "abstract": "<p>Backing up virtual machines in larger environments is usually not a simple task. With the new oVirt-Plugin for Bareos you can now easily backup and restore your oVirt virtual machines.\nThis talk will give a short introduction how Bareos backs up oVirt virtual machines.</p>",
    "description": "",
    "persons": []
  },
  {
    "start": 1577883600000,
    "duration": 15,
    "room": "UD2.119",
    "title": "Preserve kubernetes state using heptio velero",
    "subtitle": "",
    "track": "Backup and Recovery",
    "abstract": "<p>Stateful applications like databases needs to preserve their state as they need to save client data of one session for use in next session in persistent storage. Managing state in Kubernetes is difficult because the system’s dynamism is too chaotic for most databases to handle. So backup of data is very important especially in case of node failures, disk failures etc.\nVelero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes.\nIn this talk, I will elaborate on why, how and when to use velero for your Kubernetes cluster and volumes.</p>",
    "description": "",
    "persons": [
      "Harshita Sharma"
    ]
  },
  {
    "start": 1577884800000,
    "duration": 15,
    "room": "UD2.119",
    "title": "Percona XtraBackup Current and Future State",
    "subtitle": "What's the future for the open-source industry standard for MySQL hot backup?",
    "track": "Backup and Recovery",
    "abstract": "<p>A brief overview of the current state of the backup tool, architecture, MySQL 8.0 support, new cloud native features, and the roadmap.</p>",
    "description": "<p>During this brief lecture, I will present the roadmap for Percona XtraBackup, talk about the importance of our Cloud direction, why PXB 8.0 is a separate binary, and how PXB fits into our Percona Distribution model for 2020.</p>",
    "persons": [
      "Tyler Duzan"
    ]
  },
  {
    "start": 1577886000000,
    "duration": 15,
    "room": "UD2.119",
    "title": "Percona Backup for MongoDB: Status and Plans",
    "subtitle": "Open Source solution for consistent backups of multi-shard MongoDB",
    "track": "Backup and Recovery",
    "abstract": "<p>A brief overview of the current state of backup tool, architecture, existing features, and the roadmap.</p>",
    "description": "",
    "persons": [
      "Mykola Marzhan"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 30,
    "room": "UD2.119",
    "title": "Self-hosted server backups for the paranoid",
    "subtitle": "Using Borg, SSH, Python and FreeNAS to securely backup Linux servers",
    "track": "Backup and Recovery",
    "abstract": "<p><a href=\"https://quarkslab.com/en/\">Quarkslab</a> is a French company specializing in information security R&amp;D, consulting and software development.</p>\n\n<p>Due to strong data security constraints imposing self-hosted solutions coupled with limited resources in a fast-growth environment, data safety has been a pain point in our infrastructure.</p>\n\n<p>After our backup server failed, we decided to recreate a new backup system from scratch, adapted to our needs and using technologies we were familiar with, to backup 30+ Linux servers.</p>\n\n<hr />\n\n<p>In this talk, we will present how our old backup system failed, the key requirements we learned from this failure, and how we designed and implemented a new backup system based on <a href=\"https://borgbackup.readthedocs.io/en/stable/\">Borg Backup</a>, <a href=\"https://github.com/witten/borgmatic\">borgmatic</a>, SSH, Python and <a href=\"https://www.freenas.org/\">FreeNAS</a> to solve those requirements.</p>\n\n<p>We will conclude by listing the shortcomings and improvement points of our approach, as well as comparing our solution to seven important properties every backup system should have.</p>\n\n<hr />\n\n<p>Some interesting features of our new backup solution are strong data safety and security, fully self-hosted, using only open-source tools, simple to set up and easy to understand.</p>\n\n<p>One specific requirement we solved was for the sysadmin team to be blind to the data they backup, managing only the process itself.</p>\n\n<p>This lets people working on confidential project on dedicated and access-restricted servers to still use a centralized and resilient backup system without compromising data and server security.</p>\n\n<hr />\n\n<p>We will open-source our Ansible roles and Python scripts on <a href=\"https://github.com/quarkslab\">Github</a> before FOSDEM.</p>",
    "description": "",
    "persons": [
      "Axel Tripier"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 30,
    "room": "UD2.119",
    "title": "FASTEN: Scaling static analyses to ecosystems",
    "subtitle": "",
    "track": "Dependency Management",
    "abstract": "<p>As recent events, such as the leftpad incident and the Equifax data breach, have demonstrated, dependencies on networks of external libraries can introduce projects to significant operational and\ncompliance risks as well as difficult to assess security implications. FASTEN introduces fine-grained, method-level, tracking of dependencies on top of existing dependency management networks. In our talk, we will present how FASTEN works on top of the Rust/Cargo and Java/Maven ecosystems.</p>",
    "description": "",
    "persons": [
      "Georgios Gousios"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 30,
    "room": "UD2.119",
    "title": "There's no sustainability problem in FOSS",
    "subtitle": "Except that there is.",
    "track": "Dependency Management",
    "abstract": "<p>The community seems to be rife with conversations about our sustainability problems. Do we actually have one? We’ll lead a discussion and debate around how we as a community can think about these issues, while drawing out the nuanced aspects of each as well as their potential solutions.</p>",
    "description": "<p>When something like left-pad or event-stream happens, how much responsibility should be taken on by companies who deployed a dependency that was critical enough to their operations that removing it created immediate crisis, but not well supported or understood enough that there was any kind of mitigation strategy or backup plan?</p>\n\n<p>And yet, when you look at OpenSSL, curl, and other pieces of open source infrastructure that live in our dependency chains, there are many examples of projects that are important enough to be critical, but are under-resourced to the point that maintainers are having to make quality-of-life tradeoffs to stay on top of the project. We are responsible for ensuring that our shared dependencies are sustainably developed. But who is holding us accountable?</p>\n\n<p>If a maintainer is driving themselves to burnout because they are supporting too many of their open source projects, don’t they bear some responsibility for that choice?</p>\n\n<p>But how are we supposed to untangle which of the thousands of dependencies that we use are in most need of support - and what kind of support they prefer?</p>\n\n<p>Is there a sustainability problem in FOSS after all?</p>\n\n<p>This presentation will be co-presented with Duane O'Brien, Head of Open Source at Indeed.com, the world’s #1 jobs site.</p>",
    "persons": [
      "Carol Smith",
      "Duane O'Brien"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 30,
    "room": "UD2.119",
    "title": "Comparing dependency management issues across packaging ecosystems",
    "subtitle": "",
    "track": "Dependency Management",
    "abstract": "<p>In the last couple of years, the Software Engineering Lab of the University of Mons has extensively studied different aspects of dependency management within and across different package management ecosystems, including Cargo, npm, Packagist, Rubygems, CPAN, CRAN and NuGet. These ecosystems contain a large number of package releases with many interdependencies. They face challenges related to their scale, complexity, and rate of evolution. Typical problems are backward incompatible package updates, and the increasing proportion of fragile packages due to an excessive number of transitive dependencies.</p>",
    "description": "<p>This talk reports on our findings based on multiple empirical studies that we have conducted to understand different aspects of dependency management and their practical implications. This includes:\n- the outdatedness of package dependencies, the transitive impact of such \"technical lag\", and its relation to the presence of bugs and security vulnerabilities.\n- the impact of using either more permissive or more restrictive version contraints on dependencies.\n- the virtues and limitations of being compliant to semantic versioning, a common policy to inform dependents whether new releases of software packages introduce possibly backward incompatible changes.\n- the impact of specific characteristics, policies and tools used by the packaging ecosystem and its supporting community on all of the above.</p>\n\n<p>The contents of the talk will be adapted to the target audience of open source software practitioners, but will be primarily based on the following peer-reviewed scientific articles:\n- What do package dependencies tell us about semantic versioning? Alexandre Decan, Tom Mens. IEEE Transactions on Software Engineering, 2019. https://doi.org/10.1109/TSE.2019.2918315\n- An empirical comparison of dependency network evolution in seven software packaging ecosystems. Alexandre Decan, Tom Mens, Philippe Grosjean. Empirical Software Engineering 24(1):381-416, 2019. https://doi.org/10.1007/s10664-017-9589-y\n- A formal framework for measuring technical lag in component repositories and its application to npm. Ahmed Zerouali, Tom Mens, Jesus Gonzalez‐Barahona, Alexandre Decan, Eleni Constantinou, Gregorio Robles. Journal of Software: Evolution and Process 31(8), 2019. https://doi.org/10.1002/smr.2157\n- On the Impact of Security Vulnerabilities in the npm Package Dependency Network. Alexandre Decan, Tom Mens, Eleni Constantinou. International Conference on Mining Software Repositories, 2018. https://doi.org/10.1145/3196398.3196401\n- On the Evolution of Technical Lag in the npm Package Dependency Network.  Alexandre Decan, Tom Mens, Eleni Constantinou. International Conference on Software Maintenance and Evolution, 2018. https://doi.org/10.1109/ICSME.2018.00050</p>",
    "persons": [
      "Tom Mens"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 30,
    "room": "UD2.119",
    "title": "Building Confidence & Overcoming Insecurity",
    "subtitle": "The ultimate software supply chain self-help guide",
    "track": "Dependency Management",
    "abstract": "",
    "description": "<p>The days of having only a few open source dependencies are over. Projects often have thousands of open source dependencies in their supply chain and companies may have millions. Even worse, risk is viral -- projects inherit and pass on the risks of all their dependencies. At the same time, software is shipping more frequently.</p>\n\n<p>This creates numerous challenges for commercial and open source projects of any size -- how to discover the myriad of components being used across a range of ecosystems and scenarios, where to get high quality data to drive smart decisions, how to capture and evaluate comprehensive policies.</p>\n\n<p>Enabling high-confidence, rapid delivery, requires integrating supply chain management automation deep into the engineering system. Core to this is accurate discovery and identification of dependencies and trustworthy, high-quality compliance and security data about the discovered components.</p>\n\n<p>In this talk we detail the challenges in this space, look at various approaches such as ClearlyDefined, a crowd-sourced, open source project aimed at discovering and curating compliance data about open source components, and relate experiences running high performance, massive scale compliance systems for a wide range of open source and commercial projects.</p>",
    "persons": [
      "Jeff McAffer"
    ]
  },
  {
    "start": 1577898000000,
    "duration": 30,
    "room": "UD2.119",
    "title": "Precise, cross-project code navigation at GitHub scale",
    "subtitle": "",
    "track": "Dependency Management",
    "abstract": "<p>GitHub has recently added Code Navigation features (jump to definition and find all references) that let you navigate code directly on <a href=\"github.com\">github.com</a>. For the languages that we support, we extract and store symbol information for every named branch and tag, of every repository, public or private, with no configuration necessary. The compute and storage requirements to do this for all of the code on GitHub are quite large. In this talk, we'll discuss some of the trade-offs we've made to make this tractable at GitHub's scale, to be able to operate and monitor this service effectively, and to let us add support for new languages quickly and easily. We'll also talk about our ongoing work to extend Code Navigation to handle links that cross package and repository boundaries.</p>",
    "description": "",
    "persons": [
      "Douglas Creager"
    ]
  },
  {
    "start": 1577899800000,
    "duration": 30,
    "room": "UD2.119",
    "title": "Spack's new Concretizer",
    "subtitle": "Dependency solving is more than just SAT!",
    "track": "Dependency Management",
    "abstract": "<p>Dependency resolution is deceptively complex; simply selecting a set of compatible versions for an arbitrary network of dependencies is NP-hard.  Much effort has been spent on this problem for modern single-language ecosystems, but many of these ecosystems rely on natively compiled libraries, and dependency mangers often fail at managing the additional complexities that native libraries entail.  Further, dependency resolution has traditionally been modeled as a SAT problem, where the package manager should find <em>any</em> workable solution to satisfy package constraints.  However, <em>any</em> solution may not be good enough.  Users want the most tested, most optimized, or most secure configuration, and this is a SAT problem coupled with complex optimization.</p>\n\n<p>Spack is a package/dependency manager rapidly gaining popularity in High Performance Computing (HPC) that aims to address many of the complexities of native, multi-language, cross-platform dependency management.  Spack has recently been reworked to use Answer Set Programming (ASP), a declarative logic programming paradigm that also provides sophisticated facilities for optimization.  This talk will cover how we’ve been able to model the compiler toolchain, ISA, build options, ABI, and other constraints on native libraries. We’ll also talk about how ASP has been a useful tool for finding <em>optimized</em> dependency configurations.  This work can be used to improve dependency resolvers in general — so that they can prefer more secure or tested configurations instead of simply selecting the most recent workable versions.</p>",
    "description": "<p>Expected prior knowledge / intended audience:\nAudience should have basic knowledge of build systems and compiled languages, but we'll explain this up front with some brief background. The talk is aimed broadly -- for users, developers, packagers, researchers, package manager implementors, and HPC administrators.</p>",
    "persons": [
      "Todd Gamblin"
    ]
  },
  {
    "start": 1577901600000,
    "duration": 45,
    "room": "UD2.119",
    "title": "Package managers: resolve differences",
    "subtitle": "Lively panel discussion on package management",
    "track": "Dependency Management",
    "abstract": "<p>Package managers have become the default way for managing dependencies for most projects but they’re not without their challenges and risks. In this panel we bring together experts representing several popular package managers for a lively discussion on package management best practices, the state of package management communities, and a look forward at what we can expect to see in the future.</p>",
    "description": "<p>Join our facilitators as they put representatives of popular package managers on the spot with difficult questions on package management infrastructure, security, and compliance.</p>\n\n<p>We’ll tackle topics such as:\n* Versioning and naming\n* Knowing the full graph of packages you’re consuming\n* Best practices for securing your use of package managers\n* Finding and resolving vulnerabilities in packages you’re using\n* Malicious packages and typo-squatting\n* Meeting your open source license obligations\n* Dealing with dependencies that aren’t packages</p>\n\n<p>With package managers becoming the default way for managing dependencies, they are now a critical part of the software supply chain and while at first each package manager appears quite different, they share common requirements, are used in similar workflows, and are all targets for malicious actors.</p>\n\n<p>In this panel we will focus on those common problems so that regardless of which package manager you use, you’ll come away with a breadth of knowledge on how to securely use package managers in your software supply chain.</p>",
    "persons": [
      "William Bartholomew"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Introducing Tanka",
    "subtitle": "a scalable Jsonnet based tool for deploying and managing Kubernetes Infrastructure",
    "track": "Infra Management Devroom",
    "abstract": "<p>Introducing Tanka, a scalable Jsonnet based tool for deploying and managing Kubernetes Infrastructure</p>",
    "description": "<p>There are various tools available for managing Kubernetes resources. Whether they be Helm, Kustomize or others. Ksonnet offered a powerful approach with tremendous promise, but was discontinued by developers. In this presentation we will introduce Tanka, a drop in replacement for Ksonnet developed at Grafana labs and available on GitHub. For those not familiar with Ksonnet, we will introduce the Jsonnet language, and demonstrate its power as a way of interacting with Kubernetes. We will then demonstrate some of the enhancements we have already made to Tanka, and then explain our roadmap for the tooling, and how we believe it is already the best of breed configuration management solution for Kubernetes.</p>",
    "persons": []
  },
  {
    "start": 1577876400000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Using OpenAPI to Maximise Your Pulp 3 Experience",
    "subtitle": "",
    "track": "Infra Management Devroom",
    "abstract": "<p>Pulp (https://pulpproject.org) enables users to organize and distribute software. Now that Pulp 3.0 is generally available, it’s time to integrate it into your software delivery workflows. While the REST API is the primary integration point, it is the OpenAPI schema definition of that API that enables users to build client libraries in various languages. These clients simplify the integration with Pulp 3.</p>\n\n<p>This talk will provide a brief introduction to OpenAPI. This will be followed by a demonstration of how to use the Pulp’s OpenAPI schema to generate a Python client for Pulp’s REST API. The Python client will then be used to perform various workflows in Pulp 3.</p>",
    "description": "",
    "persons": [
      "Dennis Kliban"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Doomed are the dinosaurs!",
    "subtitle": "Dealing with diversity by utilizing the versatility of Ansible and open source",
    "track": "Infra Management Devroom",
    "abstract": "<p>It may be hard to image, but some sysadmins do not operate in ideal, tightly controlled circumstances. Apparently, not every developer, application or organization is ready for Kubernetes…</p>\n\n<p>In this presentation we will share a real world use case: deploying and configuring a brand new natural history museum. We’ll show how we built the museum with open source software and config management tools, dealing with a broad set of technologies, a tight schedule, a sector dominated by traditional organizations fixated on proprietary solutions and a whole bunch of actual fossils. We’ll show how far we’ve come, and what choices we made along the way.</p>\n\n<p>Check out this talk if you want to see how Ansible, MAAS, PlatformIO, Nextcloud and other tools were used to not just automatically deploy and configure Linux based media players, games and digital signage screens, but also to manage Cumulus Linux-based switches, OPNsense firewalls, show controllers, Arduino microcontrollers, KNX gateways, projectors and even the odd OSX machine.</p>",
    "description": "<p>It may be hard to image, but some sysadmins do not operate in ideal, tightly controlled circumstances. Apparently, not every developer, application or organization is ready for Kubernetes…</p>\n\n<p>In this presentation we will share a real world use case: deploying and configuring a brand new natural history museum. We’ll show how we built the museum with open source software and config management tools, dealing with a broad set of technologies, a tight schedule, a sector dominated by traditional organizations fixated on proprietary solutions and a whole bunch of actual fossils. We’ll show how far we’ve come, and what choices we made along the way.</p>\n\n<p>Check out this talk if you want to see how Ansible, MAAS, PlatformIO, Nextcloud and other tools were used to not just automatically deploy and configure Linux based media players, games and digital signage screens, but also to manage Cumulus Linux-based switches, OPNsense firewalls, show controllers, Arduino microcontrollers, KNX gateways, projectors and even the odd OSX machine.</p>",
    "persons": [
      "David Heijkamp"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Compliance management with OpenSCAP and Ansible",
    "subtitle": "Using OpenSCAP and Ansible for compliance management of large computing environments",
    "track": "Infra Management Devroom",
    "abstract": "<p>Managing compliance of large IT environment is complex and challenging task. Today's hybrid cloud environments are having different life cycles, when considering many short lived system like cloud instances its difficult to manage compliance on the go. This talk focuses on how OpenSCAP policies, tools and Ansible can be used to have greater control of compliance of large environments.</p>",
    "description": "<p>Compliance management with OpenSCAP</p>\n\n<p>Enterprise computing environments may consist of thousands of computer systems, having multiple applications and services. These systems are accessed by large and diverse set of users and applications. To have a greater control over security of these vast environments a standard and unified way to scan systems for compliance with security policies is needed.</p>\n\n<p>This talk focuses on using SCAP tools to retain control over large environments, scan compliance with desired policy, and use Ansible to remediate detected problems,</p>\n\n<pre><code>Install and use the SCAP Security Guide.\nEvaluate a server's compliance with the requirements specified by a policy from the SCAP Security Guide using OpenSCAP tools.\nCreate a tailoring file to adjust the policy's security checks so that they’re relevant and correct for a specific system and its use case.\nRun Ansible Playbooks, included in the SCAP Security Guide, to remediate compliance checks that failed an OpenSCAP scan.\nDemonstration\n</code></pre>",
    "persons": [
      "Amit Upadhye"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 55,
    "room": "UD2.120 (Chavanne)",
    "title": "Introduction to Metal³",
    "subtitle": "Kubernetes-native bare metal infrastructure management",
    "track": "Infra Management Devroom",
    "abstract": "<p>An introduction to Metal³, bare metal server provisioning in Kubernetes</p>",
    "description": "<p>Metal³ (https://metal3.io/) (pronounced “metal cubed”) is an open source bare metal provisioning project built on cloud-native technologies like Kubernetes and the Machine API from Kubernetes' SIG ClusterLifecycle. Metal³ can provision an operating system, manage the machine's life cycle, and ultimately decommission the machine. Metal³ can be used to provision any OS, but along with the cluster-api-provider, it can be used to install and scale bare metal Kuberentes clusters.</p>\n\n<p>We'll explain the motivations behind the project and what has been accomplished so far. This will be followed by an architectural overview and description of the Custom Resource Definitions (CRDs) for describing bare metal hosts, leading to a demonstration of using Metal³ in a cluster.</p>",
    "persons": [
      "Stephen Benjamin"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Ephemeral Environments For Developers In Kubernetes",
    "subtitle": "",
    "track": "Infra Management Devroom",
    "abstract": "<p>A key aspect of a microservice architecture is to make sure individual services work in isolation. But as a developer its also important to make sure the service works in the full system. Providing developers a way to run pre-production code in a multi-service environment is challenging.</p>\n\n<p>Making use of existing Helm charts and defaulting to production configuration does part of the work. Also important is being able to extend upon tools like Telepresence/Ksync for debugging in k8s. But while these great tools are available, what has been lacking is the \"easy to use\", single command that gives a developer a place to work with their own full, self-contained system. There are now a few open source solutions to do just that (like Garden, Acyl, &amp; Armador). In this talk, Jeff will break down how these tools work, and what makes them different.</p>",
    "description": "",
    "persons": [
      "Jeff Knurek"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Code Workload Management into the Control Plane",
    "subtitle": "What it means to be \"Kubernetes Native\"",
    "track": "Infra Management Devroom",
    "abstract": "<p>Join us to learn why Operators are the leading and default approach for managing workloads on Kubernetes. We will pull back the curtain to show you exactly what an Operator is, how to make one, and what it means to be “Kubernetes Native”.</p>",
    "description": "<p>SREs automate every aspect of workload management. Applying this mentality to the Kubernetes space, a pattern has emerged for coding such automation directly into the control plane. By adding native extensions to the Kubernetes API that are tailored to individual workloads, the Operator pattern enables infrastructure and workloads to be managed side-by-side with one set of tooling and access control.</p>\n\n<p>Join us to learn why Operators are the leading and default approach for managing workloads on Kubernetes. We will pull back the curtain to show you exactly what an Operator is, how to make one, and what it means to be “Kubernetes Native”. To close we will discuss use cases from the field; how real organizations have created and/or re-used Operators to automate their operations.</p>",
    "persons": [
      "Michael Hrivnak"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Immutable deployments: the new classic way for service deployment",
    "subtitle": "Adopt the new immutable infrastructure paradigm using your old toolbox.",
    "track": "Infra Management Devroom",
    "abstract": "<p>Immutable infrastructure paradigm is often associated with relative new concept like containers and orchestrators like kubernetes. In this talk will be illustrate how to obtain the same result but using most of the classic concepts, tools and simple cloud platforms.</p>",
    "description": "<p>In particular will be shown the usage of:</p>\n\n<ul>\n<li>Terraform for the orchestration</li>\n<li>Packer for the creation of the instance image</li>\n<li>Simple shell scripts for the provisioning</li>\n<li>DigitalOcean as cloud platform</li>\n</ul>\n\n\n<p>The illustrated approach is based on lessons learned in almost two years of using this methodology on a production service.</p>",
    "persons": [
      "Matteo Valentini"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Foreman meets Ansible",
    "subtitle": "",
    "track": "Infra Management Devroom",
    "abstract": "<p>This talk focuses on how Ansible and Foreman integrate with each other and what\nadded value can the users get when using Ansible from Foreman. It describes two\nprimary approaches of using Ansible from Foreman. The first is a traditional\nconfiguration management approach, where hosts are kept in a predefined state,\nwhile the other works in a more remote execution fashion. The talk goes over\nseveral scenarios and demonstrates how Foreman can leverage Ansible to\neffortlessly solve the issues present in the given scenarios.</p>",
    "description": "<p>This talk focuses on how Ansible and Foreman integrate with each other and what\nadded value can the users get when using Ansible from Foreman. It describes two\nprimary approaches of using Ansible from Foreman. The first is a traditional\nconfiguration management approach, where hosts are kept in a predefined state,\nwhile the other works in a more remote execution fashion. The talk goes over\nseveral scenarios and demonstrates how Foreman can leverage Ansible to\neffortlessly solve the issues present in the given scenarios.</p>",
    "persons": [
      "Adam Růžička"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Hacking Terraform for fun and profit",
    "subtitle": "",
    "track": "Infra Management Devroom",
    "abstract": "<p>Using Terraform is often simple, extending it to do what YOU want, can be challenging (or “impossible”). Want to manage unsupported resources? Maintain lots of resources? Integrate non-integrable? The talk is an advanced guide about HOW to extend, integrate and execute Terraform to get things DONE.</p>",
    "description": "<p>If you’ve been using Terraform just by following the official documentation, you are not getting all from it.</p>\n\n<p>As soon as one cloud provider announces a new service or a feature, you dream that Terraform has zero-day support for it. Well, it is not always like this, and I will show what we can do about it.</p>\n\n<p>Are you using Terraform and keep asking yourself why I should copy-paste so much? What if you need to manage more than a dozen resources with Terraform (e.g., hundreds of GitHub repositories with permissions, or hundreds of IAM users and their permissions)? How can I use Terraform to manage absolutely ANY type of resource? What is beyond Terraform modules? What is a really dynamic module and what Terraform 0.12 will help us with?</p>\n\n<p>Let's see the advanced and very unusual solutions of how Terraform can be extended, integrated, executed, or merely hacked to get the job done with the help of external open-source services and integrations.</p>",
    "persons": [
      "Anton Babenko"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Building a self healing system with SaltStack",
    "subtitle": "",
    "track": "Infra Management Devroom",
    "abstract": "<p>As the number of servers that we are responsible for increases, the ability to manage issues on those systems becomes more and more difficult.  Situations arise like log files filling up disks, failed login attempts that could be brute force attacks, and unwanted processes and services running.  Using the Beacon and Reactor systems of SaltStack, we can monitor a system for these have SaltStack restore those systems to the desired state.  In this talk, we’ll look at some real-life examples of these scenarios and how Saltstack can help to automatically heal the systems.</p>",
    "description": "<p>As the number of servers that we are responsible for increases, the ability to manage issues on those systems becomes more and more difficult.\nSituations arise like log files filling up disks, failed login attempts that could be brute force attacks, and unwanted processes and services running.</p>\n\n<p>Using the Beacon system of SaltStack, we can monitor a system for these &amp; other scenarios.\nPairing this with the Reactor system, we can have SaltStack restore those systems to the desired state.</p>\n\n<p>In this talk, we’ll look at some real-life examples of these scenarios and how Saltstack can help to automatically heal the systems.</p>\n\n<p>The talk will include:</p>\n\n<ul>\n<li>A brief introduction to Salt Stack.</li>\n<li>Using Salt Beacons to monitor aspects of a system.</li>\n<li>Using Salt Reactors to react to events from the Salt Beacons.</li>\n<li>Using those reactors to re-mediate issues as they occur.</li>\n</ul>",
    "persons": [
      "Gareth J Greenaway"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 55,
    "room": "UD2.120 (Chavanne)",
    "title": "Infrastructure testing, it's a real thing!",
    "subtitle": "",
    "track": "Infra Management Devroom",
    "abstract": "<p>Software developers have been testing their code for years. Why is it still not a common thing for infrastructure and operations people? We are in an era where it is expected everyone moves fast. Moving too fast can negatively affect our customers so it's vital that we ensure the changes we make to our infrastructure are tested like other code changes would be.</p>\n\n<p>In this talk, Paul is going to demonstrate some of the methods for testing infrastructure code. The talk will demonstrate how to establish fast feedback loops that provisions infrastructure, as well as being able to check that the code adheres to company policies, and has not drifted from the plan of record as specified in our infrastructure as code repository.</p>",
    "description": "<p>Software developers have been testing their code for years. Why is it still not a common thing for infrastructure and operations people? We are in an era where it is expected everyone moves fast. Moving too fast can negatively affect our customers so it's vital that we ensure the changes we make to our infrastructure are tested like other code changes would be.</p>\n\n<p>In this talk, Paul is going to demonstrate some of the methods for testing infrastructure code. The talk will demonstrate how to establish fast feedback loops that provisions infrastructure, as well as being able to check that the code adheres to company policies, and has not drifted from the plan of record as specified in our infrastructure as code repository.</p>",
    "persons": [
      "Paul Stack"
    ]
  },
  {
    "start": 1577899800000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Mgmt Config: Autonomous Datacentres",
    "subtitle": "Real-time, autonomous, automation",
    "track": "Infra Management Devroom",
    "abstract": "<p>Mgmt is a real-time automation tool that is fast and safe. One goal of the tool is to allow users to model and manage infrastructure that was previously very difficult or impossible to do so previously.</p>\n\n<p>The tool has two main parts: the engine, and the language. This presentation will have a large number of demos of the language.</p>\n\n<p>To showcase this future, we'll show some exciting real-time demos that include scheduling, distributed state machines, and reversible resources.</p>\n\n<p>As we get closer to a 0.1 release that we'll recommend as \"production ready\", we'll look at the last remaining features that we're aiming to land by then.</p>\n\n<p>Finally we'll talk about some of the future designs we're planning and discuss our free mentoring program that helps interested hackers get involved and improve their coding, sysadmin, and devops abilities.</p>",
    "description": "",
    "persons": [
      "James Shubin"
    ]
  },
  {
    "start": 1577901600000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Gofish - a Go library for Redfish and Swordfish",
    "subtitle": "",
    "track": "Infra Management Devroom",
    "abstract": "<p>Gofish is a Golang library for interacting with Redfish and Swordfish enabled devices.</p>",
    "description": "<p>Gofish is a Golang library for interacting with Redfish and Swordfish enabled devices. This presentation will give an overview of the current state of the library and how it can be used to manage compute and storage resources using a common, standard API.</p>",
    "persons": [
      "Sean McGinnis"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Podman - The Powerful Container Multi-Tool",
    "subtitle": "An use case driven hands-on to the container management tool Podman",
    "track": "Containers",
    "abstract": "<p>Podman is the container management tool of your choice when it comes to boosting\nday-to-day development tasks around containers. The journey of Podman started as\na drop-in replacement for docker, but nowadays it’s even more than just that.\nFor example, Podman is capable of managing pods, running containers without\nbeing root and supports fine granular configuration possibilities.</p>",
    "description": "<p>In this presentation, we will deep dive into the exciting world of Podman. We\ndiscover how Podman fits into the containers ecosystem, learn about the\narchitecture behind the project and utilize practical examples for daily\ndevelopment tasks.</p>\n\n<p>For example, we will learn how rootless containers work technically, how to\nsafely share resources within multiple containers and which benefits a\ndaemon-less container management tool like Podman provides. All these exciting\nfeatures will be explained by utilizing live demos which leaves room for an open\ndiscussion at the end of the talk.</p>",
    "persons": [
      "Sascha Grunert"
    ]
  },
  {
    "start": 1577876100000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Lazy distribution of container images",
    "subtitle": "Current implementation status of containerd remote snapshotter",
    "track": "Containers",
    "abstract": "<p>The biggest problem of the OCI Image Spec is that a container cannot be started until all the tarball layers are downloaded, even though more than 90% of the tarball contents are often unneeded for the actual workload.</p>\n\n<p>This session will show state-of-the-art alternative image formats, which allow runtime implementations to start a container without waiting for all its image contents to be locally available.</p>\n\n<p>Especially, this session will put focus on CRFS/stargz and its implementation status in containerd (https://github.com/containerd/containerd/issues/3731).\nThe plan for BuildKit integration will be shown as well.</p>",
    "description": "",
    "persons": [
      "Akihiro Suda"
    ]
  },
  {
    "start": 1577877600000,
    "duration": 30,
    "room": "UD2.208 (Decroly)",
    "title": "BPF as a revolutionary technology for the container landscape",
    "subtitle": "",
    "track": "Containers",
    "abstract": "<p>BPF as a foundational technology in the Linux kernel provides a powerful tool for systems developers and users to dynamically reprogram and customize the kernel to meet their needs in order to solve real-world problems and without having to be a kernel expert. Thanks to BPF we have come to the point to overcome having to carry legacy accumulated over decades of development grounded in a more traditional networking environment that is typically far more static than your average Kubernetes cluster. In the age of containers, they are no longer the best tool for the job, especially in terms of performance, reliability, scalability, and operations. This talk provides a few examples on how BPF allows to rethink container networking based on recent work we did in Cilium. Among others, the audience will learn about running a fully functioning Kubernetes cluster without iptables, Netfilter and thus without kube-proxy in a scalable and secure way with the help of BPF and Cilium.</p>",
    "description": "",
    "persons": [
      "Daniel Borkmann"
    ]
  },
  {
    "start": 1577879700000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Kata Containers on openSUSE",
    "subtitle": "",
    "track": "Containers",
    "abstract": "<p>Kata Containers provide a secure container runtime offering an experience close to that of native containers, while providing stronger workload isolation and host infrastructure security by using hardware virtualization technology. This is particularly useful when containers are used to host and run third-party applications. In this presentation, after a short intro to Kata, we will demonstrate how easy it is to install and use on openSUSE. We will show it in action both as part of a podman setup as well as within a full-featured Kubernetes environment.</p>",
    "description": "<p>With containers becoming not only the preferred way of deploying applications, but also the building blocks of microservice architectures, infrastructure security and workload isolation concerns are being raised. The Kata Containers Open Source project addresses these concerns by using virtualization technology, in compliance with the \"defense in depth\" design principles. It is also a very flexible, dynamic and fast-moving project, with many components that need to be integrated among each others.</p>\n\n<p>This presentation will illustrate how easy it can already be to use Kata as a container runtime on top of the openSUSE distribution. In fact, after giving a short introduction of Kata Containers and its architecture, we will provide a DEMO of how we have integrated Kata into openSUSE and how it can be used with podman to run containers in a secure and isolated fashion. As Kata is compatible with the OCI (Open Container Initiative) runtime specification, it can be used to seamlessly replace or coexist with other runtimes (e.g. runc) in existing Container Engines (podman, CRI-O, docker, ...), even inside a Kubernetes cluster. We will therefore be able to show how native containers and strongly isolated Kata containers can run together on the same platform. Finally, we will also demonstrate how to set Kata Containers up as an alternative runtime inside of a Kubernetes Cluster.</p>",
    "persons": [
      "Ralf Haferkamp"
    ]
  },
  {
    "start": 1577881200000,
    "duration": 30,
    "room": "UD2.208 (Decroly)",
    "title": "Evolution of kube-proxy",
    "subtitle": "",
    "track": "Containers",
    "abstract": "<p>Kube-proxy enables access to Kubernetes services (virtual IPs backed by pods) by configuring client-side load-balancing on nodes. The first implementation relied on a userspace proxy which was not very performant. The second implementation used iptables and is still the one used in most Kubernetes clusters. Recently, the community introduced an alternative based on IPVS.\nThis talk will start with a description of the different modes and how they work. It will then focus on the IPVS implementation, the improvements it brings, the issues we encountered and how we fixed them as well as the remaining challenges and how they could be addressed. Finally, the talk will present alternative solutions based on eBPF such as Cilium.</p>",
    "description": "",
    "persons": [
      "Laurent Bernaille"
    ]
  },
  {
    "start": 1577883300000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Container Live Migration",
    "subtitle": "",
    "track": "Containers",
    "abstract": "<p>The difficult task to checkpoint and restore a process is used in many container runtimes to implement container live migration. This talk will give details how CRIU is able to checkpoint and restore processes, how it is integrated in different container runtimes and which optimizations CRIU offers to decrease the downtime during container migration.</p>\n\n<p>In this talk I want to provide details how CRIU checkpoints and restores a process. Starting from ptrace() to pause the process, how parasite code is injected into the process to checkpoint the process from its own address space. How CRIU transforms itself to the restored process during restore. How SELinux and seccomp is restored.</p>\n\n<p>I want to end this talk with an overview about how CRIU is integrated in different container runtimes to implement container live migration.</p>",
    "description": "",
    "persons": [
      "Adrian Reber"
    ]
  },
  {
    "start": 1577884800000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Supervising and emulating syscalls",
    "subtitle": "",
    "track": "Containers",
    "abstract": "<p>Recently the kernel landed seccomp support for SECCOMP<em>RET</em>USER_NOTIF which enables a process (supervisee) to retrieve a fd for its seccomp filter. This fd can then be handed to another (usually more privileged) process (supervisor). The supervisor will then be able to receive seccomp messages about the syscalls having been performed by the supervisee.</p>\n\n<p>We have integrated this feature into userspace and currently make heavy use of this to intercept mknod(), mount(), and other syscalls in user namespaces aka in containers.\nFor example, if the mknod() syscall matches a device in a pre-determined whitelist the privileged supervisor will perform the mknod syscall in lieu of the unprivileged supervisee and report back to the supervisee on the success or failure of its attempt. If the syscall does not match a device in a whitelist we simply report an error.</p>\n\n<p>This talk is going to show how this works and what limitations we run into and what future improvements we plan on doing in the kernel.</p>",
    "description": "",
    "persons": [
      "Christian Brauner"
    ]
  },
  {
    "start": 1577886300000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Below Kubernetes: Demystifying container runtimes",
    "subtitle": "",
    "track": "Containers",
    "abstract": "<p>Today, the task of running containers involves a lot of technologies and levels of abstraction, and it can be difficult to understand, or just to keep up. How do CRI-O and containerd overlap ? Does Kata containers compete with Firecracker ? Is there any relationship between OCI and CRI ? How many different meanings can \"container runtime\" have ?</p>\n\n<p>In this talk, we will navigate this treacherous sea of overlapping technologies and acronyms that take care of running container workloads, below Kubernetes all the way down to the Linux kernel. We will present at a high-level how these technologies, interfaces and levels of abstraction combine and overlap, and hopefully clarify which are spec vs. implementation, which are complementary, and which are alternative solutions.</p>",
    "description": "<p>This talk will cover the following points:</p>\n\n<ul>\n<li>The world used to be simple: the case of Docker</li>\n<li>Interfaces: OCI and CRI</li>\n<li>More puzzle pieces: Podman, Containerd and CRI-O</li>\n<li>Workload isolation: Kata Containers, GVisor and Firecracker</li>\n<li>Q&amp;A</li>\n</ul>",
    "persons": [
      "Thierry Carrez"
    ]
  },
  {
    "start": 1577887800000,
    "duration": 30,
    "room": "UD2.208 (Decroly)",
    "title": "Linux memory management at scale",
    "subtitle": "Building the future of kernel resource management",
    "track": "Containers",
    "abstract": "<p>Memory management is an extraordinarily complex and widely misunderstood topic. It is also one of the most fundamental concepts to understand in order to produce coherent, stable, and efficient systems and containers, especially at scale. In this talk, we will go over how to compose reliable memory heavy, multi container systems that can withstand production incidents, and go over examples of how Facebook is achieving this in production at the cutting edge. We'll also go over the open-source technologies we're building to make this work at scale in a density that has never been achieved before.</p>\n\n<p>We will go over widely-misunderstood Linux memory management concepts which are important to site reliability and container management with an engineer who works on the Linux kernel's memory subsystem, busting commonly held misconceptions about things like swap and memory constraints, and giving advice on key and bleeding-edge kernel concepts like PSI, cgroup v2, memory protection, and other important container-related topics along the way.</p>",
    "description": "",
    "persons": [
      "Chris Down"
    ]
  },
  {
    "start": 1577889900000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Running full Linux systems in containers, at scale",
    "subtitle": "A look at LXD and its clustering capabilities",
    "track": "Containers",
    "abstract": "<p>LXD is a system container manager, its goal is to safely run full Linux systems at very high density and low overhead.\nContainers may be created from pre-made images, covering most Linux distributions, or by importing an existing virtual machine or physical system.</p>\n\n<p>Advanced resource control and device passthrough is available to expose as much or as little system resources to those containers.\nSnapshot and backup tooling is available to safeguard those containers and data.\nStorage pools and networks can be used to offer a variety of storage and network options to the containers.</p>\n\n<p>Management happens through a REST API with a default CLI client.\nLXD has built-in support for clustering which makes it trivial to scale a deployment to dozens of servers, all acting as one virtual LXD server.</p>\n\n<p>In this presentation, we'll go over LXD's main features through a demonstration including usage of LXD's clustering abilities, running a variety of Linux distributions and converting existing systems to containers.</p>",
    "description": "",
    "persons": [
      "Stéphane Graber"
    ]
  },
  {
    "start": 1577891400000,
    "duration": 30,
    "room": "UD2.208 (Decroly)",
    "title": "How (Not) To Containerise Securely",
    "subtitle": "Lessons Learned the Hard Way",
    "track": "Containers",
    "abstract": "<p>This talk details low level exploitable issues with container and Kubernetes deployments. We focus on lessons learned, and show attendees how to ensure that they do not fall victim to avoidable attacks.</p>",
    "description": "<p>Andy has made mistakes. He's seen even more. And in this talk he details the best and the worst of the container and Kubernetes security problems he's experienced, exploited, and remediated.</p>\n\n<p>See how to bypass security controls, exploit insecure defaults, evade detection, and root clusters externally (and more!) in this interactive and highly technical appraisal of the container and cluster security landscape.</p>",
    "persons": [
      "Andrew Martin"
    ]
  },
  {
    "start": 1577893500000,
    "duration": 30,
    "room": "UD2.208 (Decroly)",
    "title": "Using crio-lxc with Kubernetes",
    "subtitle": "",
    "track": "Containers",
    "abstract": "<p>Running application containers within Kubernetes presents a challenge to the operator for quickly handling security updates - every container must be patched, rebuilt and re-tested, and then updated separately. The slowest dev turnaround of all your containers is the fastest you can fully update your cluster.</p>\n\n<p>However, for many fixes, the application likely will not care which compatible version of a system library it is using.\nUsing AtomFS, operators can update individual libraries inside app containers without a rebuild. Containers using an AtomFS storage backend can simply be restarted after a fix is applied, and they will see it reflected in their filesystems.</p>\n\n<p>The AtomFS storage backend requires minor changes to your container runtime, and we demonstrate it with the LXC runtime and crio-lxc, an adapter to enable using LXC-based containers in Kubernetes using CRI-O.</p>\n\n<p>In this talk Tycho will cover how AtomFS works, what changes are needed to make application container builds work with AtomFS, and fix an exploit live without a rebuild.</p>",
    "description": "",
    "persons": [
      "Tycho Andersen",
      "Mike McCracken"
    ]
  },
  {
    "start": 1577895600000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Containers and Steam",
    "subtitle": "Putting games under pressure",
    "track": "Containers",
    "abstract": "<p>The availability of namespaces inside user sessions is increasing, and Valve's Steam game distribution platform is taking advantage of this for better gaming on Linux.</p>\n\n<p>A recent beta of Steam for Linux adds pressure-vessel, an experimental mechanism developed by Collabora to put games in containers. This gives the game partial isolation from various aspects of the host system, and in particular allows it to use a runtime library stack that is not entangled with the host's, with different games using different runtimes.</p>\n\n<p>Meanwhile, the unofficial Steam Flatpak app distributed on Flathub puts the entire Steam client and all of its games in a container. This gives the Steam client more thorough isolation from the host system, but all the games have to share that single container.</p>\n\n<p>In this talk, pressure-vessel developer and Flatpak contributor Simon McVittie will compare the two approaches and the challenges they encounter, and look at where Steam containers might go in the future.</p>",
    "description": "",
    "persons": [
      "Simon McVittie"
    ]
  },
  {
    "start": 1577897100000,
    "duration": 30,
    "room": "UD2.208 (Decroly)",
    "title": "Distributed HPC Applications with Unprivileged Containers",
    "subtitle": "",
    "track": "Containers",
    "abstract": "<p>We will present the challenges in doing distributed deep learning training at scale on shared heterogeneous infrastructure. At NVIDIA, we use containers extensively in our GPU clusters for both HPC and deep learning applications. We love containers for how they simplify software packaging and enable reproducibility without sacrificing performance. Docker is a popular tool for running application containers on Linux, and while it is possible to enable container workflows for users by granting them access to the docker daemon, the security impact needs to be carefully considered, especially in a shared environment. Relying on docker for the container runtime also requires a large amount of complicated boilerplate code to start multi-node jobs using the Message Passing Interface (MPI) for communication. In this presentation, we will introduce a new lightweight container runtime inspired from LXC and an associated plugin for the Slurm Workload Manager. Together, these two open-source projects enable a more secure architecture for our clusters, while also enabling a smoother user experience with containers on multi-node clusters.</p>",
    "description": "<p>There are many container runtimes available, but none met all of our needs for running distributed applications with no performance overhead and no privileged helper tools. For our use case, we built a simple container runtime called enroot - it's a tool to turn traditional container images into lightweight unprivileged sandboxes; a modern chroot. One key feature is that enroot remaps all UIDs inside the container to a single UID on the host. So, unlike runtimes which rely on <code>/etc/subuid</code> and <code>/etc/subgid</code>, with enroot there is no risk of overlapping UID ranges on a node, and no need to synchronize ranges across the cluster. It is also trivial to remap to UID 0 inside the container which enables users to safely run <code>apt-get install</code> to add their own packages. Enroot is also configured to automatically mount drivers and devices for accelerators from NVIDIA and Mellanox using enroot's flexible plugin system. Finally, enroot is highly optimized to download and unpack large docker images, which is particularly useful for images containing large applications.</p>\n\n<p>We also created a new plugin for the Slurm Workload manager which adds command-line flags for job submission. When the “--container-image” flag is set, our plugin imports a container image, unpacks it on the local filesystem, creates namespaces for the container, and then attaches the current job to these new namespaces. Therefore, tasks transparently land inside of the container with minimal friction. Users can even make use of the PMI2 or PMIx APIs to coordinate workloads inside the containers without needing to invoke mpirun, further streamlining the user experience. Currently, the plugin works with two different tools - enroot and LXC. It could be extended to other container runtimes in the future.</p>",
    "persons": [
      "Felix Abecassis",
      "Jonathan Calmels"
    ]
  },
  {
    "start": 1577899200000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Kubernetes on ARM64",
    "subtitle": "Raspberry PI 4 Kubernetes cloud for a few Euros!!",
    "track": "Containers",
    "abstract": "<p>Building a Kubernetes cloud using Raspberry PI 4.\nThe RPI4/4G offers enough memory and cpu to build an educative Kubernetes cluster.\nThe presentation will show how to put the pieces togother to get an Apache Tomcat\noperator to deploy a small web application in the build RPI4 Kubernetes cloud.</p>",
    "description": "<p>We will show:\n- how to build a kernel for RPI4, use it to make a bootable SD card for a RPI4.\n- how to configure it to use the WIFI board\n- how to prepare Docker images for ARM64\n- how to join the Kubernetes master\n- how to use weave plugin to get the Kernetes internal network\n- how to build and install the operator for Apache Tomcat.\n- then run a small webapp using the operator.</p>",
    "persons": [
      "Jean-Frederic Clere"
    ]
  },
  {
    "start": 1577900700000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Inspektor Gadget and traceloop",
    "subtitle": "Tracing containers syscalls using BPF",
    "track": "Containers",
    "abstract": "<p>I will present Inspektor Gadget and traceloop, a tracing tool to trace system calls in cgroups or in containers using BPF and overwritable ring buffers.</p>",
    "description": "<p>Many people use the “strace” tool to synchronously trace system calls using ptrace. Traceloop similarly traces system calls but asynchronously in the background, using BPF and tracing per cgroup. I’ll show how it can be integrated with systemd and with Kubernetes via Inspektor Gadget.</p>\n\n<p>Traceloop's traces are recorded in a fast, in-memory, overwritable ring buffer like a flight recorder. As opposed to “strace”, the tracing could be permanently enabled on systemd services or Kubernetes pods and inspected in case of a crash. This is like a always-on “strace in the past”.</p>\n\n<p>Traceloop uses BPF through the gobpf library. Several new features have been added in gobpf for the needs of traceloop: support for overwritable ring buffers and swapping buffers when the userspace utility dumps the buffer.</p>",
    "persons": [
      "Alban Crequy"
    ]
  },
  {
    "start": 1577902200000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Extending and embedding: containerd project use cases",
    "subtitle": "A 2020 containerd project update and description of uses",
    "track": "Containers",
    "abstract": "<p>Over the past year, projects looking to extend and embed core container runtime functionality have looked to containerd and its clean API and extension points as a valuable resource. In this talk we'll look at the projects which have extended or embedded containerd for specific use cases and how containerd has enabled these uses via its design. We will also do a brief project update for the broader container ecosystem and community.</p>",
    "description": "<p>As containerd reaches its fourth birthday, it has already been adopted as a container runtime underneath Kubernetes in public cloud providers and various developer tools and platforms. In this talk we'll look deeper at the architecture choices and clean API layer which has enabled further use of containerd as an embedded and extensible runtime in additional projects, like Amazon's Firecracker integration, Kata's use of the v2 shim API, and Microsoft Azure's creation of the Teleport registry feature. We'll also look at in-flight work with CERN, Google, and others around remote \"pre-seeded\" snapshotters which allow for significant speedups in container startup time with special-case clusters like CERN's compute cloud.</p>",
    "persons": [
      "Phil Estes"
    ]
  },
  {
    "start": 1577903700000,
    "duration": 25,
    "room": "UD2.208 (Decroly)",
    "title": "A way of GPU virtualization for container",
    "subtitle": "",
    "track": "Containers",
    "abstract": "<p>Containers are widely used in clouds due to their lightweight and scalability. GPUs have powerful parallel processing capabilities that are adopted to accelerate the execution of applications. In a cloud environment, containers may require one or more GPUs to fulfill the resource requirement of application execution, while on the other hand exclusive GPU resource of a container usually results in underutilized resource. Therefore, how to share GPUs among containers becomes an attractive problem to cloud providers. In this presentation, we propose an approach, called vCUDA, to sharing GPU memory and computing resources among containers. vCUDA partitions physical GPUs into multiple virtual GPUs and assigns the virtual GPUs to containers as request. Elastic resource allocation and dynamic resource allocation are adopted to improve resource utilization. The experimental results show that vCUDA only causes 1.015% of overhead by average and it effectively allocates and isolates GPU resources among containers.</p>",
    "description": "",
    "persons": [
      "Shengbo Song"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 25,
    "room": "UD2.218A",
    "title": "Integrating Julius Speech Recognition Engine",
    "subtitle": "",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>This presentation deals with the integration of Julius Speech Recognition Engine.</p>\n\n<p>The aim of this Proof of Concept is to have a connectionless speech engine, working on an embedded device,\nintegrated as a binding of the AGL Application Framework.\nThe recognition uses Deep Neural Network realtime decoding, and for safer results and performances purpose,\nuses a grammar.</p>\n\n<p>Julius does not support wakewords out of the box, some hacking has been done to enable it in an efficient way.\nTests have been done on Renesas' H3, and UPSquare boards.</p>",
    "description": "",
    "persons": [
      "Thierry Bultel"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 25,
    "room": "UD2.218A",
    "title": "Building Homebridge with the Yocto Project",
    "subtitle": "",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>Homebridge is a lightweight NodeJS server that emulates Apple HomeKit API. Combined with versatile plugins it allows you to make any device Homekit-compatible.\nIn the presentation you will understand how Homebridge works and how to integrated it in a custom embedded Linux distribution built with the Yocto Project and OpenEmbedded. We will go through the exact steps for leveraging the latest release of Poky, the reference system of the Yocto Project, with systemd, X11, openbox, surf web browser, nodejs, npm, Homebridge and some of its most popular plugins. Only open source software will be used, without any commercial licenses.\nPractical examples for home automation with Homebridge on Raspberry Pi and the new STM32MP1 development boards will be demonstrated. The end result is an embedded device mounted in rack with a DIN rail that provides simple and user-friendly way to manage and configure Homebridge out of the box. The talk is appropriate for beginners.</p>",
    "description": "<p>Homebridge is a lightweight NodeJS server that you can run on your home network and emulate Apple HomeKit API. Started more than 5 years ago and available at GitHub under Apache License 2.0, Homebridge has a large and vibrant open source community.\nMultiple plugins allow Homebridge to handle user's requests either via Siri or the Home app and this way to make any device Homekit-compatible. Raspberry Pi is the perfect platforms for hobbyists to install a local Homebridge instance. However, the installation of  Homebridge on Raspbian requires numerous steps and despite the excellent tutorials, users without previous Linux experience face difficulties. Another disadvantage is that Raspbian is available only as 32-bit images which doesn’t use the full capabilities of the ARMv8 64-bit processors on Raspberry Pi 3 and 4.\nThe Yocto Project and OpenEmbedded provide all required tools to create a custom Linux distribution that out of the box offers user-friendly experience for configuring Homebridge in just a few easy steps. In the this presentation we do a code review of meta-homebridge Yocto/OE layer and we will walk through the exact steps for creating a lightweight Linux distribution with graphical user interface and a web browser that acts like a kiosk. We will integrated and configure popular open source software tools such as the Linux kernel, systemd,  X11, openbox, surf web browser, nodejs, npm and of course Homebridge. Thanks to the meta-raspberrypi BSP Yocto/OE layer we will be able to unleash the full power of Raspberry Pi 3 and 4 by building 64-bit images.\nAt the end of the presentation demonstrations and tips for making an embedded device mounted in rack with a DIN rail will be shared. We will also mention the new STM32MP1 industrial grade development boards as alternatives of Raspberry Pi for this and similar projects in the maker community.\nThis talk brings benefits to the ecosystems of several open source communities. It will spread the word about Homebridge and significantly improve the getting started experience for user. Practical examples for using the Yocto Project and OpenEmbedded for makers will be revealed. As part of the development efforts for meta-homebridge Yocto/OE, a couple of new recipes, surf (simple web browser) and stalonetray (X11 system tray), have been contributed to the upstream of meta-openembedded.\nThe talk is appropriate for beginners. No previous experience is required. Hopefully, this presentation will encourage the audience to try out Homebridge and leverage their knowledge about the Yocto Project and OpenEmbedded with the example of this real-world entirely open source project.</p>",
    "persons": [
      "Leon Anavi"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 25,
    "room": "UD2.218A",
    "title": "Building an embedded VoIP network for video intercom systems",
    "subtitle": "How to leverage open standards to bring voice and video capabilities to IP hardware intercom solutions",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>IP video intercom systems combined with smartphones can leverage regular RTP/SIP VoIP technology to offer a new set of services to end-users: getting a notification when visitors press the door bell, seeing them on video before answering the call, interacting with them via voice and video and deciding to open the door, at home or anywhere else via wifi or 3G coverage.</p>\n\n<p>Linphone (a SIP user-agent) and Flexisip (a SIP proxy server) can be integrated into IP video door phones, in-house panels and video surveillance devices to build a complete VoIP network.</p>\n\n<p>Linphone and Flexisip use open standards to reliably send the audio and video streams captured from IP video intercoms to in-house devices, including smartphones and tablets, connected either to a local network or to the public internet.\nThese open source SIP-based software solutions can run perfectly on small hardware devices with reduced footprint, and can easily be integrated into GNU/Linux embedded systems, thanks to their Yocto packages.</p>\n\n<p>This lecture will describe how Linphone and Flexisip can be used together to build an embedded SIP network dedicated to home automation or video surveillance.\nThe network architecture used in these contexts can also be deployed in other areas, such as the emergency services or the Internet of Things.</p>",
    "description": "<p>Linphone and Flexisip can be integrated into IP video intercom systems to make the audio and video capabilities of a door entry panel accessible by in-house control screens and smartphones, connected either to a local network or to the public internet.</p>\n\n<p>Indeed, the linphone software fits well in embedded systems, which makes it a good candidate for being used in home automation devices, such as outdoor panels or indoor monitors, where video is to be capture or displayed.\nHowever a SIP user-agent itself is not sufficient for setting up a fully functional SIP network: we propose the use of Flexisip, which is also able to run with reduced footprint on embedded devices as well as on a large scale cloud deployment, to fork incoming calls to in-house monitoring panels, smartphones or tablets.</p>\n\n<p>When used together, Linphone and Flexisip offer advanced features for IP door phones and video monitoring systems, such as :\n-   HD video and HD voice (with support for H.264 and H.265 hardware accelerated codecs, and Opus codec)\n-   Call forking with early media video\n-   ICE, STUN and TURN support for optimised NAT traversal allowing peer-to-peer audio and video connections whenever possible\n-   secure user authentication with TLS client certificates\n-   Interconnection with push notifications systems, for reliably notifying of people ringing the door</p>",
    "persons": [
      "Elisa Nectoux"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 25,
    "room": "UD2.218A",
    "title": "ROS2: The evolution of Robot Operative System",
    "subtitle": "",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>In FOSDEM 2013, Open Robotics introduced an overview of the Robot Operating System (ROS), an open software integration framework for robots created in 2007. After more than a decade of great success, powering from Robocup teams to NASA robots in space, ROS2 was born to break any limitation detected previously by roboticians all around the globe. It's an exciting time.</p>\n\n<p>This talk will explain the design changes and technical motivations that lead to the creation of ROS2 giving a quick overview of new features present on it: multi-platform, embedded devices, real time, etc.</p>",
    "description": "",
    "persons": [
      "Jose Luis Rivero"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 25,
    "room": "UD2.218A",
    "title": "Introduction to Eclipse iceoryx",
    "subtitle": "Writing a safe IPC framework for autonomous robots and cars",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>Bosch has open sourced a true zero-copy middleware for inter-process communication\non modern robotics and vehicle computers. The shared memory based\nsolution is compatible with Linux/QNX and achieves data independent communication\nin constant time without serializing data. We would like to present our\nrecent development towards an open-source release and demonstrate our performance\nand timing benchmarks on a privately developed embedded robot.</p>",
    "description": "<p>On FOSDEM 2018 Bosch presented OpenADx, an initiative to collaborate and accelerate\nthe development of automated driving with the open source community.\nOn FOSDEM 2020 we would like to present the first project under the OpenADx\numbrella called Eclipse iceoryx TM.\nOver the course of its company history, Bosch could establish a solid understanding\nof the needs and requirements of the automotive domain in terms of liability,\nreliability, safety and determinism. In a time where highly automated driving hits\nthe road, these aspects become more and more important. An automated driving\nkit is a networked system that processes a sensor data stream in the range\nof GBytes/s. This naturally arises the need of an efficient and lightweight data\ntransfer mechanism. Our group at Bosch has its main focus on tackling exactly\nthis challenge. In order to approach the problem of distributing the high frequency,\nhigh throughput data streams on fusion and planning computers, we’ve developed\na solution which can guarantee a time constant communication channel independently\nof the size of data to be transported. Our approach is based on shared\nmemory which allows for transparently connecting the same range of memory\nbetween multiple processes and thus enables a true zero-copy communication.\nGiven the nature of shared memory, an efficient data transport can thus be realized\nsolely by passing pointers to memory addresses from publishers to subscribers.\niceoryx is fully compatible with the ROS2 and Adaptive AUTOSAR APIs\nand can be used as an implementation for both.</p>",
    "persons": [
      "Simon Hoinkis"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 25,
    "room": "UD2.218A",
    "title": "Building a low-cost test fixture",
    "subtitle": "",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>When printed circuit boards come out of the assembly line, a test fixture is required to perform functional testing and program the firmware.\nThese fixtures, called bed of nails, are sturdy setups usually built for high volume production, and can be quite costly.\nThe goal of this talk is to describe how you can build your own low cost fixture with basic PCB design skills and off the shelves components.</p>",
    "description": "<p>Functional testing of printed circuit boards (PCB) is typically done with a bed of nails fixture. The fixture holds the PCB in place over spring-loaded probes that make contact with the board’s test points.\nThe probes can be connected to an acquisition system that runs functional tests and to a programmer that flashes production firmware.\nThe Internet has tutorials and DIY kits suitable for PCBs with large test points (on a 2.54 mm grid). However as PCBs get smaller and more crowded, test points have to be smaller and closer to each other.\nThe goal of this talk is to describe how to build a test fixture with tighter requirements (test points with 0.6 mm diameter and 1.27 mm spacing) on a tight budget.</p>\n\n<p>Main talking points:\n- Making a PCB (with Kicad) to hold the probes in place and align the device under test (DUT),\n- Using a Raspberry Pi Zero to instrument the setup and communicate with the DUT,\n- Running OpenOCD on the Raspberry Pi Zero to flash the production firmware.</p>",
    "persons": [
      "Guillaume Vier"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 25,
    "room": "UD2.218A",
    "title": "How to integrate secure elements",
    "subtitle": "A visually annotated summary of Opensource compatible secure elements with instructions to integrate",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>In this half hour we study aspects of physically and cryptographically secure hardware (often termed secure element or SE) and the integration into existing circuits. We illustrate utility of such integration by inspecting a cryptocurrency wallet design, and explain the difficulty presented by nondisclosure agreements (NDA) common to industry closed adversaries. We examine several hardware devices, study their parts under a close range circuit camera, and suggest instructions on their use.</p>",
    "description": "<p>Building secure applications involves research of new technology while leveraging well known practices, for example when using ECDSA to secure systems with low power devices.</p>\n\n<p>In this half hour lecture, we study an in depth example of using cryptoaccelerated hardware to research such secure applications.</p>\n\n<p>We review common cryptography practices.</p>\n\n<ul>\n<li>Applied security paradigms</li>\n<li>Asymetric public key exchange</li>\n<li>Encryption and signing algorithms</li>\n<li>Challenges of low power computing</li>\n<li>Noncomputational security features\n...for example mechanical UI constructs</li>\n</ul>\n\n\n<p>We proposes new hardware supported techniques.</p>\n\n<ul>\n<li>Open FPGA platforms</li>\n<li>NDA unencumbered SE</li>\n<li>Circumventing black boxes</li>\n<li>Benchmark measurements</li>\n<li>Hardened serial interfaces</li>\n</ul>\n\n\n<p>We end by viewing a number of real device hardware circuits under a close range microscope, and possibly offer a device petting zoo to encourage exploration of hardware.</p>",
    "persons": [
      "Michael Schloh von Bennewitz"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 50,
    "room": "UD2.218A",
    "title": "Embedded systems, the road to Linux",
    "subtitle": "Early boot, comparing and explaining different systems.",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>As soon as you are on Linux you are generally confident with the embedded system, whatever it is.\nBut often the boot process may hide some misteries, and understanding the details may help to recover a\nbricked board or to upgrade or replace a bootloader. The explained path would start from comparing some different\nSoC's, passing from the ROM boot loader, static RAM, sdram init, secondary bootloader, and so on,\nuntil the last \"jump\" to Linux. Most common non volatile boot devices would be introduced. A basic\nknowledge would be ok for the audience.</p>",
    "description": "",
    "persons": [
      "Angelo Dureghello"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 25,
    "room": "UD2.218A",
    "title": "boot/loader — How to boot Linux and nothing else",
    "subtitle": "",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>To boot Linux on a new ARM/ARM64 platform we have to port Linux to that platform and a bootloader. Aside from the platform setup code, we need to add and maintain some device drivers to both Linux and the bootloader. We decided to avoid the extra effort and get rid of the dedicated bootloader. With a few dozen lines of assembly code, Linux Kernel and a pinch of userland tools the \"boot\" kernel was running with all the cool features we wanted! Then we used kexec(2) to start a \"full\" Linux kernel.</p>",
    "description": "<h1>How to boot Linux and nothing else</h1>\n\n<p>To boot Linux on a new ARM/ARM64 platform we have to port Linux to that platform and a bootloader. Aside from the platform setup code, we need to add at least some platform specific device drivers both Linux and the bootloader and maintain the drivers in both trees. We decided to avoid the extra effort and get rid of the dedicated bootloader.</p>\n\n<p>We took a widely available Odroid XU4 board and replaced bootloader (U-Boot) with a few dozen lines of assembly code, Linux Kernel and a pinch of userland tools. The \"boot\" kernel was running with all the cool features we wanted! Then we used kexec(2) to start a \"full\" Linux kernel.</p>\n\n<p>Dedicated bootloaders perform two types of tasks: platform specific setup and management (starting an OS, managing OS updates). We show that Linux is a better environment to implement management proccedures.</p>\n\n<p>We want to share our experience and encourage others to join our effort to use Linux Kernel as a bootloader on ARM/ARM64 platforms.</p>\n\n<h1>Target audience</h1>\n\n<p>The presentation is meant for everyone interested in how the Linux Kernel handles the boot process and especially developers who commit to arch/* directories, and bootloader developers. We present our experience with ARM Odroid XU4 board, but we expect people working on other platforms will benefit too.</p>\n\n<h1>How we want to improve the ecosystem</h1>\n\n<p>Development of new bootloading code for ARM and other embedded platforms. We believe using Linux kernel for this task is beneficial in three different ways. Less platform specific code needs to be created and maintained in different repositories.  General purpose code like filesystem drivers or network stack are maintained better in Linux than in U-Boot. This, as well as wide verity of libraries makes Linux better environment to develop advanced management functions (e.g OS updates, security checks etc.) in contemporary bootloader.</p>",
    "persons": [
      "Łukasz Stelmach"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 25,
    "room": "UD2.218A",
    "title": "PipeWire in the Automotive Industry",
    "subtitle": "",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>PipeWire has recently been adopted by Automotive Grade Linux for its implementation of the low-level platform audio service, replacing entirely previous solutions. Getting there had, of course, many challenges. In this talk, George is going to talk about how PipeWire has managed to overcome these challenges and has evolved to support automotive use cases and hardware through the design and implementation of a new, reusable, session &amp; policy management component, WirePlumber.</p>",
    "description": "",
    "persons": [
      "George Kiagiadakis"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 25,
    "room": "UD2.218A",
    "title": "WPE, The WebKit port for Embedded platforms",
    "subtitle": "",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>WPEWebKit is a WebKit flavour (also known as port) specially crafted for embedded platforms and use-cases. During this talk I would present WPEWebKit's architecture with a special emphasis on its multimedia backend based on GStreamer. I would also demonstrate various use-cases for WPE, spanning from Kiosk apps and Set-top-box user-interfaces to advanced scenarios such as Web overlays for live TV broadcasting.</p>",
    "description": "<p>WPEWebKit is designed for simplicity and performance. It allows application developers to easily deploy hardware-accelerated fullscreen (or not) browsers with multimedia support, small (both in memory usage and disk space) and light as possible, and implementing the most relevant HTML specifications.</p>\n\n<p>Traditionally WebKit ports are associated with a specific widget toolkit library (GTK, Qt, Cocoa,...) but WPEWebKit breaks with this monolithic design and thus enables a new range of use-cases. By delegating the final web page rendering to dedicated view-backends, WPEWebKit allows flexible and tight integration in a wide range of hardware platforms. We also provide a Qt5 QML plugin that can easily replace the deprecated QtWebKit-based module.</p>\n\n<p>WPEWebKit leverages GStreamer for its multiple multimedia backends, ensuring your WPEWebKit-based browser supports zero-copy hardware video decoding on the most common embedded platforms such as the Raspberry Pi, i.MX6 and i.MX8M SoCS.</p>\n\n<p>WPEWebKit can also be used in pure GStreamer applications! Thanks to the GstWPE plugin, web-pages can be \"injected\" in GStreamer pipelines as audio and video streams. This new plugin thus enables use-cases such has HTML overlays.</p>\n\n<p>WPEWebKit is an open source project with a growing community, and it is developed within the ecosystem of the WebKit project, which powers many open source and proprietary web browsers.</p>",
    "persons": [
      "Philippe Normand"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 50,
    "room": "UD2.218A",
    "title": "How Yocto extra tools help industrial project",
    "subtitle": "Yocto is not (only) bitbake",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>Yocto is the most famous \"build system\" for embedded Linux. During this conference we'll study how to use some Yocto  features to help the development of a free industrial project. We will study the eSDK (extended cross-toolchain), Ptest and Testimage (CI), Devtool and Devshell (recipe modification). We will also learn how to be in compliance with the GPLv3 / LGPLv3 licenses thanks to the \"archiver\" class (and more).</p>",
    "description": "",
    "persons": [
      "Pierre Ficheux"
    ]
  },
  {
    "start": 1577899800000,
    "duration": 25,
    "room": "UD2.218A",
    "title": "The State of PTXdist",
    "subtitle": "",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>PTXdist has been around as a build tool for embedded systems for more than 16 years now, but many new features were added in the recent time. Most importantly this includes support for kconfig diffs and layered BSPs, infrastructure for code signing and license compliance, a homepage with online documentation and a cute logo, as well as several small improvements. This talk gives an overview for new and old users over the current feature set and the core concepts behind PTXdist.</p>",
    "description": "",
    "persons": [
      "Roland Hieber"
    ]
  },
  {
    "start": 1577901600000,
    "duration": 25,
    "room": "UD2.218A",
    "title": "lognplot - logging and plotting data from micro's",
    "subtitle": "Tracing data on a modern laptop",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>Embedded systems are hard to debug. Complex systems have a lot of variables.\nWhen debugging those systems, we often log data into some files, and visualize\nthem later on, using excel, matplotlib or something else.</p>\n\n<p>This talk is about tracing and logging. What are the options we have as embedded\nsoftware developers? I will present the lognplot tool, a project to plot incoming data\non the fly.</p>",
    "description": "<p>Embedded systems are hard to debug. Complex systems have a lot of variables.\nWhen debugging those systems, we often log data into some files, and visualize\nthem later on, using excel, matplotlib or something else.</p>\n\n<p>This talk is about tracing and logging. What are the options we have as embedded\nsoftware developers? I will present the lognplot tool, a project to plot incoming data\non the fly. There are two implementations, one in python, and one in rust\nwith gtk-rs. The data is stored internally in a zoomable format, allowing\nlarge sets of data to be browsed easily.</p>\n\n<p>During the talk you will learn how to draw a chart, and how to aggregate\nlarge sets of data into summaries.</p>\n\n<p>I will demo a STM32 serial wire viewer output connected to this tool\nto enable live tracing of an embedded system.</p>",
    "persons": [
      "Windel Bouwman"
    ]
  },
  {
    "start": 1577903400000,
    "duration": 25,
    "room": "UD2.218A",
    "title": "U:Kit: Open-source software and hardware smoke detector",
    "subtitle": "",
    "track": "Embedded, Mobile and Automotive",
    "abstract": "<p>The presenter will show the audience U:Kit ( https://github.com/attachix/ukit).\nU:Kit is an open source (software and hardware) smoke and motion detector with the help of open source tools.\nU:Kit is easy to assemble, has a plastic case, and can be attached to the ceiling and used with minimum efforts also from non-technical savvy people. But that is just the tip of the iceberg.\nThe presenter will share with the audience his experience in creating the devices and talk about some of the software and hardware challenges with which he and his team had to solve.</p>",
    "description": "",
    "persons": [
      "Slavey Karadzhov"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 50,
    "room": "Janson",
    "title": "Open Source Under Attack",
    "subtitle": "How we, the OSI and others can defend it",
    "track": "Freedom",
    "abstract": "<p>Whether it is \"Open Core\", the Mongo SSPL or the Common Clause, the core ethos of open source has been under attack for some time. As those parties who seek to limit the promise of free software enjoys more and more success, the community will need stronger and more forceful tools to defend ourselves. Presenters: Michael Cheng (Facebook), Max Sills (Google), Chris Aniszczyk (Linux Foundation)</p>",
    "description": "",
    "persons": [
      "Chris Aniszczyk",
      "Max Sills",
      "Michael Cheng"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 50,
    "room": "Janson",
    "title": "Is the Open door closing?",
    "subtitle": "Past 15 years review and a glimpse into the future.",
    "track": "Freedom",
    "abstract": "<p>\"Open Source\" has been wildly successful, no doubt.</p>\n\n<p>Yet, in recent years, we have seen a massive amount of failed 'open' projects.</p>\n\n<p>Why is that?</p>\n\n<p>I have identified 10+ scenarios in which the 'Open' approach works. But what is most interesting, is that those scenarios have enabling conditions, and while those conditions are taken for granted, they are not.</p>\n\n<p>Not every 'Open' project is sustainable. Not every project is worth adopting or contributing to.</p>\n\n<p>During the presentation, we will look into what works and why, and what to expect from different 'Open' initiatives. We will cover almost everything that can be open - starting from hardware, through software, education, and we will end up covering Open Governments.</p>\n\n<p>Each sector is different, and for some of them, the 'Open' approach will not work. Come and see what I have found out in this space during my research, and evaluate whether you are working on the right project.</p>\n\n<p>Because the only resource you will never get back is time.</p>",
    "description": "<p>If you are using or contributing to a software projects, especially on your own, you certainly want to know whether your project has a chance of slipping into oblivion.\nDescribed scenarios will not only help you to answer that question, but will also help you to figure out what is most important for your project, right now.</p>\n\n<h1>The scenarios that will be covered include:</h1>\n\n<ul>\n<li>Open Hardware - DIY movements, when 'Open' is used quite cunningly to explore possibilities.</li>\n<li>Open Hardware - farming and ecology - a noble idea which is deeply flawed because it does not take into account basic economic rules.</li>\n<li>Open Content - beneficial to many people and organisation, but not necessarily for content creators.</li>\n<li>Open Education - yet another great idea, which has a hidden catch that makes or breaks it, depending on how the idea is executed.</li>\n<li>Open Access &amp;  Science - a rebellion against corporations that slow down the growth of humanity.</li>\n<li>Open Collaboration - projects run in this spirit let us advance knowledge and technical capabilities, but they do not promise financial returns.</li>\n<li>Bypass high cost of adoption - the idea that you can use software without going through a 3 months approval process was appealing in 1990. But today... that business model is going away thanks to the security folks.</li>\n<li>Open as a marketing tool - if it has 'Open' in the name, it is not open.</li>\n<li>Open Pet Projects - that deserves only mention, because there are so many such projects, but almost none of them is sustainable.</li>\n<li>Open Legislations and Governments - a futile attempt to increase transparency</li>\n<li>Open Data - very difficult to monetise, increasingly dangerous to consume</li>\n<li>Open Standards - a lot of legal uncertainties</li>\n</ul>",
    "persons": [
      "Krzysztof Daniel"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 50,
    "room": "Janson",
    "title": "The core values of software freedom",
    "subtitle": "",
    "track": "Freedom",
    "abstract": "<p>If you are a Free Software (Open Source Software) developer, do you have to follow an open development model or a certain business model? Do you have to believe in or be a supporter of socialism, capitalism, or liberalism? Do we, when we work for software freedom, have to agree on certain positions on privacy, intelligence services, the military, the climate catastrophe, nuclear power, vaccinations, or animal rights?</p>\n\n<p>Or should we accept to have different views or even allow each other not to discuss certain views, because what brings us together are other values?</p>",
    "description": "<p>I will argue that the core values of our movement are that everybody, no matter what background, can use the software for every purpose without discrimination. That everybody is allowed to study how software works. That you are always allowed to share your software with others, either to help other human beings or to make money. And that no individual, organisation or government should be forced to change their behaviour because of the software, but according to our principles, adapt and thereby improve the software for themselves and others.</p>\n\n<p>Furthermore, the talk understands itself as a plea for more respect and diversity in Free Software communities. It will be argued that while sticking to those values we should treat others decently who might have other believes, or another or no opinions at all about a topic we ourselves care about. That we should not try to put an emphasis on our other believes while working together on Free Software/Open Source Software, but instead work together with other groups or movements to bring our other topics forward.</p>",
    "persons": [
      "Matthias Kirschner"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 50,
    "room": "Janson",
    "title": "Why open infrastructure matters",
    "subtitle": "",
    "track": "Freedom",
    "abstract": "<p>A lot of open source developers choose to deploy their software on infrastructure based on proprietary software. Behind this apparent paradox is the need to adapt to changing environments, adopt new technologies fast, and use increasing amounts of computing power. Open infrastructure (computing, networking and storage infrastructure based on open source software) has a lot to offer, but it's easy to overlook if you don't take the time to take a step back and analyze the situation rationally. In this talk, Thierry Carrez, VP of Engineering at the OSF, explains all the reasons why open infrastructure matters, and why it makes sense for you to adopt it today.</p>",
    "description": "<p>Outline of this talk:</p>\n\n<ul>\n<li>Infrastructure (what do we mean by infrastructure, and why it matters in the general evolution of computing)</li>\n<li>Open (key benefits of using open source software in general, and openly-developed community-led projects in particular)</li>\n<li>Capabilities, Compliance and Cost (extra benefits of using open source software for providing infrastructure)</li>\n<li>Interoperability (benefits of using the same open source projects across providers)</li>\n<li>Future-proof (open source allows to invest in adaptive communities rather than static products)</li>\n<li>Enabling innovation everywhere (infrastructure should not be only provided by a couple of Internet giants like Amazon and Google)</li>\n</ul>",
    "persons": [
      "Thierry Carrez"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 50,
    "room": "Janson",
    "title": "Why the GPL is great for business",
    "subtitle": "Debunking the current business licensing discussion",
    "track": "Freedom",
    "abstract": "<p>In the past few years we saw a lot of discussions around free software licenses and why they are bad for companies. This talk debunks this claim and shows how free software licenses are actually great for startups if done right.</p>",
    "description": "<p>In the last few years we saw a lot of discussion in the open source and free software startup space around licenses. Several companies stepped forward and claimed that it’s not possible to build a working company on top of a free software product. Some changed the license of their product to proprietary license like the Commons Clause or the Business Source License. They claim that this is needed to ‘save’ free software. This talk describes why this is fundamentally wrong. It’s possible to build a working startup and company on top of a free software product. This talk discusses how companies like Red Hat, SUSE and Nextcloud manages to have a 100% free software product including a big contributor community but is still able to pay developers and grow.</p>",
    "persons": [
      "Frank Karlitschek"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 50,
    "room": "Janson",
    "title": "United Nations Technology and Innovation Labs",
    "subtitle": "Open Source isn't just eating the world, it's changing it",
    "track": "Freedom",
    "abstract": "<p>Amanda is the chair of the United Nations Technology and Innovation Labs' Open source and IP Advisory Board and will give an overview of the work being done by the labs and take the audience through a couple of case studies using data and blockchain for good in an open way.</p>",
    "description": "<p>Amanda will explain the goals and workings of the UNTIL Open Source Advisory Board and the opportunities for further community engagement with the labs, allowing a wider open community to be built supporting the labs through contributions and mentoring and the potential opportunities for Fellowship placings within the labs.\nShe will also look at the first projects working in the labs, with the Advisory and the open data and blockchain models that they have applied to these, using case studies.</p>",
    "persons": [
      "Amanda Brock"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 50,
    "room": "Janson",
    "title": "Regaining control of your smartphone with postmarketOS and Maemo Leste",
    "subtitle": "Status of Linux on the smartphone",
    "track": "Freedom",
    "abstract": "<p>Linux mobile software (and GNU/Linux distributions) are not widely available on smartphones yet. This talk covers why it is desirable to have GNU/Linux (not: Android or Android-based) on the smartphone, what current state of various software attempts at Linux on the smartphone is, the progress they've been making, and will also dive into the available old and new hardware (including the PinePhone and Librem 5) to run the software &amp; distributions on.</p>",
    "description": "<p>Smartphones running regular (F)OSS Linux distributions are not common. We intend to provide an overview of the current Linux FOSS mobile stacks, distributions that package/provide the mobile stacks and to discuss the hardware that one can use to run this software on. We will provide additional details for the postmarketOS distribution and for Maemo Leste (Debian based FOSS mobile software). We also hope to go into some detail about the upcoming PinePhone (https://www.pine64.org/pinephone/)</p>\n\n<p>postmarketOS is a distribution based on alpine, with a focus on minimalism, security and mobile software. postmarketOS supports many old and new smartphones with varying degrees of support, and also packages/ships with various mobile software suites like Plasma Mobile, Maemo/Hildon, Phosh and more.</p>\n\n<p>Maemo Leste is based on Maemo Fremantle (from the Nokia N900 days), but completely open source. It's a repository on top of Debian/Devuan that pulls in the entire Maemo/Hildon user interface and suite of applications. Building on top of a proven set of interfaces, Maemo Leste also aims to be mostly compatible with Maemo the way many people might remember it, with a modern twist.</p>\n\n<p>Pine64 (known for ARM laptops and SBC (Single Board Computers) has decided to get into the mobile business with the Pine Tab and the Pine Phone device. Aiming to deliver developer devices in 2020Q1 and enthusiastic end-user devices in 2020Q2, they've energized software developers writing mobile interfaces for Linux and have been producing a mobile phone at remarkable pace. We will show the Pine64 device and discuss the current state of Linux support on the device.</p>\n\n<p>We plan to give live demos during the presentation, but will have pre-recorded videos as fallback.</p>",
    "persons": [
      "Merlijn B. W. Wajer",
      "Bart Ribbers"
    ]
  },
  {
    "start": 1577901300000,
    "duration": 20,
    "room": "Janson",
    "title": "Closing FOSDEM 2020",
    "subtitle": "",
    "track": "Keynotes",
    "abstract": "<p>Some closing words.  Don't miss it!</p>",
    "description": "",
    "persons": [
      "FOSDEM Staff"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "SECCOMP your PostgreSQL",
    "subtitle": "",
    "track": "Databases",
    "abstract": "<p>SECCOMP (\"SECure COMPuting with filters\") is a Linux kernel syscall filtering mechanism which allows reduction of the kernel attack surface by preventing (or at least audit logging) normally unused syscalls. Recent security best-practices recommend, and certain highly security-conscious organizations are beginning to require, that SECCOMP be used to the extent possible. The major web browsers, container runtime engines, and systemd are all examples of software that already support SECCOMP.</p>\n\n<p>This talk covers SECCOMP applied to PostgreSQL via 2 different methods -- namely top-down using systemd, and at the session level using a PostgreSQL extension called pgseccomp. The two methods will be explained and compared. We will also discuss how and why the two methods might be used in conjunction. Finally, a process to determine the list of expected/legitimate PostgreSQL kernel syscalls is described.</p>",
    "description": "",
    "persons": [
      "Joe Conway"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "dqlite: High-availability SQLite",
    "subtitle": "An embeddable, distributed and fault tolerant SQL engine",
    "track": "Databases",
    "abstract": "<p>SQLite has proven extremely successful at providing applications with a powerful, portable and embeddable SQL engine that can handle most of their data storage needs.</p>\n\n<p>Unfortunately, SQLite is neither replicating nor fault tolerant. These two features are however very important for the rising Edge/IoT market: dqlite delivers both of them.</p>\n\n<p>dqlite is a C library which exposes a SQLite database over the network and replicates it using the Raft algorithm, with built-in automatic failover.</p>\n\n<p>It allows to build and operate a fault-tolerant cluster of nodes each running an instance of the user application.</p>\n\n<p>dqlite was created to support clustering in the LXD container management project, where it has been used for over a year. In this talk we will look at its design, implementation and various use cases.</p>",
    "description": "<p>Distributed systems are ubiquitous these days: we need to commoditize the underlying technologies and algorithms, making them easy to consume. The dqlite project offers application developers the opportunity to build on top of a storage engine which is as easy and convenient as a plain SQLite database, but also meets higher durability and fault tolerance requirements.</p>",
    "persons": [
      "Free Ekanayaka"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "MySQL Goes to 8!",
    "subtitle": "",
    "track": "Databases",
    "abstract": "<p>The latest and greatest version of MySQL is MySQL 8.</p>\n\n<p>Currently the most used version of MySQL is MySQL 5.7. This talk will highlight what is new in MySQL 8.0 - a huge step forward for our users. MySQL 8.0 delivers significant improvements on all fronts, such as dramatically improved SQL, GIS, and JSON support. The talk will also cover the MySQL Document Store (MySQL = NoSQL + SQL) and MySQL InnoDB Cluster (HA out of the box) as well as MySQL Shell which ensures power, freedom, and flexibility for the Developer.</p>",
    "description": "",
    "persons": [
      "Geir Høydalsvik"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "SWIM - Protocol to Build a Cluster",
    "subtitle": "SWIM gossip protocol, its implementation, and improvements",
    "track": "Databases",
    "abstract": "<p>SWIM - is a relatively new protocol to discover and monitor cluster nodes, to disseminate events and data between them. The protocol is extremely lightweight, decentralised, and its speed and load per node do not depend on cluster size.</p>\n\n<p>The protocol solves several tasks at once. First - build and keep up to date topology of a cluster without explicit configuration. The task is quite intricate because:</p>\n\n<ul>\n<li>new just started nodes know nothing about others, and they should somehow discover them;</li>\n<li>already working nodes can fail, and it should be detected so as to change a master, or evict an unrecoverable node from the cluster, or restart it.</li>\n</ul>\n\n\n<p>According to the protocol, cluster nodes broadcast packets and send p2p ping requests. Broadcast helps to discover new nodes, p2p pings help to detect failure of a known node.</p>\n\n<p>A second task - events dissemination in a cluster. Event is a node failure; UUID change; IP address update; new node appearance - anything that affects cluster state. Sometimes users define their own event types. When a node learns about an event, it needs to disseminate the event to other nodes. SWIM protocol describes an algorithm how to detect and disseminate events, and gives the following guarantees:</p>\n\n<ul>\n<li>it takes a constant time to learn about an event on at least one node in the cluster;</li>\n<li>it takes logarithmic from cluster size time to disseminate that event to each node of the cluster.</li>\n</ul>\n\n\n<p>In the talk I tell about how SWIM works, how and with which essential improvements it was implemented, how to use SWIM, and what are the practical performance results.</p>\n\n<p>Implementation is a part of Tarantool DBMS. Tarantool is the biggest Russian Open-Source DBMS. Tarantool currently goes toward better scalability, improvements in horizontal scaling, in cluster-wide calculations, and better cluster management. In scope of that roadmap SWIM protocol implementation was recently released.</p>",
    "description": "",
    "persons": [
      "Vladislav Shpilevoy"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "Improving protections against speculative execution side channel",
    "subtitle": "",
    "track": "Miscellaneous",
    "abstract": "<p>Speculative execution side channel methods pose new challenges to not only system administrators, users and security experts but also to developers. Developers can use different techniques to harden their code and reduce the feasibility of a possible malicious actor using these methods to leak secrets. But what is a secret? How can someone leak any of my data using these methods? This presentation introduces some architectural concepts that these methods use. It will also present how these methods work and how malicious actors might try to infer data from other users and codes. We will introduce some of the techniques that developers can use for mitigation, together with details about specific challenges that developers of different programming languages might face when implementing these mitigation techniques. Finally, we will present some of the mitigations that we are introducing in software to help ensure that these techniques can not be exploited in production environments.</p>",
    "description": "<p>No security or computer architecture background is required. Basic to intermediate programming skills are recommended.</p>\n\n<p>Attendees will come away with a better understanding of what speculative execution side channel issues are, how they work, and what they really mean for developers.</p>",
    "persons": [
      "David Stewart"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "SaBRe: Load-time selective binary rewriting",
    "subtitle": "",
    "track": "Miscellaneous",
    "abstract": "<h2>Abstract</h2>\n\n<p>Binary rewriting is a technique that consists in disassembling a program to modify its instructions, with\nmany applications, e.g. monitoring, debugging, reverse engineering and reliability. However, existing solutions suffer from well-known\nshortcomings in terms of soundness, performance and usability.</p>\n\n<p>We present <em>SaBRe</em>, a novel load-time framework for selective binary rewriting. SaBRe rewrites specific constructs of\ninterest — mainly system calls and function prologues — when the program is loaded into memory. This enables users to intercept those constructs at runtime\nvia a modular architecture allowing custom plugins to be linked with SaBRe using a simple and flexible\nAPI. We also discuss the theoretical underpinnings of disassembling and rewriting, including conditions for\ncoverage, accuracy, and correctness; and how they affect SaBRe.</p>\n\n<p>We developed two backends for SaBRe — one for x86_64 and one for RISC-V — which were in turn used to\nimplement two open-source plugins: a fast system call tracer and a fault injector. Our evaluation\nshows that SaBRe imposes little performance overhead, between 0.2% and 4.3% on average.\nIn addition to explaining the architecture of SaBRe and demonstrating its performance,\nwe also show on a concrete example how easy creating a new plugin for SaBRe is.</p>\n\n<p>SaBRe is a free open-source software released under the GPLv3 license\nand originally developed as part of the Software Reliabilty Group at Imperial College London.</p>",
    "description": "<h2>Introduction</h2>\n\n<p>The goal of binary rewriting is to add, delete and replace\ninstructions in binary code. There are two main types of binary\nrewriting techniques: static and dynamic.\nIn static binary rewriting, the binary file is statically rewritten on disk, while\nin dynamic binary rewriting it is rewritten in memory, as the\nprogram executes.</p>\n\n<p>Static binary rewriting has the advantage\nthat the rewriting process does not incur any overhead during\nexecution, as it is performed before the program starts running.\nHowever, static binary rewriting is hard to get right: creating a\nvalid modified executable on disk is challenging, and correctly\nidentifying all the code in the program is error-prone in the\npresence of variable-length instructions and indirect jumps.</p>\n\n<p>By contrast, dynamic binary rewriting modifies the code in\nmemory, during program execution. This is typically accomplished by translating one basic block at a time and caching the\nresults, with branch instructions modified to point to already\ntranslated code. Since translation is done at runtime, when the\ninstructions are issued and the targets of indirect branches are\nalready resolved, dynamic binary rewriting does not encounter\nthe challenges discussed above for static binary rewriting.\nHowever, the translation is heavyweight and incurs a large\nruntime overhead.</p>\n\n<p>In this presentation, we introduce SaBRe, a system that implements\na novel design point for binary rewriting. Unlike prior techniques, SaBRe operates at load-time, after the program is\nloaded into memory, but before it starts execution. Like static\nbinary rewriting techniques, SaBRe rewrites the code in-place,\nbut the translation is done in memory, as for dynamic binary\nrewriting. To achieve a high level of both performance and reliability,\nSaBRe relies by default on trampolines, which are extremely efficient\nand can be used more than 99.99% of the time, and only falls back\non illegal instructions triggering a signal handler for pathological\ncases.</p>\n\n<p>The main limitation of SaBRe is that it is designed\nto rewrite only certain types of constructs, namely system\ncalls (including vDSO), function prologues and some architecture-\nspecific instructions (e.g. RDTSC in x86). However, as we illustrate\nlater on, this is enough to support a variety of tasks, with\nmuch lower overhead than with dynamic binary rewriting and\nwithout incurring the precision limitations of static binary\nrewriting.</p>\n\n<p>We implemented two binary rewriters based on this design:\none for x86 64 and one for RISC-V code. Both rewriters\nfeature a flexible API, which we used to implement three\ndifferent plugins: a fast system call tracer, a multi-version\nexecution system (not open-sourced yet) and a fault injector.\nIn summary, our main contributions are:\n1. A new design point for selective binary rewriting which\ntranslates code in memory in-place at load time, before\nthe program starts execution.\n2. An implementation of this approach for two architectures, one for x86 64 and the other for RISC-V.\n3. A comprehensive evaluation using two open-source plugins: a fast <code>strace</code>-like\nsystem call tracer and a fault injector.\n4. An extremely simple API that can be leveraged by users to\nimplement and integrate their own plugins.</p>",
    "persons": [
      "Paul-Antoine Arras"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "The year of the virtual Linux desktop",
    "subtitle": "",
    "track": "Miscellaneous",
    "abstract": "<p>We made the Linux desktop work in VR. Join me to hear about the history and future of xrdesktop and the FOSS XR landscape.</p>",
    "description": "<p>With tracked controllers, heads and hands, AR and VR introduced the requirement for a new set of user interactions. In this talk you will learn about existing implementations and how the classical UX model with keyboard and mouse translates to these new concepts. I will highlight the technical aspect of these requirements and how they were solved in xrdesktop. Featuring 3D window management and synthesis for traditional input, xrdesktop is a software stack that integrates VR in the GNOME and KDE desktop environments. You will also get an overview of the history and status of Open Source in AR and VR.</p>",
    "persons": [
      "Lubosz Sarnecki"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 50,
    "room": "K.1.105 (La Fontaine)",
    "title": "Making & Breaking Matrix's E2E encryption",
    "subtitle": "In which we exercise the threat model for Matrix's E2E encrypted decentralised communication",
    "track": "Miscellaneous",
    "abstract": "<p>Matrix is an open protocol and open network for decentralised real-time communication; shifting control over communication from the big proprietary silos back to the general population of the Internet. In 2016 we added E2E Encryption based on the Double Ratchet, and since then have been working away on getting the encryption so polished that we can transparently turn it on by default everywhere.  In this talk, we'll show how we have finally done this, what the blockers were, and then try to smash the encryption to pieces to illustrate the potential attacks and how we mitigate them.</p>",
    "description": "<p><a href=\"https://matrix.org\">Matrix</a> is an ambitious project to build a open decentralised real-time communication network; providing an <a href=\"https://matrix.org/docs/spec\">open standard protocol</a> and <a href=\"https://matrix.org/docs/projects/try-matrix-now/\">open source reference implementations</a>, letting anyone and everyone spin up a Matrix server and retake control of their real-time communication. Matrix is looked after by the non-profit <a href=\"https://matrix.org/foundation\">Matrix.org Foundation</a>, and as of Oct 2019 we have over 11.5M addressable users and around 40K servers on the public network.</p>\n\n<p>Over the course of 2019 we spent a huge amount of time finalising Matrix's end-to-end encryption so we could finally turn it on by default without compromising any of the behaviour users had grown accustomed to in non-encrypted rooms.  Specifically, the main remaining blockers were:\n * Ability to search in E2E encrypted rooms (now solved by <a href=\"https://github.com/matrix-org/seshat\">Seshat</a>: a Rust-based full-text-search engine embedded into Matrix clients)\n * Ability to get compatibility with non-E2E clients, bots and bridges (now solved by <a href=\"https://github.com/matrix-org/pantalaimon\">pantalaimon</a>: a daemon which offloads E2E encryption)\n * Reworking the whole encryption UI to expose cross-signing to radically simplify key verification (including QR-code scanning for simplicity)\n * Ability to receive notifications in E2E encrypted rooms.</p>\n\n<p>However, we have finally got there, and this talk will demonstrate how the final E2EE implementation works; the final problems we had to solve; the threat model we have implemented; and how we're doing on rolling it out across the whole network.  More interestingly, we will then demonstrate a variety of attacks against the encryption (e.g. shoulder-surfing QR codes during device verification; MITMing TLS; acting as a malicious server implementation; global passive adversary) to demonstrate how well we handle them.</p>",
    "persons": [
      "Matthew Hodgson"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Open Source - Killing standards organizations or saving them",
    "subtitle": "Open source and standards join forces for mutual benefit",
    "track": "Lightning Talks",
    "abstract": "<p>Open source communities move quickly, value running code, and docs are best effort at best. Standards move slowly, value precise specs, and negotiate compromises for broad alignment. Given these differences, why would open source communities fraternize with standards orgs? Standards orgs such as IETF and MEF realize they need to change to remain relevant. By embracing open source, standards orgs benefit from the speed and collaborative spirit of open source and get timely feedback on the clarity and correctness of standards as they evolve in parallel with running code. Open source communities gain users, address additional use cases, and gain the stability of standards to ease integration efforts and avoid forks. This session explores this evolution in standards orgs, highlights areas of mutual interest, and shares ideas on the benefit of closer collaboration.</p>",
    "description": "<p>By collaborating with standards organizations and supporting existing and evolving standards, the open source community gains users, address a larger set of use cases, and benefit from the stability of standards that can help avoid harmful forking and ease integration efforts. Standards orgs benefit from the speed and collaborative spirit characteristics of open source, and they gain timely and critical feedback on the clarity and correctness of their standards as they evolve iteratively and in parallel with the open source code. The end result is open source code that is more consumable by industry, and standards that are more consumable by the open source community.</p>",
    "persons": [
      "Charles Eckel"
    ]
  },
  {
    "start": 1577874000000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "emissions API",
    "subtitle": "a service to easily access air quality data from remote sensing",
    "track": "Lightning Talks",
    "abstract": "<p>The European Space Agency’s Sentinel-5P satellite is built to monitor air quality data (carbon hydroxide, sulfur monoxide, ozone, …). All data gathered are publicly available …if you know what to do with those data sets, great, but if not:</p>\n\n<p>Emissions API’s mission is to provide easy access to this data without the need of being an expert in satellite data analysis and without having to process terabytes of data.</p>\n\n<p>This way, we hope to empower others to easily build apps that use this data – e.g. visually showing emissions of countries over time.</p>",
    "description": "<p>Achievements of climate goals are so far only verifiable for a very small group of people with specialized know-how. As a result, public discussion remains abstract and elusive for many people. Easy access to emissions data provides a more general audience with the opportunity to form a fact-based opinion. For example, one could evaluate the effectiveness of environmental regulations – such as diesel driving bans in inner cities or new sulfur limits in shipping–by comparing actual measurements from before and after on a map.</p>\n\n<p>Emissions API is a solution that provides simple access to emissions data of climate-relevant gases. For this purpose, data of the European Space Agency’s Sentinel-5P earth observation satellite will be prepared in such a way that it allows programmers easy access without the need to have a scientific background in the field.</p>\n\n<p>The project strives to create an application interface which lowers the barrier to use the data for visualization and/or analysis.\nTackling the problem</p>\n\n<p>The project’s core is an API, which can be used to query the processed data. For this purpose, we develop a cloud service which queries the freely accessible data of Sentinel-5P, aggregates it, stores it in a cache and makes it available.\nTarget audience</p>\n\n<p>This project targets developers who want to build their own services based on the satellite data of the Copernicus program, but who do not want to work with huge amounts of scientific data directly. We will provide examples and libraries to quickly get you started without being an expert in satellite data analysis.</p>",
    "persons": [
      "Timo Nogueira Brockmeyer"
    ]
  },
  {
    "start": 1577875200000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "git-issue",
    "subtitle": "Git-based decentralized issue management with GitHub/GitLab integration",
    "track": "Lightning Talks",
    "abstract": "<p>Git-issue is a minimalist decentralized issue management system based on Git,\noffering (optional) biderectional integration with GitHub and GitLab issue management.\nIt has the following advantages over other systems.</p>\n\n<ul>\n<li><strong>No backend, no dependencies:</strong>\nYou can install and use <em>git issue</em> with a single shell script.\nThere's no need for a server or a database back-end, and the corresponding\nproblems and requirements for their administration.</li>\n<li><strong>Decentralized asynchronous management:</strong>\nAnyone can add, comment, and edit issues without requiring online access\nto a centralized server.\nThere's no need for online connectivity; you can pull and push issues\nwhen you're online.</li>\n<li><strong>Transparent text file format:</strong>\nIssues are stored as simple text files, which you can view, edit, share, and\nbackup with any tool you like.\nThere's no risk of losing access to your issues because a server has\nfailed.</li>\n<li><strong>Git-based:</strong>\nIssues are changed and shared through Git.\nThis provides <em>git issue</em> with a robust, efficient, portable,\nand widely available infrastructure.\nIt allows you to reuse your Git credentials and infrastructure, allows\nthe efficient merging of work, and also provides a solid audit trail\nregarding any changes.\nYou can even use Git and command-line tools directly to make sophisticated\nchanges to your issue database.</li>\n</ul>",
    "description": "",
    "persons": [
      "Diomidis Spinellis"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "The Heptapod project",
    "subtitle": "Bringing Mercurial to GitLab",
    "track": "Lightning Talks",
    "abstract": "<p>Heptapod is a friendly fork of GitLab CE that supports the Mercurial DVCS.\nToday, Bitbucket starts dropping the support for Mercurial. Heptapod can provide nice new homes for projects that have to migrate out of Bitbucket.\nWe are looking for contributors - lots of different skills can be useful.</p>",
    "description": "<p>Mercurial is a free software distributed version control system (DVCS) written primarily in Python, with an intuitive command line interface and strong, safe history rewriting features.</p>\n\n<p>Mercurial is in active development and in use at several large organisations, which\nappreciate especially its extensibility and its ability to handle very large repositories.</p>\n\n<p>However, Mercurial has been somewhat lacking public exposure in the past few years for not being a first class citizen in the prominent integrated hosting and collaboration solutions.\nThis culminated recently with Bitbucket announcing last summer its plan to drop support for Mercurial, in particular planning to stop accepting new repositories by February 1st, 2020 (that's the first day of this FOSDEM edition!).</p>\n\n<p>In this talk, we will present the Heptapod project, which brings Mercurial support to GitLab Community Edition, the well-known open-source integrated platform for source collaboration and dev-ops. Lately, GitLab CE has been selected by some major free software projects, such as Debian and Gnome, to name only a few.</p>\n\n<p>Several free and open-source projects have successfully migrated from Bitbucket to Heptapod. We are willing to help more of them doing so, either by hosting them directly if possible (contact us) or by giving them a hand in the transition.</p>\n\n<p>Heptapod is a community-driven effort, whose development involves many programming languages: Ruby, Go, Python, Javascript and potentially Rust, but one does not need to be a expert in all of these to start contributing.</p>\n\n<p>We are calling interested people to join us on our Heptapod instance (of course), there's a bit of low hanging fruit to grab there.</p>",
    "persons": [
      "Georges Racinet"
    ]
  },
  {
    "start": 1577877600000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "puavo.org",
    "subtitle": "Linux desktops in Finnish schools",
    "track": "Lightning Talks",
    "abstract": "<p>A Finnish company Opinsys has provided and maintained\nmany thousands of Linux desktops in Finnish schools for\nabout fifteen years now.  This is a short introduction\nto the technology (called Puavo) they have developed\nfor this purpose.</p>",
    "description": "<p>Puavo can be used to manage Linux desktops suitable\nfor school environments.  It is a combination of web\nsoftware (Puavo Web) and a specially configured system\n(Puavo OS) using Debian GNU/Linux operating system as\nits base.  Puavo Web is built for managing user accounts\nand devices.  Puavo OS is designed for large-scale\ndeployment in primary and secondary schools.  The source\ncode for both is free software under GPLv2+ license.</p>",
    "persons": [
      "Juha Erkkilä"
    ]
  },
  {
    "start": 1577878800000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Open Source for students, by students",
    "subtitle": "Teaching university students how to contribute to open source projects by providing a product to their peers",
    "track": "Lightning Talks",
    "abstract": "<p>Five years ago, Semester.ly was created for students, by students. As a one stop shop for students to find classes that fit their schedule, Semester.ly has course reviews, textbook prices, and the classes your Facebook friends are taking. As a student start-up, it quickly expanded to 8+ schools and accumulated 100,000 users. In 2016, Semester.ly went open source and many of its core members graduated. Over the past three years, university administrators and students have worked together to maintain Semester.ly’s success and grow it as an educational tool within universities. Semester.ly’s serves two purposes now: make course registration easier and teach students how to contribute to open source projects.</p>",
    "description": "<p>Every semester, students at universities across the country scramble to plan their course schedules for the upcoming academic term. Post-it notes, Excel spreadsheets, emails, and group chats all make up a stressful period in an already stressed out student’s life. The problem is simple: university backed course scheduling infrastructure is outdated. Universities have been around for hundreds of years and their internal software for decades, yet their students are a part of a technology revolution that expects things to be fast and social&lt;sup>1&lt;/sup>.</p>\n\n<p>A few months after tangling with course registration woes, students are engrossed in another stressful time period in which they attempt to acquire jobs. Employers look for experienced qualified candidates. Students go to school to become these candidates but most of the time, they just get a piece of paper that is losing value in the public mind&lt;sup>2&lt;/sup>. Universities try their best to prepare their students for the workforce through rigorous classwork and career resources but today’s workforce is ultra competitive and a college education isn’t enough&lt;sup>3&lt;/sup>.</p>\n\n<p>Here lie two seemingly unrelated problems for universities. University course registration systems are falling behind and universities aren’t fitting student’s needs in preparing them for post-graduation employment. Semester.ly solves both problems.</p>\n\n<p>Semester.ly was founded by an initial development team of Noah Presler, Felix Zhu, and Rohan Das during the summer of 2014. Back then, the goal of Semester.ly was to improve course registration. Before the founding team graduated, Semester.ly went open source in Spring 2016 with the hope that other students would carry on the torch. However, it was hard to find dedicated developers and funding without a business plan that involved making money later. These are problems that many open source organizations face but being based at a university also meant that team members would inevitably graduate every few years. There was a need for consistent members, incentives for developers, and funding for the server.</p>\n\n<p>Three organizational structures were created to solve these problems. The first was a student organization. This seemed like the obvious first choice since it's how the original team started but student developers didn’t have any incentive and would stop working once school work picked up. The second structure was a course to obtain credit for working on open source projects. After conversations with the Chair of the Computer Science Department at Johns Hopkins University, a course called Software for Resilient Communities was created focusing on four open source projects to benefit local and global communities&lt;sup>4&lt;/sup>. Since this was a course, students would take the first half of the semester to learn the technology stack, make an impact, and then never contribute again because they couldn’t repeat the class. This did succeed in getting lots of students exposed to open source projects but eventually the professor running the course left to run his own company and the course wasn’t offered again. The third and final organizational structure that Semester.ly maintains today is as a paid on-campus job. After all of these years, Semester.ly has become a strong presence on campus and school administrators have taken notice. To support our mission, Johns Hopkins Information Technology provides hosting, hourly pay for student developers, and consistent leadership for students. This organizational structure has been in place for about a year now and we’ve made great progress onboarding new student developers and creating new features.</p>\n\n<p>With this partnership, Semester.ly is more integrated with the university and several other partnerships have arisen. For example, Semester.ly now shows what tutoring services are available through the university and we’re developing a way to register for these tutoring services on Semester.ly next semester. Going forward, Semester.ly will continue to make course registration easier as well as provide whatever other services students see fit. Semester.ly proves that it's possible for open source projects to thrive on college campuses as a service and educational tool.</p>",
    "persons": [
      "Kristin Yim"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Open Adult Education: a curriculum to bridge the digital skills gap with free and open source technologies",
    "subtitle": "",
    "track": "Lightning Talks",
    "abstract": "<p>The OPEN-AE project is developing an open, and modular curriculum directed to train e-facilitators and trainers working with people who are in need of upskilling and reskilling in free and open-culture. The OPEN AE curriculum is meant to be modular and adaptable to the immediate training needs of the efacilitators. The training aims to introduce the trainers to the values of free and open-culture and empower them with the values behind this culture so it can be transmitted to low-skilled adults. the training will also have modules supporting a transition to free and open source software and culture.  Open-AE is not merely about teaching free and open softwares, but aims to have the trainers be active participants in the culture, knowing how to license these openly how to collaborate and develop as a collective.</p>",
    "description": "<p>The OPEN-AE project is developing an open, and modular curriculum directed to train e-facilitators and trainers working with people who are in need of upskilling and reskilling in free and open-culture. The OPEN AE curriculum is meant to be modular and adaptable to the immediate training needs of the efacilitators. The training aims to introduce the trainers to the values of free and open-culture and empower them with the values behind this culture so it can be transmitted to low-skilled adults. the training will also have modules supporting a transition to free and open source software and culture.  Open-AE is not merely about teaching free and open software but aims to have the trainers be active participants in the culture, knowing how to license these openly how to collaborate and develop as a collective.</p>\n\n<p>ALL DIGITAL is a network that wants to ensure every European, or rather every person can be empowered by the digital transformation, in short, we work in the field of digital inclusion. Our organisation started slightly over ten years ago as a grassroots movement among digital competency centres, and this scaled to the European level. Today 43% lack basic digital skills with half of them having skills at all. It is important to ensure</p>\n\n<p>At the moment ALL DIGITAL is coordinating the OPEN-AE project which aims to introduce free and open-source technologies to those who do not have enough digital skills or are in need upskilling. The project consortium involves partners working in the sector of digital inclusion running digital competency centres around Europe and one European network, ALL DIGITAL. The project aims to bridge the digital skills gap with free and open source technologies by developing a training curriculum directed to e-facilitators working in the non-formal sector. This curriculum will be open and modular allowing trainers to immediately adapt it to their training needs.</p>\n\n<p>Open-AE aims to not merely be an one-off project but wants to start a movement in the digital inclusion sector to ensure the digital skills gap is not bridge merely with proprietary software, but empower users so they can be empowered users with access to the relevant software and they know how to participate in the open culture after the training ends. Many in the digital inclusion sector are intimidated by free and open source software, believing they need a higher level than basic skills to use the software effectively. Breaking down the image of free and open source software and making it more approachable is essential to ensure lower skills users can not only access and use digital technologies but are empowered by them.</p>\n\n<p>The Open-AE consortium would like to present their work to FOSDEM to be engaged in a dialogue with developers and the open source community in activities directed to bridge the digital skills gap, to know what work developers are doing, but also engage in the open community in a direct capacity. Two partners based in Brussels ALL DIGITAL a European network, and Maks, Medie en Actie in Kureghem, will present the work. The presentation will go over the need to work in the field of digital inclusion.</p>\n\n<p>The presentation will go over the challenges in the digital inclusion with approaching free and open source technologies, the process of how the curriculum was developed, the curriculum itself, and how to scale this movement and carry it further.</p>",
    "persons": [
      "Pia Groenewolt"
    ]
  },
  {
    "start": 1577881200000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Prototyping the Internet of Things with Wyliodrin STUDIO",
    "subtitle": "An open source platform for building IoT prototypes",
    "track": "Lightning Talks",
    "abstract": "<p>In 2014, teaching a Raspberry Pi programming course was a real challenge, mostly due to the lack of development devices. This is how we came up with the idea of building Wyliodrin STUDIO.</p>\n\n<p>Wyliodrin STUDIO is an easy to use IDE for the Internet of Things that enables remote control over embedded devices. While it is a good prototyping tool, the platform also targets students and educators who want to get started in the IoT field. It is designed to help both technical and non-technical people to get started with programming devices such as the Raspberry Pi.</p>\n\n<p>In this talk we aim to present Wyliodrin STUDIO, how it works and how we and other universities used it to teach IoT technologies in classes such as computer science, power engineering and film directing.</p>",
    "description": "<p>Wyliodrin STUDIO is an open source, web-based IDE designed for fast prototyping of Internet of Things applications.</p>\n\n<p>We have build this platform because we needed an affordable way of programming embedded devices such as the Raspberry Pi. Since 2014, when the first platform version was released, we have improved the solution so now it can be used to remotely program, control and monitor devices. For the moment, the platform is compatible with devices such as the Raspberry Pi and BeagleBone Black and supports Python, Node.js and visual, block-based programming languages.</p>\n\n<p>Since 2014 the platform has been used by companies such as Intel and Cisco and in institutions such as UCLA, USC, Ulm University and Toronto Public Library.</p>\n\n<p>The purpose of our presentation is not only to make an overview of Wyliodrin STUDIO's characteristics, but also introduce the audience to our experience in teaching IoT courses and how the platform helped us.</p>",
    "persons": [
      "Alexandru Radovici"
    ]
  },
  {
    "start": 1577882400000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "DuckDB",
    "subtitle": "An Embeddable Analytical Database",
    "track": "Lightning Talks",
    "abstract": "<p>We present DuckDB, our new, Open Source embedded analytical data management system.</p>",
    "description": "<p>Data management systems have evolved into large monolithic database servers running as stand-alone processes. This is partly a result of the need to serve requests from many clients simultaneously and partly due to data integrity requirements. While powerful, stand-alone systems require considerable effort to set up properly and data access is constricted by their client protocols. There exists a completely separate use case for data management systems, those that are embedded into other processes where the database system is a linked library that runs completely within a ``host'' process. The most well-known representative of this group is SQLite, the most widely deployed SQL database engine with more than a trillion databases in active use. SQLite strongly focuses on transactional (OLTP) workloads, and contains a row-major execution engine operating on a B-Tree storage format. As a consequence, SQLite's performance on analytical (OLAP) workloads is very poor.</p>\n\n<p>There is a clear need for embeddable analytical data management. This needs stems from two main sources: Interactive data analysis and edge computing. Interactive data analysis is performed using tools such as R or Python. The basic data management operators available in these environments through extensions (dplyr, Pandas, etc.) closely resemble stacked relational operators, much like in SQL queries, but lack full-query optimization and transactional storage. Embedded analytical data management is also desirable for edge computing scenarios. For example, connected power meters currently forward data to a central location for analysis. This is problematic due to bandwidth limitations especially on radio interfaces, and also raises privacy concerns. An embeddable analytical database is very well-equipped to support this use case, with data analyzed on the edge node. The two use cases of interactive analysis and edge computing appear orthogonal. But surprisingly, the different use cases yield similar requirements.</p>\n\n<p>In this talk, we present our new system, DuckDB. DuckDB is a new purpose-built embeddable relational database management system created at the Database Architectures group of the CWI. DuckDB is available as Open-Source software under the permissive MIT license. To the best of our knowledge, there currently exists no purpose-built embeddable analytical database despite the clear need outlined above. DuckDB is no research prototype but built to be widely used, with millions of test queries run on each commit to ensure correct operation and completeness of the SQL interface.</p>\n\n<p>DuckDB is built from the ground up with analytical query processing in mind. As storage, DuckDB uses a single-file format with tables partitioned into columnar segments. Data is loaded into memory using a traditional buffer manager, however, the blocks that are loaded are significantly larger than that of a traditional OLTP system to allow for efficient random seeks of blocks. Queries are processed using a vectorized query processing engine to allow for high performance batch processing and SIMD optimizations.</p>",
    "persons": [
      "Hannes Mühleisen"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Apache DataSketches",
    "subtitle": "A Production Quality Sketching Library for the Analysis of Big Data",
    "track": "Lightning Talks",
    "abstract": "<p>In​ the analysis of b​ig data there are often problem queries that don’t scale because they require huge compute resources to generate exact results, or don’t parallelize well. Examples include c​ount-distinct, ​quantiles, most frequent items, joins, matrix computations, and graph analysis. Algorithms that can produce accuracy guaranteed approximate answers for these problem queries are a required toolkit for modern analysis systems that need to process massive amounts of data​ quickly. For interactive queries there may not be other viable alternatives, and in the case of real­-time streams, these specialized algorithms, called stochastic, s​treaming, sublinear algorithms,​ or 's​ketches',​ are the only known solution. This technology has helped Yahoo successfully reduce data processing times from days to hours or minutes on a number of its internal platforms and has enabled subsecond queries on real-time platforms that would have been infeasible without sketches. This talk provides a short introduction to sketching and to Apache DataSketches, an open source library of these algorithms designed for large production analysis systems.</p>",
    "description": "<p>Fast:\nSketches are fast. The sketch algorithms in this library process data in a single pass and are suitable for both real-time and batch. Sketches enable streaming computation of set expression cardinalities, quantiles, frequency estimation and more. This allows simplification of system's architecture and fast queries of heretofore difficult computational tasks.</p>\n\n<p>Big Data Platforms:\nThis library has been specifically designed for big data platforms. Included are adaptors for Hadoop Pig, Hive, Spark, Druid, and Postgresql, which also can be used as examples for other systems, and many other capabilities typically required in big data analysis systems. For example, a Memory package for managing large off-heap memory data structures.  Our sketch library is implemented in Java, C++ and Python and provides binary compatibility across languages and platforms.  Some of our sketches provide off-Java-heap capability which dramatically improves performance in large systems.   Our APIs provide a rich set of options to enable fine tuning performance parameters that are particularly important for large systems.</p>\n\n<p>Analysis:\nBuilt-in Theta Sketch set operators (Union, Intersection, Difference) produce sketches as a result (and not just a number) enabling full set expressions of cardinality, such as ((A ∪ B) ∩ (C ∪ D)) \\ (E ∪ F). This capability along with predictable and superior accuracy (compared with Include/Exclude approaches) enable unprecedented analysis capabilities for fast queries.</p>",
    "persons": [
      "Claude Warren"
    ]
  },
  {
    "start": 1577884800000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "DeOldify",
    "subtitle": "Colorizing images and videos with AI",
    "track": "Lightning Talks",
    "abstract": "<p>DeOldify is a Deep Learning project for colorizing and restoring old images and videos.</p>\n\n<p>In this presentation, you will learn what's behind DeOldify, how it achieves good results and if they are historically accurate.\nYou will also learn how to run the model yourself on your computer or Colab to colorize pictures/videos that you have.</p>",
    "description": "<p>DeOldify is a Deep Learning project for colorizing and restoring old images and videos.</p>\n\n<p>In this presentation, you will learn what's behind DeOldify, how it achieves good results and if they are historically accurate.\nYou will also learn how to run the model yourself on your computer or Colab to colorize pictures/videos that you have.</p>",
    "persons": [
      "Alexandre Vicenzi"
    ]
  },
  {
    "start": 1577886000000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "PICTOR: A free-to-use open source radio telescope",
    "subtitle": "",
    "track": "Lightning Talks",
    "abstract": "<p>PICTOR, located in Athens, Greece, consists of a 1.5-meter parabolic antenna that allows anyone to make continuous and spectral (i.e. hydrogen line) drift-scan observations of the radio sky in the 1300~1700 MHz regime for free. The goal of this effort is to introduce students, educators, astronomers and others to the majesty of the radio sky, promoting radio astronomy education, without the need of building a large and expensive radio telescope.</p>\n\n<p>PICTOR is a fully open source (software &amp; hardware) project: https://github.com/0xCoto/PICTOR</p>",
    "description": "<p>PICTOR is a free-to-use open source and open hardware radio telescope that aims to promote radio astronomy on a budget. It consists of a 1.5 meter parabolic dish antenna, a 1420 MHz-optimized feedhorn, a two stage low noise amplifier (LNA) with a built-in high-pass filter, and an RTL-SDR. Future upgrades may also use higher-bandwidth SDRs, such as the LimeSDR Mini.</p>\n\n<p>This radio telescope allows users to measure hydrogen line emissions from our galaxy. Under certain conditions, hydrogen atoms can emit photons with a wavelength of 21 cm, which corresponds to a frequency of 1420.405 MHz. Because our galaxy is so rich in terms of neutral hydrogen, a radio telescope like PICTOR is capable of detecting such faint radio emissions. When the telescope is pointing to the galactic plane (the Milky Way band), the intensity around 1420 MHz gets significantly stronger. Radio astronomers are able to use information like the Doppler shift such emissions have undergone, in order to determine neutral hydrogen concentration, map the spiral geometry of our galaxy, and even provide evidence for the existence of dark matter by plotting the rotation curve of the Milky Way!</p>\n\n<p>In order to observe with PICTOR, a user can just visit www.pictortelescope.com, click \"Observe\", fill in their observation parameters (frequency, observing duration etc.) and submit their observation, and as soon as the observation is finished, the user will receive an email with their observation data and the parameters they entered.</p>\n\n<p>Since the initial launch, PICTOR has gotten lots of updates and improvements, particularly in the software backend, providing more data to the users, using advanced techniques to increase the signal-to-noise ratio by calibrating spectra and mitigating radio frequency interference (RFI) (if present), and more.</p>\n\n<p>There is also a PDF for users who are unfamiliar with radio astronomy and radio telescopes to get started: https://www.pictortelescope.com/Observing<em>the</em>radio<em>sky</em>with_PICTOR.pdf</p>\n\n<p>PICTOR is a fully open source (software &amp; hardware) project, and everything can be found on the GitHub repository: https://www.github.com/0xCoto/PICTOR</p>",
    "persons": [
      "Apostolos Spanakis-Misirlis"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Advancing science with Dataverse",
    "subtitle": "Publication, discovery, citation, and exploration of research data.",
    "track": "Lightning Talks",
    "abstract": "<p>Dataverse is open source research data repository software installed by 48 institutions around the world and translated into ten languages. It facilitates data sharing, allowing researchers to replicate and build upon each other's work and receive academic credit in the form of citations for publishing data. Data deposited into Dataverse installations is made more discoverable through harvesting of metadata via standard protocols, publication to registries such as DataCite, and indexing into scholarly search engines such as Google Dataset Search. Data exploration is enabled by a variety tools contributed by the international Dataverse community that make use of Dataverse APIs to get data in and out. These APIs also enable a variety of integrations with scholarly publishing systems such as electronic lab notebooks, journal systems, reproducibility platforms, and more.</p>",
    "description": "<p>In this talk a core developer for Dataverse will introduce the audience to the world of scholarly publishing, making the case for data publication and how it contributes to the advancement of science. An emphasis will be made on how Dataverse goes beyond simply being open source by being friendly to contributions from newcomers.</p>",
    "persons": [
      "Philip Durbin"
    ]
  },
  {
    "start": 1577888400000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Towards decentralized alternatives for code collaboration",
    "subtitle": "Building Radicle, a peer-to-peer network for code collaboration",
    "track": "Lightning Talks",
    "abstract": "<p>This talk will walk the audience through Radicle, a peer-to-peer network for code collaboration. It will touch on the design approach (outlined in the description) of the Radicle stack and outline the project's next steps and challenges for the coming year as we launch our network. It's narrative will also reflect the potential of distributed technology for free and open source sustainability and resilience. Radicle (http://radicle.xyz/) is a free and open source project supported by Monadic (https://monadic.xyz/).</p>",
    "description": "<p>Radicle is a peer-to-peer network for code collaboration. It's a decentralized collaboration environment that’s designed to be:</p>\n\n<ul>\n<li><strong>Offline first</strong> : all data, including issues, comments and other social artifacts is yours &amp; lives on your machine.</li>\n<li><strong>Peer-to-peer</strong> : with no central server or intermediary in control.</li>\n<li><strong>Cryptographically secure</strong> : user data that is tamper-proof &amp; unforgeable, using public key cryptography.</li>\n<li><strong>Programmable</strong> : developers can program the way in which they collaborate.</li>\n<li><strong>Interoperable &amp; open</strong> : reflecting the values of the open source community.</li>\n</ul>\n\n\n<p>Radicle integrates with distributed version control systems like git and includes a high-level language with reprogrammable semantics, P2P networking for sharing application state, and flexible command line tools. Inspired by P2P protocols like <a href=\"https://www.scuttlebutt.nz/\">Secure Scuttlebutt</a>, radicle connects distributed version control with peer-to-peer networking to make collaboration a primitive – starting with git and building up an entirely peer-to-peer developer experience that encourages experimentation around how we write software together. In other words, radicle lets developers program the process of writing code, shaping their workflow around a specific project or context.</p>\n\n<p>Like many P2P systems, radicle uses an offline-first model. Issues, comments, and other social artifacts are stored locally as a log of events and synced automatically with your collaborators. While git repositories are already portable, social artifacts are not - radicle aims to change this. Radicle also allows you to define entirely new message types: projects, feature requests, releases, etc., each with their own event streams, metadata, and access control policies. All of this information is completely portable and self-amending in situ.</p>\n\n<p>Making any kind of semantic modification to a decentralized data structure is typically difficult to coordinate and prone to forks, but radicle’s programmable evaluator makes this process straightforward, safe, and immediate. Additionally, built-in aggregate signatures allow for the definition of custom security models to read or modify each chain, even to revise an access control policy.</p>\n\n<p>Finally, radicle comes with sensible defaults, ensuring you can be productive right away, while also giving programmers a flexible system for rolling their own software collaboration workflow. Inspired in many ways by <a href=\"https://www.gnu.org/software/emacs/\">Emacs</a>, radicle is designed as an extensible system, where developers can share their programs with one another and extend their revision control environment however they like.</p>",
    "persons": [
      "Abbey Titcomb"
    ]
  },
  {
    "start": 1577889600000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Getting started with FPGA's for Packet Processing",
    "subtitle": "Intel FPGA opportunities",
    "track": "Lightning Talks",
    "abstract": "<p>The FPGA is an emerging technology that can increase the performance of packet processing due to need of increased protocol complexity.\nThere are many models of system where FPGA is suitable more or less depending on use-case.</p>\n\n<p>The presentation will introduce the system models presenting how to use FPGA inside larger systems.\nThe models include inline, lookaside and fast-path packet processing capabilities requiring different approaches from system level making accelerated system more usable and easier to integrate with existing components.</p>\n\n<p>In this talk we cover the challenges related to integration of the FPGA system with SW components like DPDK/kernel drivers and orchestration.\nThe examples of real FPGA deployments will be presented on base of Intel Programmable Acceleration Card family.</p>",
    "description": "<p>The talk describes a  new usage for FPGA technology used for packet processing. It presents opportunities, problems going to be solve and challenges related to use a complex programmable systems for mass deployment.</p>\n\n<p>It covers typical use-cases and some basic rules defining how to use FPGA system efficienly and integrate it with existing software stack and existing orchestration systems like Openstack or kubernetes.</p>",
    "persons": [
      "Miroslaw Walukiewicz"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Quality diagrams with PyCirkuit",
    "subtitle": "",
    "track": "Lightning Talks",
    "abstract": "<p>I'd like to present PyCirkuit, a little python application acting as a front-end to circuit-macros and dpic language, allowing the creation of high quality graphics and circuit diagrams to be included into LaTeX and other documents.</p>",
    "description": "",
    "persons": [
      "Orestes Mas"
    ]
  },
  {
    "start": 1577892000000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "License compliance for embedded Linux devices with Buildroot",
    "subtitle": "What your buildsystem can do to relieve your pain in fulfilling legal obligations",
    "track": "Lightning Talks",
    "abstract": "<p>Producing a Linux-based electronic device requires to put together lots of open source software packages, which is a complex task.</p>\n\n<p>Complying to the licensing obligations for each of them is also complex, especially if you are not a lawyer. Not complying is immoral, illegal and risky.</p>\n\n<p>Discover how your build system can help you!</p>",
    "description": "<p>With live demos, Luca will introduce you to:</p>\n\n<ul>\n<li>how Buildroot builds all the needed software components in a simple way;</li>\n<li>which are the obligations for the most common licenses;</li>\n<li>what Buildroot can (and what it cannot) do to help you in being compliant.</li>\n</ul>",
    "persons": [
      "Luca Ceresoli"
    ]
  },
  {
    "start": 1577893200000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "Open Source Support Program by OTA",
    "subtitle": "",
    "track": "Lightning Talks",
    "abstract": "<p>The Open Technology Assembly (OTA), formerly known as Belgian Unix Users Group, is a non-profit organisation  which main goal was to promote and organise meetings around Unix and Open Source in general (mainly more than a decade ago).\nOver the years the Internet took over so that the interest in our meetings declined and finally we stopped organise meetings altogether. However, as a non-profit organisation we would like to do something useful such as supporting and funding new open source related projects.</p>",
    "description": "<p>The Open Technology Assembly (OTA) will open an Open Source Support program to apply for funding. It would be nice that the chosen Open Source projects owners submit a (lightning) talk for next year's FOSDEM, however, it is not obliged.\nWe will also organise an independent jury to go over the submitted projects and make a list of the projects which will get funding. Before FOSDEM starts we will have web pages explaining most of the details.</p>",
    "persons": [
      "Gratien D'haese"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "NGI Zero: A treasure trove of tech awesome",
    "subtitle": "Sampling through the Next Generation Internet initiative",
    "track": "Lightning Talks",
    "abstract": "<p>The <a href=\"https://ngi.eu\">Next Generation Internet</a> initiative is the first concerted effort in Europe to put significant public funding to hands-on work to really fix the internet. The long term <a href=\"https://ngi.eu\">vision</a> of the initiative is to make the internet what we need and expected it to be in the first place: <strong>Resilient</strong>. <strong>Trustworthy</strong>. <strong>Sustainable</strong>. The concrete mission of the Next Generation Internet initiative is to \"re-imagine and re-engineer the Internet for the third millennium and beyond\". With new projects starting all the time, the density of awesome open source, open hardware, new science and new standards in-the-making is already intense: about 200 projects are currently on their way. These range from encrypted synchronisation for calendars and address books to symbolical protocol verification, from an open hardware RISC-V SoC to removing binary seeds from operating systems, from ethical search to the Fediverse etc.</p>\n\n<p>NGI Zero offers funding to independent researchers and FOSS developers working on free and open projects in the area of privacy and trust enhancing technologies and on search, discovery and discoverability. It also offers an elaborate 'pipeline' of supporting activities that live up to high standards (sometimes called 'walk the talk') in terms of security, privacy, accessibility, open source licensing, standardisation, packaging, etc. The talk will provide an overview of the awesome R&amp;D that is now in the pipeline, how these projects are supported, and everything you need to know about the various opportunities to 'come and work for the internet'.</p>",
    "description": "<p><a href=\"https://nlnet.nl/discovery\">NGI Zero Discovery</a> and <a href=\"https://nlnet.nl/PET\">NGI Zero PET</a> are a significant effort and ambitious effort by a large group of organisations led by <a href=\"https://nlnet.nl/foundation\">NLnet foundation</a> (that was instrumental in <a href=\"https://nlnet.nl/foundation/history\">pioneering the early internet in Europe</a>):</p>\n\n<ul>\n<li><a href=\"https://accessibility.nl/english\">Accessibility Foundation</a> - Center of expertise on accessibility of internet and other digital media for all people, including the elderly and people with disabilities</li>\n<li><a href=\"https://apc.org\">Association for Progressive Communications</a> - A global network and organisation that strives towards easy and affordable access to a free and open internet to improve the lives of people and create a more just world</li>\n<li><a href=\"https://techcultivation.org/\">Center for the Cultivation of Technology</a> - A charitable non-profit host organization for international Free Software projects</li>\n<li><a href=\"https://commonscaretakers.com\">Commons Caretakers</a> - A not-for-profit service provider for the development of Commons</li>\n<li><a href=\"https://www.netsec.ethz.ch/\">Network Security Group</a> of <a href=\"https://www.netsec.ethz.ch/\">Eidgenössische Technische Hochschule Zürich</a> - Academic research institute focused on building secure and robust network systems</li>\n<li><a href=\"https://fsfe.org\">Free Software Foundation Europe</a> - Association charity that aims to empower users to control technology.</li>\n<li><a href=\"http://www.ifross.org/\">ifrOSS</a> - Provides not-for-profit legal services and studies in the context of free and open source software</li>\n<li><a href=\"https://nixos.org/nixos/foundation.html\">NixOS Foundation</a> - Foundation supporting development and use of purely functional configuration management tools, in particular NixOS and related projects</li>\n<li><a href=\"https://nlnet.nl\">NLnet Foundation (NL)</a> - Grantmaking public benefit organisation founded by pioneers of the early European internet</li>\n<li><a href=\"https://ps.zoethical.com/\">Petites Singularités</a> - Non profit organisation working with free sofware and focusing on collective practices</li>\n<li><a href=\"https://radicallyopensecurity.com\">Radically Open Security</a> - Not-for-profit open source security company</li>\n<li><a href=\"http://securesoftware.nl\">TIMIT</a> - Experts in secure software</li>\n<li><a href=\"http://translatehouse.org/\">Translate House</a> - Develops and implements open source localization solutions</li>\n</ul>\n\n\n<p>The budget for the effort is kindly provided by the European Commission.</p>",
    "persons": [
      "Michiel Leenaars"
    ]
  },
  {
    "start": 1577895600000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "European Software Engineering funded research",
    "subtitle": "What has happened under Horizon 2020 and what we already now about the future.",
    "track": "Lightning Talks",
    "abstract": "<p>This lightning talk will explain the latest funding results of the European Framework Programme for Research (Horizon 2020) concerning software engineering.</p>",
    "description": "",
    "persons": [
      "Luis C. Busquets Pérez"
    ]
  },
  {
    "start": 1577896800000,
    "duration": 15,
    "room": "H.2215 (Ferrer)",
    "title": "FOSDEM infrastructure review",
    "subtitle": "",
    "track": "Lightning Talks",
    "abstract": "<p>Informational and fun.</p>",
    "description": "",
    "persons": [
      "Richard Hartmann"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 10,
    "room": "H.1301 (Cornil)",
    "title": "Kotlin DevRoom Welcoming Remarks",
    "subtitle": "",
    "track": "Kotlin",
    "abstract": "<p>Welcoming participants to the first edition of the Kotlin DevRoom @ FOSDEM</p>",
    "description": "",
    "persons": [
      "Nicola Corti"
    ]
  },
  {
    "start": 1577870100000,
    "duration": 40,
    "room": "H.1301 (Cornil)",
    "title": "Useful coroutine patterns for Android applications",
    "subtitle": "",
    "track": "Kotlin",
    "abstract": "<p>Kotlin Coroutines are a great match for implementing common features in Android applications. In this presentation, we will go through a number of patterns that solves the pattern we often encounter when implementing traditional asynchronous features.</p>",
    "description": "<p>Coroutines can help us build robust solution for asynchronous work in Android applications, but it can also be difficult to learn what pattern should be used in different scenarios. When should you use launch instead of async, or when should you use a Channel instead of Flow, or maybe both? What use is a conflated channel and why is the catch() operator important? All this and some more will be covered in this talk.</p>",
    "persons": [
      "Erik Hellman"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 25,
    "room": "H.1301 (Cornil)",
    "title": "Migrating FOSDEM Companion to Kotlin",
    "subtitle": "",
    "track": "Kotlin",
    "abstract": "<p>FOSDEM Companion is currently the most used mobile application at FOSDEM. It has been around since 2014 and is updated every year. In 2020, it's finally made the big leap to Kotlin!</p>",
    "description": "<p>The app has been entirely rewritten using the Kotlin programming language. This talk will cover the conversion process, and how the new code makes use of language features and APIs that are not available in Java to become more than a simple Java conversion.</p>\n\n<p>For example, we'll talk about:</p>\n\n<ul>\n<li>How Data classes and Parcelize remove a lot of boilerplate code in the model classes</li>\n<li>How immutability and null safety have been enforced in the code with minimal effort</li>\n<li>How KTX makes Android framework and Jetpack APIs more Kotlin-friendly</li>\n<li>Coroutines integration in the app.</li>\n</ul>\n\n\n<p>The talk will be illustrated by many code examples.</p>\n\n<p><em>Intended audiences:</em> Android developers with medium experience looking to improve their Kotlin codebases. Java developers curious about Kotlin.</p>",
    "persons": [
      "Christophe Beyls"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 25,
    "room": "H.1301 (Cornil)",
    "title": "Idiomatic Kotlin Microservices",
    "subtitle": "A live coding session on how to go pure Kotlin with microservices",
    "track": "Kotlin",
    "abstract": "<p>Although Kotlin is, from a language perspective, 100% interoperable with Java, due to the slight paradigm shift (nullability) there might be some pain when using Java frameworks, e.g.: the need of <code>private var lateinit</code> when using JUnit, having to use compiler plugins to open up Spring annotated beans or a generated zero-arg constructor for Hibernate.\nThis talk goes through a full fledged alternative to the common Java stack when writing microservices, using: Ktor (Web framework), Kodein (DI container), Exposed (SQL library), Spek (test framework), Gradle Kotlin DSL, ...</p>",
    "description": "<p>This live coding session aims to share my experiences on how to combine (and how not to combine!) several open source libraries into a maintainable, scalable, sustainable, and all the other xxx-ables you can think of... based on several years of experience using Kotlin in production in different companies. Using libraries written from scratch for Kotlin enables us to fully make use of this beautiful language. Its functional nature, extension function types, null type safety, delegated properties; its preference for immutability and simply the feeling of writing proper, idiomatic Kotlin. The demo itself will cover a basic CRUD web service, following a TDD approach resulting in a production ready artifact.</p>\n\n<p>The target audience are primarily developers/architects and requires basic knowledge of the language (intermediate level) and preferably some experience with the existing Java ecosystem.</p>",
    "persons": [
      "Christoph Pickl"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 25,
    "room": "H.1301 (Cornil)",
    "title": "Automate your workflows with Kotlin",
    "subtitle": "Forget everything about bash and perl!",
    "track": "Kotlin",
    "abstract": "<p>From git housekeeping to releasing. From keeping a sane backlog to\nhandling internationalization and monitoring status, the life of a\ndeveloper involves more than meets the eye.</p>\n\n<p>This talk will be a feedback about how we use Kotlin extensively to:</p>\n\n<ul>\n<li><p>automate the dev workflows with command-line apps on dev\nmachines.</p></li>\n<li><p>achieve CI independence with kotlin runners on CI machines.</p></li>\n<li><p>run Kotlin on the server using the gradle appengine plugin.</p></li>\n<li><p>ditch bash and perl and use kscript instead.</p></li>\n</ul>\n\n\n<p>That's a lot to cover and the goal of this talk is not to dig into\neach technology but more to inspire and show the range of possibles\nopened by Kotlin.</p>",
    "description": "<p>From git housekeeping to releasing. From keeping a sane backlog to\nhandling internationalization and monitoring status, the life of a\ndeveloper involves more than meets the eye.</p>\n\n<p>This talk will be a feedback about how we use Kotlin extensively to:</p>\n\n<ul>\n<li><p>automate the dev workflows with command-line apps on dev\nmachines.</p></li>\n<li><p>achieve CI independence with kotlin runners on CI machines.</p></li>\n<li><p>run Kotlin on the server using the gradle appengine plugin.</p></li>\n<li><p>ditch bash and perl and use kscript instead.</p></li>\n</ul>\n\n\n<p>That's a lot to cover and the goal of this talk is not to dig into\neach technology but more to inspire and show the range of possibles\nopened by Kotlin.</p>",
    "persons": [
      "Martin Bonnin",
      "Michel Gauzins"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 25,
    "room": "H.1301 (Cornil)",
    "title": "Experimenting with the Kotlin Compiler",
    "subtitle": "",
    "track": "Kotlin",
    "abstract": "<p>A lot of us will agree that Kotlin is an awesome language - but have you ever thought about experimenting with the Kotlin Compiler? This talk explores how to set up your development environment, add, remove or change language features, get feedback from the community and get your suggestions merged.</p>\n\n<p>You will get to know how to set up IntelliJ for working on different parts of the compiler in the Kotlin repo and learn how the compiler is structured and the different compilation phases. After looking at the structure, you will get know how to modify existing language features such and how to add a language keyword to Kotlin. In the end, we will look at different ways of bringing your changes to the Kotlin programming language!</p>\n\n<p>You will leave this talk with a solid grasp of Kotlin Compiler mechanisms and how to change things up a bit. You don't need to know anything about compilers or Computer Science theory!</p>",
    "description": "",
    "persons": [
      "Jossi Wolf"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 25,
    "room": "H.1301 (Cornil)",
    "title": "Communication Break Down | Coroutines",
    "subtitle": "",
    "track": "Kotlin",
    "abstract": "<p>Coroutines are great, I think we all agree on that. But as the async, and possibly parallel, programming becomes easier the risk of sharing mutable variables between coroutines arises. When the boundaries are abstracted away we should rely on safe ways to communicate between our coroutines.\nIn this session I will go through safe and unsafe ways of communication between different coroutines, with the main focus on the safe ones and what differentiates them.</p>",
    "description": "",
    "persons": [
      "Bob Dahlberg"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 25,
    "room": "H.1301 (Cornil)",
    "title": "How Kotlin can change developer experience with modern graphics APIs",
    "subtitle": "",
    "track": "Kotlin",
    "abstract": "<p>Creating modern games in Kotlin forces you to use non-object oriented graphic languages and getting your hand dirty with native resource managemente. With wrappers around OpenGL and Vulkan, we make powerful graphics familiar to jvm devs and type-safe.</p>",
    "description": "<p>Vulkan and OpenGL are the cross-platform standard for creating modern games and graphics. However, their respectively stateless and state-based systems have no relation to an object-oriented language like Kotlin and you have to be careful to handle native resources interacting with the API.</p>\n\n<p>Wrappers that we have built at Kotlin Graphics allow for them to become object oriented. This talk will teach the attendees how to use our wrappers VKK and GLN for Vulkan and OpenGL respectively in order to create performant, modern graphics.</p>\n\n<p>Graphic power meets Kotlin expressiveness</p>\n\n<p>VKK and GLN bring features such as type safety through inline classes and enums, DSL constructs, extension functions, typealias, less verbosity, pure jvm allocation strategy and an easy to pick up system. Because of the type-safety guarantee, we can directly call the native methods without performing potentially expensive checks.</p>\n\n<p>This allows users of Kotlin to create performant, modern games like those you could build on C or C++.</p>",
    "persons": [
      "Giuseppe Barbieri"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 25,
    "room": "H.1301 (Cornil)",
    "title": "Improve your Android app with coroutines",
    "subtitle": "Application level integration of kotlinx coroutines",
    "track": "Kotlin",
    "abstract": "<p>Koltinx Coroutines library implementation for higl level programmation.</p>\n\n<p>Presentation of application development with Jetbrains kotlinx library.</p>",
    "description": "<p>Demonstration of kotlinx Coroutines framework use for application development.</p>\n\n<p>Coroutines helped a lot to improve VLC app performances on Android.\nThis talk is a feedback on this conversion and a collection of good practices.</p>\n\n<p>We will see how it helped make the app more performant and maintainable.\nEspecially the benefit of structured concurrency and how it greatly helps solving most asynchronism problems.\nUse your components lifecycle to scope asynchronous jobs.\nGet rid of callback APIs with Flow API.\nAnd some more examples of how to dismiss the GUI frameworks limitations with coroutines</p>",
    "persons": [
      "Geoffrey Métais"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 25,
    "room": "H.1301 (Cornil)",
    "title": "Confessions of a Serial K–otlin Multiplatform–er",
    "subtitle": "__just don’t 𝚎𝚡𝚙𝚎𝚌𝚝 too much__",
    "track": "Kotlin",
    "abstract": "<h3>What is <em>Multiplatform</em>?</h3>\n\n<ol>\n<li><p> <strong>HELL</strong></p>\n\n<ul>\n<li><p>harder to develop, no documentation</p></li>\n<li><p>just a trend, failed in the past</p></li>\n<li><p>not suitable for performant apps that feel “native”<br/>\n </p></li>\n</ul>\n</li>\n<li><p> <strong>HEAVEN</strong></p>\n\n<ul>\n<li><p>simpler to develop, removes platform barriers</p></li>\n<li><p>write half the code, ship in half the time</p></li>\n<li><p>only need to hire “generalists” instead of “specialists”<br/>\n </p></li>\n</ul>\n</li>\n<li><p> <strong>OTHER</strong>  ✔︎</p>\n\n<ul>\n<li>elaborate in 25 minutes or less:<br/>\n<em>________________________________</em><br/>\n <br/>\n </li>\n</ul>\n</li>\n</ol>",
    "description": "",
    "persons": [
      "Eugenio Marletti"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 25,
    "room": "H.1301 (Cornil)",
    "title": "Kotlin MP: Into the Multi-Verse",
    "subtitle": "",
    "track": "Kotlin",
    "abstract": "<p>Kotlin Multiplatform is the new kid on the cross-platform block. The approach although is very different from what you have seen in the past. The new approach utilizes Kotlin Native to compile Kotlin language to native binaries for specific target platform which can run without a virtual machine. Thus enabling simplified code sharing across multiple platforms.\nIn this talk, you will be introduced to Kotlin/Native and demonstrate how to build a Kotlin Multiplatform app that runs on both iOS and Android using shared Kotlin code.</p>",
    "description": "",
    "persons": [
      "Nishant Srivastava"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 25,
    "room": "H.1301 (Cornil)",
    "title": "Multiplatform Kotlin Library Development",
    "subtitle": "",
    "track": "Kotlin",
    "abstract": "<p>Multiplatform Kotlin facilitates code-sharing by making platform-agnostic portions of the standard library available in common code that is written once but can run on any target. As Multiplatform development really starts to take off over the next year, there must also be a robust ecosystem of third party libraries available to application developers.</p>\n\n<p>I’ll talk through what it looks like to create such a library, with lessons from my experiences building one of the early libraries in the mobile Multiplatform space. We'll talk about how to find shared abstractions around different platform APIs, how to handle the fast-paced evolution of this environment, and what this all felt like as a first-time library developer. When we're done, you’ll be ready to leverage the growing ecosystem as well as make your own contributions.</p>",
    "description": "",
    "persons": [
      "Russell Wolf"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 35,
    "room": "H.1301 (Cornil)",
    "title": "Bridge the physical world: Kotlin/Native on Raspberry Pi",
    "subtitle": "",
    "track": "Kotlin",
    "abstract": "<p>With Kotlin/Native, we can now compile Kotlin code to run on various platforms, including Raspberry Pi. This cross-platform ability has drawn the attention of many developers. Through the process of building a hand game robot which can play rock paper scissors with human beings, this talk aims to show you the possibility of using Kotlin to control GPIO pins on a Raspberry Pi and other experiments such as performing machine learning operations in Kotlin using TensorFlow backend, thanks to the interoperability with C libraries.</p>",
    "description": "",
    "persons": [
      "Qian Jin"
    ]
  },
  {
    "start": 1577893200000,
    "duration": 35,
    "room": "H.1301 (Cornil)",
    "title": "Dissecting the inline keyword in Kotlin",
    "subtitle": "A deep-dive into the internal working of the inline keyword",
    "track": "Kotlin",
    "abstract": "<p>Kotlin has a keyword called <code>inline</code>. While being mostly auto-suggested by the IDE, this little optimization forms the backbone for features like coroutines and APIs for sequences and collections and a lot more!</p>",
    "description": "<p>Java 8 introduced the concept of lambdas, while this was done at the language level, using bytecode instructions introduced in Java 7. Kotlin, tries to make this lambda functionality available for application targeting even Java 6! How does it do it? What optimizations does it do to make this compatibility happen?</p>\n\n<p>How does this feature form the backbone for features like coroutines and APIs for collections and sequences.</p>\n\n<p>How it doesn't stop there and introduces the concept of Inline classes, which enable Kotlin to have unsigned integers.</p>\n\n<p>How all of this is done, by just adding one keyword support in Kotlin!</p>",
    "persons": [
      "Suraj Shah"
    ]
  },
  {
    "start": 1577895600000,
    "duration": 40,
    "room": "H.1301 (Cornil)",
    "title": "Designing a DSL with Kotlin",
    "subtitle": "",
    "track": "Kotlin",
    "abstract": "<p>Kotlin is one of those “new” JVM languages that are currently rocking the boat. Although it’s made a great impact on Android, it’s equally good on the server side. As Domain-Specific Languages are constrained by the language they run on, Kotlin frees developers from Java fluent builders to propose something better.</p>\n\n<p>Using the Vaadin web framework as an example, I’ll demo how one could design its own DSL with Kotlin.</p>",
    "description": "",
    "persons": [
      "Nicolas Frankel"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "WebMIDI",
    "subtitle": "The garlic bread of the music industry",
    "track": "JavaScript",
    "abstract": "<p>The connection between Web and MIDI seems as likely as the joining of Garlic with Bread! Yet, we now have the power to create music from the web browser! Either by generating MIDI files for later manipulation, or as live instruments, WebMIDI provides us with the power to build some amazing online music applications.</p>",
    "description": "<p>In this short form talk, Steven will cover the internal details of the MIDI protocol, and its related file format, to show how music is represented by computers. This details both the recording and playback of music within the browser, and external application. It comes complete with live demos, synthesized sounds, and occasional music jokes!</p>",
    "persons": [
      "Steven Goodwin"
    ]
  },
  {
    "start": 1577871000000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Building Decentralized Social Virtual Reality using WebXR on your browser",
    "subtitle": "",
    "track": "JavaScript",
    "abstract": "<p>Learn how to build auditable privacy aware social vre experinces right inside your webpage in javascript using WebXR API. That will be corss platform, instant and run in every device with a browser. Adapting to the MIxed Reality capability of your user.\nThis session will give you a short primer on WebXR API's and hands-on on building a small social VR experience using open source tools and javascript in your browser.</p>",
    "description": "<p>Virtual Reality (VR) is undoubtedly one of the most sought after technology of the present times. Everybody is trying to make their presence felt in the sphere. And one of the prime use cases of VR is shared spaces.\nIt opens up possibility of having shared earning space, meeting space and even training space in real-time and in an interactive way.\nHowever often these kind of experiences are mired with privacy concerns, who is hosting the space, the data retained and who is observing the behaviors. On top the data retention implications.</p>\n\n<p>What if we could build Virtual Reality experiences right on the browser using JavaScript and all the shared room logic too in an open auditable way. And what if we could host it in decentralized way. Where it would be resistant to takedowns and have ability for users to host their own rooms.</p>\n\n<p>Interested in social VR, know about the possibilities it open, want to get your hands wet but don't want to invest learning another tool? Don't want to commit to the steep learning curve or buy expensive machines up front? Or are you afraid of the walled garden of SDK's? In this talk we rip open the veiled wall of proprietary VR with Web Virtual Reality designed to run in browser in any device including your phone. Learn how to build auditable privacy aware social VR experiences right inside your webpage in JavaScript using WebXR API. Adapting to the Mixed Reality capability of your user.</p>",
    "persons": [
      "Rabimba Karanjai"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "PWAs on steroids",
    "subtitle": "",
    "track": "JavaScript",
    "abstract": "<p>PWAs bring the best of both mobile and native apps to user. PWAs equipped with service workers provide features like offline availability, push notifications etc. Now with modern webAPIs, PWAs are beyond the browsers; in Hardware. Consider turning bulb on/off with your PWA, sounds cool? Lets learn how to, in this talk!</p>",
    "description": "<p>PWAs have been limited to offline availability and push notifications for long time, I want to explain to developers there is much more to PWAs, I want to show how to interact with more device specific features with modern webAPIs like web Bluetooth, speech synthesis, speech recognition, webShare etc. With this talk I want to give attendees a compelling reason to develop PWA instead of a mobile app for there next thing. I plan to have a quick introduction to serviceworkers and then majorly explaining webAPIs, usage and implementation with demos.</p>",
    "persons": [
      "Trishul Goel"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Machine Learning on the Web",
    "subtitle": "",
    "track": "JavaScript",
    "abstract": "<p>Have you ever wondered how you can make machine learning applications on the web, without learning Python or R? Well, brace yourself to explore the power of JavaScript and get to know how you can use it to build a machine learning web app.</p>",
    "description": "<p>Machine Learning is a new trending topic that everyone wants to be a part of. But it is a hard field to master. What if I tell you that you don’t need to know another language, only JavaScript is enough?\nIn my talk Machine Learning for Web Developers, I will present the views of a web developer. How a WebDev can get started, without learning any new languages.\nThe talk will cover a basic understanding of the following topics:\n- Classification\n- Regression\n- Error Functions\n- Optimizers\nFurther on, we'll not only discuss it's uses for you, but also we'll deploy them on an application with working code snippets and understand how you can deploy it all! We'll make use of Open Source ML Libraries like TensorFlow.js and Brain.js!\nIn the end the attendees will also get to know the advantages and drawbacks of running Machine learning applications on the browser. Finally we will then look for the use-cases where it is advisable to run machine learning models on browsers.</p>",
    "persons": [
      "Harshil Agrawal"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "XR adds: “Try before you buy”",
    "subtitle": "",
    "track": "JavaScript",
    "abstract": "<p>One day people will wake up and realize that Augmented Reality is here.\nAdvertisers are keen on AR because it can create higher levels of engagement than traditional ads. The power of gamification and the power of people experimenting with AR technology helps them to make better decisions as consumers.\nYou can find an ad in your news feed, open the camera, and preview a product in the \"Real\" world.</p>",
    "description": "<p>One day people will wake up and realize that AR is here. There are many questions around this sometimes-overhyped sector. And the biggest of them is how it will make money?\nThere are several answers to that, and we will take a look at the most popular ones.</p>\n\n<p>Advertisers are keeping the eye on AR because it can create higher levels of engagement than traditional advertisements.</p>\n\n<p>People are very interested in things like shopping and ads, things that help to make a better decision.\nOn the other side, brands want their customers to spend more time thinking about their products and personalizing them to their needs.</p>\n\n<p>And XR (Extended Reality) can enable that!</p>",
    "persons": [
      "Anastasiia Miroshnichenko"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "ReactJs, Redux & Apollo - state management in SUSI.AI",
    "subtitle": "The event mainly talks about patterns of state management with React using Redux & Apollo and their comparision",
    "track": "JavaScript",
    "abstract": "<p>The talk will be mainly around the state managements techniques and patterns used in ReactJs and throw light on the coding patterns for Redux and Apollo.\nI will be discussing about the the common patterns of the same and talk about the advantages and disadvantages of both. Lastly and most importantly, will\ntalk about an Open Source Voice Assistant - SUSi.AI implemented using ReactJs and Redux and the app architecture.</p>",
    "description": "<p>Minutes of the event</p>\n\n<p>00:00 - 00:01 - Basic introduction of myself and what I do.\n00:01 - 00: 04 - It will start with the brief introduction of ReactJs and the need of state management (both local and global).\n00:04 - 00:06 - Tell about the solutions in the market like Redux, MobX, Apollo.\n00:06 - 00:15 - Discuss about the commonly used coding patterns using them with examples and the best practices for them.\n00:15 - 00:20 - Talk about how we are making SUSI.AI - An Open Source Voice Assistant for the web and how is the app architectured to build an immersive and real time experience.</p>\n\n<p>Last 5 mins for the Q&amp;A session.</p>",
    "persons": [
      "Akshat Garg"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Improve React App using design patterns",
    "subtitle": "Don't reinvent the wheel",
    "track": "JavaScript",
    "abstract": "<p>Design patterns in layman's term are solutions to a recurring set of problems. Every problem has multiple solutions, design patterns are essentially a collection of the best solution for commonly occurring problems. These are not code snippets but the structure of the solution rather than the solution itself. Developers should learn design patterns so that they don’t have to come up with new solutions to every problem for which optimal tested solutions already exist(Standing on the shoulder of giants). Participants will also learn a common vocabulary to talk about solution patterns (Ex: “You can use a higher order component to solve X”). Once you understand design patterns you can identify them when you are going through a codebase and understand it from a higher perspective easily thus saving your time. Developers will be equipped with an array of patterns from which she can choose reusable solutions and avoid alternatives that can limit reusability. Developers will learn to write solutions in a mannerism that would be easily understandable by other developers(very important in large teams).</p>",
    "description": "<p>Design patterns in layman's term are solutions to a recurring set of problems. Every problem has multiple solutions, design patterns are essentially a collection of the best solution for commonly occurring problems. These are not code snippets but the structure of the solution rather than the solution itself. Developers should learn design patterns so that they don’t have to come up with new solutions to every problem for which optimal tested solutions already exist(Standing on the shoulder of giants). Participants will also learn a common vocabulary to talk about solution patterns (Ex: “You can use a higher order component to solve X”). Once you understand design patterns you can identify them when you are going through a codebase and understand it from a higher perspective easily thus saving your time. Developers will be equipped with an array of patterns from which she can choose reusable solutions and avoid alternatives that can limit reusability. Developers will learn to write solutions in a mannerism that would be easily understandable by other developers(very important in large teams).</p>\n\n<p>Writing components is hard, whereas writing reusable components is even harder.  As languages have become more powerful and have aspects of multiple programming paradigms(FP, OOP) it has opened up a new domain for us to explore in terms of design patterns.</p>\n\n<p>Participants will learn patterns that are commonly used in ReactJS. We will demonstrate a handful of design patterns which we have across, old and new while working on the Red Hat OpenShift(Container platform designed to improve developer productivity.) user interface. They will learn to identify problems that can be easily solved with design patterns. We will also show implementations and share the thought process that goes into deciding the design pattern so they can learn how to implement and judge whether it’s the best one.</p>\n\n<p>We expect participants to be familiar with Javascript and ReactJS. If they would like to follow along then a laptop would be useful. This talk would be helpful to anyone who is new to React or even to a seasoned developer who is not aware of these patterns.</p>\n\n<p>One short description of a design pattern and its use case:\nWe can consider the problem of composing a collection of components which all need to share a common state.\nBad Solution: Pass the state information to all the components through props manually.\nGood Solution: Create a parent component holding the state. Use components that require common state information as children of the parent component. And inside the parent component using React.Children.map function to pass the state information to all the children through cloning.\nThe good solution is known as “Compound Component” one of the many patterns we are going to talk about.</p>",
    "persons": [
      "Ankush Behl"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Let's Get Func-y",
    "subtitle": "",
    "track": "JavaScript",
    "abstract": "<p>a.k.a. a web app is just multiple functions stacked on top of each other, wearing a trenchcoat.\nLet's get back to basics, for a minute. No frameworks, no libraries; just pure and plain ol' JavaScript.\nIn a nutshell, this talk is going to explain all the delightful ways to write functional code with Javascript and how it can be applied to your application to optimize performance.\nWe're going to focus on keywords like 'immutability' and 'state'.\nIf we're feeling adventurous, we'll even throw in a closure or two.</p>",
    "description": "",
    "persons": [
      "Jemima Abu"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Are PWAs ready to take over the world?",
    "subtitle": "Implementing main progressive web app features in practice",
    "track": "JavaScript",
    "abstract": "<p>This talk offers a walk-through of the main PWA features and a comparison how they behave across different platforms (Linux, Android, iOS), on various web browsers (Firefox, Chrome, Safari). Practical code examples will come from Sojourner - a FOSDEM conference companion app. We will also discuss some UX/UI challenges and their potential solutions specific for PWAs.</p>",
    "description": "<p>During this talk we will discuss the following aspects of PWA and illustrate them with practical code examples.</p>\n\n<p>Performance and network reliability:\n- Service Worker strategies\n- Offline-First operation</p>\n\n<p>Persistent Storage:\n- availability, limitations, access request\n- no login required</p>\n\n<p>Installation:\n- In-browser via Add to Home Screen (A2HS)\n- From app-store</p>\n\n<p>Design:\n- choosing the right design system\n- handling colors, icons and screen mode gracefully</p>\n\n<p>At the end we will summarize the state of PWAs in 2020 and for which types of applications they work best.</p>\n\n<p>Examples used to illustrate this talk will come from Sojourner FOSDEM conference companion app, written using Vue.js framework.</p>",
    "persons": [
      "Jarek Lipski"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "2nd Generation JavaScript Frameworks & Libraries: Beyond Angular, React, and Vue!",
    "subtitle": "",
    "track": "JavaScript",
    "abstract": "<p>An overview of an interesting new development over the past years -- many vendors, large and small, have been making their JavaScript-based technology stacks available on GitHub. What does that mean and how to evaluate this development? Find out in this session, which includes small code demos and tips and tricks.</p>",
    "description": "<p>Did you know that over the past few years, large enterprises have been developing and open sourcing their JavaScript technology stacks? On GitHub, you'll find solutions by ING, Uber, PayPal, the Financial Times, Oracle, Microsoft, and many others. Some of these are software vendors, while others are in a variety of other industries. Each of them start from open source frameworks and libraries and all of them are interested in contributions.</p>\n\n<p>The session, with several live coding scenarios, focuses on something that's been going on below the surface, mostly unseen: large enterprises are using open source solutions in the JavaScript ecosystem (e.g., React, Vue, Knockout, Angular), developing their own internal tech stacks, and then pushing these stacks out to GitHub.</p>\n\n<p>Let's explore the advantages of these and see what can be done and how practical these developments are.</p>",
    "persons": [
      "Geertjan Wielenga"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Pushing the limits of the Web with WebAssembly",
    "subtitle": "",
    "track": "JavaScript",
    "abstract": "<p>The Web is omnipresent nowadays and meets most of the needs of our applications.</p>\n\n<p>For almost 10 years, leading browsers have been working hard to push the boundaries and performance of our connected apps.</p>\n\n<p>This is also the main reason why WebAssembly, the new binary standard initially implemented by Safari, Chrome, Firefox and Edge, appeared. It allows the execution, in the browser, of your favourite programming languages ​​at almost native speed.</p>",
    "description": "<p>During this session, we browse the current means implemented by browsers to optimise the execution of JavaScript code. We will outline the limitations of current solutions, the benefits provided by the WebAssembly and review its performance.</p>\n\n<p>Enjoy the future of the Web, now!</p>",
    "persons": [
      "Jon Lopez Garcia"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "State of Node.js Core",
    "subtitle": "",
    "track": "JavaScript",
    "abstract": "<p>Node.js is now over a decade old. With Node.js 12 just entering into long-term support, and Node.js 13 being released, let us take a look at the new features, breaking changes, and what is next.</p>",
    "description": "",
    "persons": [
      "Bethany Griggs"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Serverless.com framework",
    "subtitle": "Doing serverless in the open source way",
    "track": "JavaScript",
    "abstract": "<p>Who told that everything about serverless computing should be proprietary?\nDo you want deploy your functions and infrastructure in the open source way?\nDo you want to have modular, JS-based tool for it?\nCome and learn about Serverless.com - open source multicloud tool which support Kubeless, AWS Lambdas, Azure functions and many more!</p>",
    "description": "<p>2- minutes talk covering topics:\n- what is the serverless computing?\n- why is it important to have an open-source deployment tool?\n- how serverless framework working\n- advantages, weak moments and lesson learned</p>",
    "persons": [
      "Kirill Kolyaskin"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "New features of Vue 3.0",
    "subtitle": "",
    "track": "JavaScript",
    "abstract": "<p>Vue 3.0 is scheduled to be released in Q1 2020. With lots of new features in Vuejs 3.0 we look at biggest features and how they can be used in your code base to improve your programming experience.</p>",
    "description": "<p>Title: New features in Vue Js 3.0</p>\n\n<p>Description:\nVue 3.0 is scheduled to be released in Q1 2020. It has been 6 years since the first version 0.6 of Vue Js was released. Since then the community has grown and it is now in the top 3 most popular JavaScript frameworks to use. Currently Vuejs 2.6 is released which has a lot of plugins that can expand the programming and functionality features. Some of these programming features have been incorporated in vue 3.0 code base.</p>\n\n<p>The talk will explain the following new features and demo how they are used.</p>\n\n<ul>\n<li>Composition API</li>\n<li>Slots</li>\n<li>Optimized Slots Generations</li>\n<li>Proxy based observations</li>\n<li>Fragments</li>\n<li>Suspense components</li>\n<li>Multiple v-models</li>\n<li>New custom directives API</li>\n</ul>",
    "persons": [
      "Martin Naughton"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "How to create Javascript-powered Smartglasses",
    "subtitle": "(no soldering knowledge required)",
    "track": "JavaScript",
    "abstract": "<p>After having worked at an AR-focused company with the Vuzix, a Google Glass-like product, Ruben had a lot of fun with the hardware and really wanted to have one at home to play around with. Unfortunately, they are too expensive to get one just for hobby purposes.</p>\n\n<p>This session will cover what was required to build his own wearable, the pitfalls, the compromises, and the sheer joy of saying \"Screw it, I'll build it myself!\".</p>",
    "description": "<p>With the use of a Raspberry Pi, Vufine and a lot of Javascript, I've managed to cobble a hobby project together where I essentially have my own Javascript-powered smartglasses. The main reason why I built it is because I want to prototype ideas on what you could with the platform, but I think it's an interesting example to showcase how you can use Javascript and all out of the box. While the focus of the talk is how the stack works and everything is connected to each other, I also want to highlight what you can get out of hobby projects.</p>",
    "persons": [
      "Ruben van der Leun"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 25,
    "room": "H.1302 (Depage)",
    "title": "Web of Twins",
    "subtitle": "From IoT to Immersive worlds and beyond...",
    "track": "JavaScript",
    "abstract": "<p>Overview of Web of thing concept, Mozilla WebThings IoT platform demonstrated using A-Frame 3D framework for building virtual reality experiences.</p>",
    "description": "<p>The Web of Things connects real-world objects to the World Wide Web,\nMozilla proposed an open source implementation of Web of things concept.\nto connect and control smart home devices with Privacy by design.</p>\n\n<p>Once the WebThing platform is in place WoT can be used for many other purposes,</p>\n\n<p>As a demonstration, devices can be also \"mirrored\" in a virtual world\nand interacted differently using XR devices.</p>\n\n<p>Using A-Frame framework it's very easy to create models\nand keep the the Digital Twins updated in real time on the Web.</p>\n\n<p>Each components of this \"Web of Twins\" experiment will be detailed\nfrom sensors or actuators to rich 3D user interfaces and more.</p>",
    "persons": [
      "Philippe Coval"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 30,
    "room": "H.1308 (Rolin)",
    "title": "STS in Ceph Object Storage",
    "subtitle": "",
    "track": "Software Defined Storage",
    "abstract": "<p>Ceph is an open source, highly scalable, software defined storage that provides object, file and block interfaces under a unified system. Ceph Object Storage Gateway (RGW) provides a RESTful object storage interface to the Ceph Storage cluster. It provides an interface that is compatible with a large subset of AWS S3 APIs.</p>\n\n<p>In this talk we discuss the implementation of a subset of the APIs of AWS Secure Token Service (STS). AWS STS is a web service which enables identity federation and cross-account access by providing temporary security credentials.</p>\n\n<p>Ceph Object Storage Gateway now supports some APIs of AWS STS particularly related to web identity federation and cross-account access. The advantages of these temporary credentials are that they automatically expire after a certain duration, provide limited access (via IAM policies) to resources, are provided to the user upon request, and obviate the need for users/ applications to save permanent security credentials thereby removing a potential security loophole.</p>\n\n<p>As an example consider a web application that has users and needs access to RGW S3 buckets to read/ write large files. The application can delegate identity management to a trusted third party identity provider(IDP). It can get temporary credentials from STS after authenticating with the IDP and access the required RGW S3 buckets.</p>",
    "description": "<p>Outline of the talk:</p>\n\n<ol>\n<li>Introduction to Ceph and Ceph Object Storage Gateway</li>\n<li>Current authentication mechanisms in Ceph Object Storage Gateway</li>\n<li>AWS Secure Token Service</li>\n<li>STS APIs implemented in Ceph Object Storage</li>\n<li>Advantages of using STS</li>\n<li>Example</li>\n<li>Future Work</li>\n</ol>",
    "persons": [
      "Pritha Srivastava"
    ]
  },
  {
    "start": 1577871300000,
    "duration": 30,
    "room": "H.1308 (Rolin)",
    "title": "NFS Ganesha",
    "subtitle": "",
    "track": "Software Defined Storage",
    "abstract": "<p>NFS-Ganesha is an extensible user-space NFS server that supports NFS v3, v4, v4.1, v4.2, pNFS, and 9P protocol. It has an easily pluggable architecture called FSAL (File System Abstraction Layer), which enables seamless integration with many filesystem backends (GlusterFS, Ceph, etc.). There will be a discussion on the components along with an architectural explanation of NFS Ganesha with a detailed look at how a request flows through the various layers of NFS Ganesha and see some critical aspects in using NFS Ganesha. Along with the discussion on \"your first contribution to NFS Ganesha\" the audience will be engaged in a collaborative session and with a live demo, take a detailed look at the Clustered HA implementation using pacemaker/corosync with a specific example of a distributed storage, GlusterFS.</p>\n\n<p>Finally, there will be an open dialogue about the inclusion of Transport Layer Security into NFS Ganesha. One major drawback seen with NFS is the lack of transmitting encrypted data packets to and from NFS Server and Client. This lack is widely frowned upon, and it seems like there is enough communication gap within the community about its development, which I hope to shorten and revive the chatter to begin progress in this course.</p>",
    "description": "<p>The intentions behind this talk:</p>\n\n<p>-> Introduce architectural explanation of NFS Ganesha\n-> A short code walk-through to explore crucial features of NFS Ganesha\n-> Live demo of Clustered High Availability implementation using pacemaker/corosync\n-> Talk about the implementation of Transport Layer Security(TLS) into NFS Ganesha</p>",
    "persons": [
      "Arjun Sharma"
    ]
  },
  {
    "start": 1577873400000,
    "duration": 20,
    "room": "H.1308 (Rolin)",
    "title": "Evolution of path based Geo-replication in Gluster",
    "subtitle": "",
    "track": "Software Defined Storage",
    "abstract": "<p>As data is becoming more and more important in the world, we can't afford to lose it even if there is a natural calamity. We will see how Geo-Replication came in to solve this problem for us and how it evolved over the days.\nThrough this session, the users will learn how easy it is to set up Georep for Gluster to use it for their storage and back up their data with minimal understanding of storage and linux. Having a basic Gluster knowledge will make it even more easy</p>",
    "description": "",
    "persons": [
      "Hari Gowtham"
    ]
  },
  {
    "start": 1577874900000,
    "duration": 15,
    "room": "H.1308 (Rolin)",
    "title": "Run ZFS in userspace",
    "subtitle": "How we used ZFS in userspace for storage engine cStor",
    "track": "Software Defined Storage",
    "abstract": "<p>While running in user space ZFS utilizes a user space binary called ztest.\nIn cStor, we followed a similar approach to create a binary called ‘zrepl’ that is part of cStor. It has been built using the libraries similar to what is used for ztest and contains transactional, pooled storage layers.\ncStor uses ZFS behind the scenes by running it in the user space. This talk we will discuss in detail how we used ZFS in userspace for storage engine cStor and highlight a few challenges that our team faced while building this data engine.</p>",
    "description": "",
    "persons": [
      "Harshita Sharma"
    ]
  },
  {
    "start": 1577876100000,
    "duration": 35,
    "room": "H.1308 (Rolin)",
    "title": "What's new in Samba ?",
    "subtitle": "Latest news from the Samba project",
    "track": "Software Defined Storage",
    "abstract": "<p>The presentation will give an overview of all the changes happening in the Samba project code, from the fileserver virtual filesystem (VFS) rewrite, the new features in the SMB3 code, the quest to remove the old SMB1 protocol and much more. Improvements in Samba scalability, clustering and the Active Directory code will be discussed.</p>\n\n<p>The intended audience is anyone who uses the Samba code, creates products with Samba or is interested in the SMB protocol.</p>",
    "description": "<p>The presentation will give an overview of all the changes happening in the Samba project code, from the fileserver virtual filesystem (VFS) rewrite, the new features in the SMB3 code, the quest to remove the old SMB1 protocol and much more. Improvements in Samba scalability, clustering and the Active Directory code will be discussed. I'll also cover the changes to Samba development and tooling, and how we are modernizing the code base to stay relevant in the Cloud-connected world of software defined storage.</p>\n\n<p>This can be either a 30 minute talk (20 mins + 5 questions) or 60 minute talk (45+10 for questions).</p>",
    "persons": [
      "Jeremy Allison"
    ]
  },
  {
    "start": 1577878500000,
    "duration": 40,
    "room": "H.1308 (Rolin)",
    "title": "Asynchronous Directory Operations in CephFS",
    "subtitle": "",
    "track": "Software Defined Storage",
    "abstract": "<p>Metadata-heavy workloads are often the bane of networked and clustered filesystems. Directory operations (create and unlink, in particular) usually involve making a synchronous request to a server on the network, which can be very slow.</p>\n\n<p>CephFS however has a novel mechanism for delegating the ability for clients to do certain operations locally. While that mechanism has mostly been used to delegate capabilities on normal files in the past, it's possible to extend this to cover certain types of directory operations as well.</p>\n\n<p>The talk will describe work that is being done to bring asynchronous directory operations to CephFS. It will cover the design and tradeoffs necessary to allow for asynchronous directory operations, discuss the server and client-side infrastructure being added to support it, and what performance gains we expect to gain from this.</p>",
    "description": "<p>This is preliminary and may change between now and the conference, but this is what I'm planning to cover:</p>\n\n<ul>\n<li>overview of problem (why metadata operations are so slow on network filesystems) and proposed solution</li>\n<li>what about error handling?</li>\n<li>CephFS caps</li>\n<li>DIR<em>UNLINK and DIR</em>CREATE caps</li>\n<li>directory completeness and dentry revalidation</li>\n<li>asynchronous unlink</li>\n<li>Inode number delegation</li>\n<li>asynchronous creates</li>\n<li>benchmarks</li>\n</ul>",
    "persons": [
      "Patrick Donnelly",
      "Jeff Layton"
    ]
  },
  {
    "start": 1577880900000,
    "duration": 35,
    "room": "H.1308 (Rolin)",
    "title": "Rook Cloud Native Storage for Kubernetes",
    "subtitle": "Overview and what is new about Rook",
    "track": "Software Defined Storage",
    "abstract": "<p>What is Rook and the architecture of Rook + the storage run in Kubernetes.\nWe'll also take a look at new features added to Rook.</p>",
    "description": "<p>The talk will give an overview of what Rook can do and what is new since last years talk about Rook Ceph storage.\nThe overview will be about what Rook is and the architecture.\nThe second part is going to show newly added features to Rook.</p>\n\n<p>Agenda:\n* What is Rook\n* Architecture of Rook\n* New features\n  * Ceph\n  * New Storage Backend: Yugabyte\n  * EdgeFS\n  * Upcoming</p>\n\n<p>Target audience are people interested in Rook, Ceph and Kubernetes.</p>",
    "persons": [
      "Alexander Trost"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 45,
    "room": "H.1308 (Rolin)",
    "title": "Building Blocks for Containerized Ceph",
    "subtitle": "How Raw Block PersistentVolumes Changed the Way We Look at Storage in Kubernetes",
    "track": "Software Defined Storage",
    "abstract": "<p>Originally, Kubernetes PersistentVolumes (PVs) could only present storage to containers as filesystems. Now, raw block PersistentVolumes (PVs) allow applications to consume storage in a new way. In particular, Rook-Ceph now makes use of them to provide the backing store for its clustered storage in a more Kubernetes-like fashion and with improved security. Now we can rethink the notion of how we structure our storage clusters, moving the focus away from static nodes and basing them on more dynamic, resilient storage devices.</p>",
    "description": "<p>Originally, Kubernetes PersistentVolumes (PVs) could only present storage to containers as filesystems. However, some applications prefer to use block storage, usually for reasons of performance, and have no need for a full filesystem. Several such applications have had to go as far as directly accessing local system directories to get the functionality they need in Kubernetes.</p>\n\n<p>Raw block PVs are a relatively new feature that went beta in Kubernetes 1.13. They allow Kubernetes to present storage to containers as block devices, removing the need for provisioners to format filesystems on top of them. This not only allows for greater performance to the applications that expect it, it also helps improve security by reducing the level of permissions such an application's containers require to run.</p>\n\n<p>Rook-Ceph is the Ceph operator for the Rook project. It provides resilient storage by running the various Ceph components as containers and managing them via Kubernetes. Originally, it would bind-mount system directories to manipulate the storage devices it consumed. It now leverages raw block PVs to store its data, expanding the types of storage it can consume. In particular, cloud environments are now a space where its storage Pods can migrate in response to node failures and have the storage devices move with their Pods.</p>\n\n<p>For the uninitiated, this presentation will start with an overview of how storage is modeled and presented in Kubernetes. It will then describe how that storage was originally consumed by Rook-Ceph, what we changed about it, and the consequences (both good and bad) of those changes.</p>",
    "persons": [
      "Jose Rivera",
      "Rohan Gupta"
    ]
  },
  {
    "start": 1577886600000,
    "duration": 35,
    "room": "H.1308 (Rolin)",
    "title": "Explicitly Supporting Stretch Clusters in Ceph",
    "subtitle": "",
    "track": "Software Defined Storage",
    "abstract": "<p>Ceph is an open source distributed object store, network block device, and file system designed for reliability, performance, and scalability. While Ceph is designed for use in a single data center, users have deployed “stretch” clusters across multiple data centers for many years, and deploying Ceph to back Red Hat’s OpenShift Container Storage product required us to support that workload explicitly and well — in particular, in the face of netsplits.\nThis requires improvements to our “monitor” leader elections and to the “OSD” peering process to keep data available without breaking our data integrity guarantees. This talk presents the whole cycle of that work from an algorithm and programmer perspective: the dangers we identified, the changes we needed, the architecture changes to support faster test iteration and coding, and the results.</p>",
    "description": "",
    "persons": [
      "Gregory Farnum"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 45,
    "room": "H.1308 (Rolin)",
    "title": "A 'Thin Arbiter' for glusterfs replication",
    "subtitle": "",
    "track": "Software Defined Storage",
    "abstract": "<p>Maintaining consistency in replication is a challenging problem involving locking of nodes, quorum checks and reconciliation of state, all of which impact performance of the I/O path if not done right. In a distributed system, a minimum of 3 nodes storing metadata is an imperative to achieve consensus and prevent the dreaded split-brain state. Gluster has had solutions like the trusted 3-way replication or the ' 2 replica + 1 arbiter' configuration to achieve this.</p>\n\n<p>The latest in the series is a 'Thin Arbiter (TA)' which is more minimalist the existing '1 arbiter', targeted at container platforms and cloud deployments. A TA node can be deployed outside a gluster cluster and can be shared with multiple gluster volumes. It requires zilch storage space and does not affect I/O path latencies in the happy case. This talk describes the design, working and deployment of TA and the potential gotchas one needs to be aware of while choosing this solution.</p>\n\n<p>The intended audience is sysadmins/dev-ops personnel who might want to try out the thin-arbiter volume and troubleshoot any operational issues that may arise.</p>",
    "description": "<p>The Thin Arbiter (TA) is different from normal arbitration logic in the sense that  even if only one file is bad in one of the copies of the replica, it marks that entire replica unavailable (despite it having other files in it that  are healthy), until it is healed and syncs up to the other good copy.  While this might seem like a very bad idea for a highly available system,  it works very well to prevent split-brains due to intermittent network disconnects rather than a whole node going off-line indefinitely. In talking about this feature, my talk will cover:</p>\n\n<ul>\n<li>Introduction to how synchronous replication in gluster works.</li>\n<li>The role of quorum in preventing split-brains.</li>\n<li>Briefly describe the working of replica 3 and arbiter volumes.</li>\n<li>The basic idea behind thin-arbiter based replication.</li>\n<li>Explain the state machine behind the thin-arbiter transaction model.</li>\n<li>Describe how it can be installed and used.</li>\n</ul>",
    "persons": [
      "Ravishankar N."
    ]
  },
  {
    "start": 1577892000000,
    "duration": 25,
    "room": "H.1308 (Rolin)",
    "title": "Management of Storage on OpenShift",
    "subtitle": "Managing storage was never so easy",
    "track": "Software Defined Storage",
    "abstract": "<p>This talk will walk-through how users can deploy storage on OpenShift and manage it all from the browser. With just a few clicks and almost zero questions asked, we will demonstrate how anyone can deploy &amp; manage storage like never before. From beginners to experts, this session has fun bits for every storage enthusiast.</p>",
    "description": "<p>Goal is to enable every participant to set up their own storage cluster and manage it from the comforts of their browser.\nNo hefty configurations, just a few clicks to get going.</p>\n\n<p><em>Key Takeaways</em>\n- Operator Lifecycle Manager Overview\n- Storage Operators Overview\n- Container Storage in OpenShift\n- Rook, Ceph\n- Management Console for Storage</p>",
    "persons": [
      "Ankush Behl",
      "Umanga Chapagain"
    ]
  },
  {
    "start": 1577893800000,
    "duration": 30,
    "room": "H.1308 (Rolin)",
    "title": "The history of error correction and detection and how it led to Ceph’s Erasure Coding Techniques",
    "subtitle": "",
    "track": "Software Defined Storage",
    "abstract": "<p>70 years of academic innovation in the development of error correction codes have led to the advanced erasure coding techniques that we use in Ceph. Learn more about how these came about, the different types, how they work, and how we use them in distributed storage today.</p>",
    "description": "<p>Erasure Coding is the latest in a long line of error detection and correction approaches over the last 70 years which have all had an impact on the way we approach storing and recovering data in sensible and efficient ways. I’ll give an overview of the main approaches over the years, including the parity bit, the hamming codes, RAID, reed-solomon, and how they have impacted media storage, distributed storage, and their usage in other unexpected ways. I’ll then provide an overview of erasure coding across distributed storage and specifically Ceph.</p>",
    "persons": [
      "Danny Abukalam"
    ]
  },
  {
    "start": 1577895900000,
    "duration": 35,
    "room": "H.1308 (Rolin)",
    "title": "Ephemeral Pinning: A Dynamic Metadata Management Strategy for CephFS",
    "subtitle": "",
    "track": "Software Defined Storage",
    "abstract": "<p>Having a separate cluster of Metadata Servers (MDS) is a well known design strategy among distributed file-system architectures.  One challenge faced by this approach is how to distribute metadata among the MDSs. Unlike data storage and it's associated I/O throughput, which can be scaled linearly with the number of storage devices, file-system metadata is a fairly complex entity to scale due to it's hierarchical nature. In hindsight, a pure hashing based metadata distribution strategy seems like a perfect fit. But, this is not exactly the case. What are the pitfalls then? Too many inter-MDS hops (due to POSIX traversal semantics), loss of hierarchical locality degrades file-system performance, and as a result, this is not beneficial for a workload whose directory hierarchy tree grows in depth rather than breadth. CephFS's metadata balancer takes a different approach by partitioning metadata sub-trees across MDSs thereby preserving good locality benefits. Although efficient, this involves a lot of back and forth migrations of sub-trees and the locality benefits are sometimes trumped by sub-optimal distributions.</p>\n\n<p>In this talk, we present a new metadata distribution strategy employed in CephFS - Ephemeral Pinning. This strategy combines the benefits of hashing and naive sub-tree partitioning by intelligently pinning sub-trees to MDSs so as to obtain a balanced distribution as the workload metadata grows by depth and breadth. A consistent hashing based load balancer helps in maintaining an optimal distribution during addition or failure of MDSs.</p>",
    "description": "<p>This talk will cover the following key ideas:</p>\n\n<ul>\n<li>How metadata is handled in distributed file systems.</li>\n<li>Why it is so important to have an optimal distribution of metadata among Metadata Servers.</li>\n<li>The drawbacks and advantages of commonly used and popular metadata distribution strategies.</li>\n<li>Ephemeral Pinning - The new metadata distribution strategy employed by CephFS. We will explain the design and implementation of the distribution strategy and delineate it's strong suits.</li>\n</ul>\n\n\n<p>This talk would be beneficial for every distributed file-system project that handles file metadata separately. They would get an overview on existing metadata distribution strategies - it's pitfall's and benefits and the reason why we at CephFS came up with this approach. The benefit's of using consistent hashing for distributing metadata are also discussed.</p>",
    "persons": [
      "Sidharth Anupkrishnan"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "Landscape of new challenges in modern virtualization platforms",
    "subtitle": "Tackling new issues in virtualization: security, performance, use cases and more",
    "track": "Virtualization and IaaS",
    "abstract": "<p>Because virtualization is everywhere, new challenges in the IT world are revealing that this crucial component has to be improved on a regular basis. This requires a lot of coordination between Open Source projects as well as intense research and development efforts.</p>\n\n<p>NVMe storage performance revealing hidden bottlenecks, Intel CPU flaws changing the security landscape regarding isolation, increasing complexity of stacks requiring more and more components working together, hardware specialization, new protocols, new use cases on top (k8s): these are a few of the challenges that a virtualization platform must answer in 2020.</p>\n\n<p>We'll first see a landscape of these new challenges, then the possible approaches to solve them, and finally a concrete example of what the XCP-ng project is doing to integrate all these changes in a fully Open Source fashion, inside a turnkey Xen distro.</p>",
    "description": "",
    "persons": [
      "Olivier Lambert"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "Public clouds and vulnerable CPUs: are we secure?",
    "subtitle": "",
    "track": "Virtualization and IaaS",
    "abstract": "<p>A whole bunch of CPU vulnerabilities were revealed in the past few years:\nMeltdown and Spectre, SSB, L1TF and MDS -- and there's little hope that we've\nseen them all. Every time there is a new vulnerability released, big cloud\nprovides on day 1 claim that their hosts were updated and that their users\nare secure. Is this so or do we also need to do something inside our Linux\nguests to mitigate these vulnerabilities? And, do we have the required tools\nto actually do the mitigations? Are all of them enabled by default or not? And,\nif not, why? In the talk I'll try to answer these questions.</p>",
    "description": "<p>The talk will cover recently discovered CPU vulnerabilities starting with\nMeltdown and Spectre. I will go through them and try to highlight 'public\ncloud specifics': what has/can to be done in the infrastructure of the\ncloud and what has/can be done inside Linux guests depending on the desired\nlevel of security and usage patterns.</p>",
    "persons": [
      "Vitaly Kuznetsov"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "virtio-fs",
    "subtitle": "A shared file system for virtual machines",
    "track": "Virtualization and IaaS",
    "abstract": "<p>This talk covers the new virtio-fs shared file system that allows a host directory tree to be shared with guests.  Sharing files with the guest is required by several use cases including container VMs, File-System-as-a-Service, and traditional virtualization.  virtio-fs goes further than previous attempts by taking advantage of the co-location of the guest and host using DAX to share the host page cache.  This presentation explains how to use virtio-fs, a bit about how it works internally, and the current status.</p>",
    "description": "<p>virtio-fs is a new shared file system giving access to a directory that both the host and guests can access.  Traditionally shared file systems have been used to make data available to the guest during installation, boot a guest from a directory tree on the host, or to develop code on the host and test it in-place without copying files into the guest.  New use cases including container VMs and File-System-as-a-Service have introduced new requirements that virtio-fs is designed to meet.</p>\n\n<p>Previous attempts at shared file systems have included virtio-9p or simply used network file systems.  virtio-fs is unique because it is possible to access files directly from the host page cache.  This eliminates data copies and communication, resulting in lower memory footprint and higher performance for many workloads.</p>\n\n<p>This presentation explains how to use virtio-fs and covers its architecture.  It includes performance benchmarks showing how various features perform and a comparison with virtio-9p.</p>",
    "persons": [
      "Stefan Hajnoczi"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "io_uring in QEMU: high-performance disk I/O for Linux",
    "subtitle": "",
    "track": "Virtualization and IaaS",
    "abstract": "<p>io<em>uring is a new kernel asynchronous I/O processing mechanism proposed as a much faster alternative for conventional Linux AIO. Patches were merged in Linux 5.1 and gave a promised performance boost. We decided to integrate it into QEMU to make virtualized storage devices work more efficiently. Let's take a look at how io</em>uring works in QEMU.</p>\n\n<p>You will get a brief overview of the new kernel feature, how we used it in QEMU, combined its capabilities to speed up storage in VMs and what performance we achieved. Should io_uring be the new default AIO engine in QEMU? Come and find out!</p>",
    "description": "<p>io<em>uring is a new kernel asynchronous I/O processing mechanism proposed as a much faster alternative for conventional Linux AIO. Patches were merged in Linux 5.1 and gave a promised performance boost. We decided to integrate it into QEMU to make virtualized storage devices work more efficiently. io</em>uring enhances the existing Linux AIO API, and provides QEMU a flexible interface, allowing you to use the desired set of features: submission polling, completion polling, fd and memory buffer registration. By explaining these features we will come to examples of how and when you need to use them to get the most out of io_uring. Expect many benchmarks with different QEMU I/O engines and userspace storage solutions (SPDK).</p>\n\n<p>You will get a brief overview of the new kernel feature, how we used it in QEMU, combined its capabilities to speed up storage in VMs and what performance we achieved. Should io_uring be the new default AIO engine in QEMU? Come and find out!</p>",
    "persons": [
      "Julia Suvorova"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "Lightweight virtualization in the Cloud and at the Edge",
    "subtitle": "hypervisors gone rogue",
    "track": "Virtualization and IaaS",
    "abstract": "<p>Running applications in the Cloud has changed the way users develop and ship\ntheir code. Quite recently, the community has given rise to microservices-based\napproaches, towards solutions that follow the paradigm of Platform-, Software-,\nand Function-as-a-Service (PaaS, SaaS, and FaaS respectively).</p>\n\n<p>To accommodate user demands, while maintaining security and isolation, Cloud\nvendors have adopted a hybrid approach where user workloads are being executed\nin lightweight sandboxed environments, where micro-hypervisors provide the\nisolation and container-based images facilitate application deployment. As a\nresult, lighter virtualization stacks remains a key aspect to maximize\nperformance in a multi-tenant but isolated environment.</p>\n\n<p>To this end, we started experimenting with various Virtual Machine Monitors\n(VMMs) that could provide the ideal trade-off between performance, flexibility\nand application portability. In this talk, we present the design of a minimal\nVMM, based on KVM, residing entirely in the Linux Kernel and showcase the\nmerits and shortcomings (minimal footprint, security concerns), for each\nuse-case (Cloud FaaS, edge multi-tenancy). Additionally, we present our\nexperience from porting Firecracker to a low-power device (RPi4) demonstrating\nthe merits of lightweight hypervisor stacks for flexible application execution\nat the edge.</p>",
    "description": "",
    "persons": [
      "Anastassios Nanos",
      "Babis Chalios"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "LXD for mixed system containers and VM workloads",
    "subtitle": "Introducing LXD's new virtual machine feature",
    "track": "Virtualization and IaaS",
    "abstract": "<p>LXD is most known as a system container manager, offering a simple user experience and images for most Linux distributions.\nIt also offers a simple REST API, network and storage management, project views and easy clustering to dozen of hosts.</p>\n\n<p>Over the past few months, LXD has now grown the ability to run virtual machines alongside containers, using the exact same REST API and configuration.\nThis presentation will cover that new feature, why it was done, where it's at now and where we're going with it, as well as provide a quick demo of setting up a small LXD cluster and running both containers and virtual machines on it.</p>",
    "description": "<p>LXD is an open source system container manager, developed by the team behind LXC, written in Go and that's been around for over 5 years now.\nIt's widely used both on servers, running the backend of services such as Travis-CI and on everyday devices like Chromebooks.\nContainers are created from images with prebuilt images available for most Linux distributions.</p>\n\n<p>Multiple hosts can easily be clustered together to form one large virtual host, exposing the exact same API as a single host would.\nStorage pools and networks can also be created and managed through LXD and resources can be segmented into projects.</p>\n\n<p>With the addition of virtual machine support (through qemu), it is now possible to manage a mixed deployment of virtual machines and containers, sharing the same configuration, storage and networks. With the use of our built-in agent, the exact same operations that are normally possible against a container also become possible against virtual machines, including executing commands, transferring files, ...</p>\n\n<p>Existing API clients for LXD can also now drive both containers and virtual machines without any API changes required.</p>",
    "persons": [
      "Stéphane Graber"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "oVirt 4k - teaching an old dog new tricks",
    "subtitle": "",
    "track": "Virtualization and IaaS",
    "abstract": "<p>Teaching oVirt to work with 4k storage.</p>",
    "description": "<p>How can we have compression and deduplication using VDO, the new Linux\ncompression layer? How can we use the latest and greatest disks drives?\nWe need to support disks with 4k block size.</p>\n\n<p>oVirt is your best friend when you need to manage your virtualized data\ncenter, but when it was created 10 years ago, support for 4k storage was\nnot considered. Can you teach an old dog new tricks? Sure you can!</p>\n\n<p>In this talk we will share what we learned implementing 4k storage\nsupport in oVirt. We will present the challenges teaching old and\nstubborn code base to work with disks using 4k storage, and how we\naddressed them; introducing storage format v5, moving from sectors to\nbytes, detection of block size on file storage, improving testing in\nstorage area, adding new 4k APis to sanlock and improving qemu block\nsize detection.</p>\n\n<p>Audience:\noVirt administrator interested in utilizing the latest and greatest\nfeatures and hardware. Developers looking for new ways to deal with old\ncode.</p>",
    "persons": [
      "Nir Soffer"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "Edge Clouds with OpenNebula",
    "subtitle": "",
    "track": "Virtualization and IaaS",
    "abstract": "<p>Edge computing is currently getting a lot of traction thanks to the growing availability of rented computing resources around the world. The idea is based on moving the core computational logic and storage to distant locations that are closer to the entities they interact with (e.g. users or sensors). The benefits come from improving network latencies, increasing user experience with the provided service, and lowering the transfers to the central locations. Edge clouds bring the flexibility and proven workflows of cloud computing to the edge.</p>\n\n<p>OpenNebula is an open source framework to build private and hybrid clouds based on KVM, LXD, and/or VMware vCenter. While the main domain is the corporate private on-premises cloud, it comes with simple and extensible tooling (\"oneprovision\") for automated deployment of edge clouds. When provided with a deployment descriptor, it allocates the physical hosts on the public bare-metal cloud provider, configures all necessary services (e.g. install libvirt/KVM or LXD), and enables them for use in OpenNebula. The process is as simple as running a command-line tool and the cloud administrator gets a fully usable configured edge cluster in a few  minutes.</p>\n\n<p>As part of a usability validation exercise, we successfully deployed public gaming servers from scratch to running services on 17 different locations worldwide in just 25 minutes: https://opennebula.org/opennebula-a-lightning-fast-video-gaming-edge-use-case-2 .</p>\n\n<p>This talk introduces the OpenNebula \"edge\" concept and shows the current state, capabilities, and limitations of edge cloud deployment tooling. It explores the difficulties of running the IaaS-in-IaaS cloud and demonstrates with practical examples the use of tooling and management of edge deployments.</p>",
    "description": "",
    "persons": [
      "Vlastimil Holer"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "Baremetal at the Edge",
    "subtitle": "Managing bare metal machines where PXE would fail",
    "track": "Virtualization and IaaS",
    "abstract": "<p>Deploying bare metal machines at the edge of the cloud may not play well with conventional PXE protocol suite. In this presentation we will explain how the emerging virtual media boot technology could significantly improve scalability, reliability and security of the Cloud.</p>",
    "description": "<p>In this talk, the latest advancements in bare metal provisioning service (ironic) will be explained and PXE-less machine deployment will be demonstrated showcasing two scenarios - bare metal management within the OpenStack cloud and a stand-alone ironic use-case (e.g. within a container orchestration system).</p>",
    "persons": [
      "Ilya Etingof"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "A VM journey from VMware to Kubernetes",
    "subtitle": "",
    "track": "Virtualization and IaaS",
    "abstract": "<p>Kubernetes became primary platform for managing containerized applications.\nIn connection with KubeVirt, it can manage both containers and virtual machines in a single cluster to enable mixed workloads and so give second breath to existing legacy workloads based on virtual machines which might not be feasible to containerize for either technical or business reasons.</p>\n\n<p>Consolidation of so far distinct clusters for VMs and containers is the next logical step.\nCome to see an end to end conversion of a VMware virtual machine into Kubernetes.</p>",
    "description": "<p>Conversion of a virtual machine from VMware to Kubernetes will be presented.</p>\n\n<p>An attendee will learn:\n- briefly about KubeVirt (virtualization add-on for Kubernetes)\n- how to convert an existing VM to Kubernetes\n- implementation aspects (deep-dive)\n- about ongoing development and how to participate</p>",
    "persons": [
      "Marek Libra"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "Back to the future",
    "subtitle": "Incremental backup in oVirt",
    "track": "Virtualization and IaaS",
    "abstract": "<p>Do you need to go back in time to restore data from important VMs? oVirt\ndoes not provide a time machine yet, but you can build one using oVirt\nbackup APIs.</p>\n\n<p>Building on changed blocks tracking in qemu, and upcoming libvirt backup\nAPI, oVirt will provide API to perform incremental backups. You will be\nable to back up VMs more efficiently, downloading only changed blocks.\nIncremental backup will be simpler and more reliable, not requiring\ncreating and deleting snapshots. Uploading will support on-the-fly\nconversion from raw to qcow2 when restoring disks.</p>\n\n<p>In this talk we will travel into the future, introducing the oVirt\nincremental backup API for starting and ending backups, and the\novirt-imageio API for downloading changed blocks. Finally, we will travel\nback to the past, and show how to restore raw guest data into new disks.</p>\n\n<p>Audience:\nBackup vendors and virtualization developers, interested in utilizing\nincremental backup API. Also, oVirt administrators and users interested in\npeeking into the future of oVirt.</p>\n\n<p>Session summary:\nPeek into the future of oVirt backup API.</p>",
    "description": "<p>Do you need to go back in time to restore data from important VMs? oVirt\ndoes not provide a time machine yet, but you can build one using oVirt\nbackup APIs.</p>\n\n<p>Building on changed blocks tracking in qemu, and upcoming libvirt backup\nAPI, oVirt will provide API to perform incremental backups. You will be\nable to back up VMs more efficiently, downloading only changed blocks.\nIncremental backup will be simpler and more reliable, not requiring\ncreating and deleting snapshots. Uploading will support on-the-fly\nconversion from raw to qcow2 when restoring disks.</p>\n\n<p>In this talk we will travel into the future, introducing the oVirt\nincremental backup API for starting and ending backups, and the\novirt-imageio API for downloading changed blocks. Finally, we will travel\nback to the past, and show how to restore raw guest data into new disks.</p>\n\n<p>Audience:\nBackup vendors and virtualization developers, interested in utilizing\nincremental backup API. Also, oVirt administrators and users interested in\npeeking into the future of oVirt.</p>\n\n<p>Session summary:\nPeek into the future of oVirt backup API.</p>",
    "persons": [
      "Eyal Shenitzky",
      "Nir Soffer"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "Running virtual machines out of thin air",
    "subtitle": "",
    "track": "Virtualization and IaaS",
    "abstract": "<p>How to run virtual machines in oVirt without copying their disks.</p>",
    "description": "<p>Have you ever wanted to run your virtual machine in oVirt without copying its hugh disks?\nTired of checking the slow transfer progress? We did, and we have a good plan to avoid the wait!</p>\n\n<p>In this talk we show how oVirt can start virtual machine without copying the disks,\nusing external disk via NBD or other protocols supported by qemu.\nOnce the virtual machine is running, copy the disks in the background to oVirt storage.\nThis minimizes downtime to seconds instead of minutes, and can be used in many scenarios\nsuch as importing virtual machines from other systems (even from foreign systems via virt-v2v),\npreviewing backups before restore, and provisioning a virtual machines.</p>\n\n<p>Audience:\nVirtualization administrators or developers interested in oVirt architecture and would like a peek into future development.</p>",
    "persons": [
      "Nir Soffer",
      "Daniel Erez"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 30,
    "room": "H.1309 (Van Rijn)",
    "title": "Reaching \"EPYC\" Virtualization Performance",
    "subtitle": "Case Study: Tuning VMs for Best Performance on AMD EPYC 7002 Series Based Servers",
    "track": "Virtualization and IaaS",
    "abstract": "<p>Virtualization brings many advantages, but what about the overhead it introduces? What about performance? This talk will show how great virtualization performance can be achieved, if proper tuning is applied to all the components of the system: hypervisor, host and guests, for both Xen and KVM. As a case study, we will describe how we tuned our OS in order to be able to reach, inside VMs, close to baremetal performance, on a server powered by a CPU from the AMD EPYC 7002 (codename \"Rome\") series. We will, of course, show the benchmarks proving that (run on KVM), even when memory encryption is used.</p>",
    "description": "<p>Virtualization is great because it decouples the software from the hardware on top of which it runs, and this brings benefits in terms of flexibility, security, reliability and cost savings. But what about the overhead that this, unavoidably, introduces?</p>\n\n<p>Well, often enough, a virtualized system is really able to fulfill its goals with an acceptable quality of service, efficient exploitation of HW resources, satisfactory user experience, etc., only if all the components are configured properly. This is not entirely new, as baremetal systems need tuning too, but in a virtualized environment one has to take care of tuning both the the host and the guests. And beware that the interactions between all the different components may not always be straightforward, especially on a large server with complex CPU architecture, such anything based on the AMD EPYC 7002 (codename \"Rome\") series of processors.</p>\n\n<p>This talk will go over some of the typical virtualization “tuning tricks” (for both Xen and KVM). Then, as a case study, we will illustrate how we managed to reach, inside Virtual Machines, a performance level that almost matches the one of the host, on a server powered by a CPU from the AMD EPYC 7002 series. In fact, we will show the results of running CPU and memory intensive benchmarks (on KVM) with and without the suggested tuning. Last (but not least :-D), we will show the impact that the Secure Encrypted Virtualization (SEV) technology has on performance.</p>",
    "persons": [
      "Dario Faggioli"
    ]
  },
  {
    "start": 1577871000000,
    "duration": 20,
    "room": "H.2213",
    "title": "Introduction to the devroom and the Open Source Design collective",
    "subtitle": "",
    "track": "Open Source Design",
    "abstract": "<p>A brief introduction to the Open Source Design collective.</p>",
    "description": "<p>Every year we take some time to introduce the Open Source Design collective, what we do, where to find us and how to get involved.</p>",
    "persons": [
      "Bernard Tyers",
      "Amit Nambiar"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 20,
    "room": "H.2213",
    "title": "Open Source design - Africa",
    "subtitle": "Open Source Design movement in Africa",
    "track": "Open Source Design",
    "abstract": "<p>Open Source Community Africa (O.S.C.A) is a movement that promotes and drive the open source culture within and across Africa. We aim to bridge the diversity gap of the open source culture through advocacy because of potential and great energy coming from the continent. This presentation will help put the African ecosystem closer to the existing platforms which will bring more diversity that includes representing the black/African community showcasing how mentorship and training are doing centred around opensource.</p>",
    "description": "<p>This presentation will cover the challenges and limitations of design contributions from Africa. What OSCA is doing to bridge the gab. As a designer who has been in the space of contributing to open source and has faced the challenges of how to convince a maintainer why design is as important as code, there has always been this problem which I have personally experienced as one who loves opensource, I got to understand the processes because I was patient enough to learn about the structure to influence it, but not every designer have that mindset to leave their comfort zone. So what I have been able to do as a designer is to influence the open source community Africa to take this challenge as a project as I believe it is something that designers should learn as a process of collaboration.</p>",
    "persons": [
      "Peace Ojemeh"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 20,
    "room": "H.2213",
    "title": "What are we talking about when we say \"open design\"?",
    "subtitle": "",
    "track": "Open Source Design",
    "abstract": "<p>As designers working with Libre/Open Source software, we have a strong opinion on how tools shape practice. And as designers, in general, we care deeply about our methods, workflows, principles and licenses. For some time now we've been debating the issue of terminology: how we describe our practice to others and how free/libre software terms and ideas can be integrated into design methods.</p>\n\n<p>In this talk, we want to propose an exercise in labeling. Building up from conversations we had with other designers moving in the sphere of Libre Software and Libre Culture, we want to:\n- present terms and try to define their scope in the hopes of improving the ways in which open design can be explained to others\n- distinguish between possible stances inside the sphere of open design, such as using libre tools vs. proprietary toolchains, designing for free software vs. designing with free software\n- explore how we understand other designers/studios/communities' ideas and practices\n- better understand how we can present our views and work vis-à-vis other design approaches -- especially to \"traditional\" and proprietary-oriented audiences</p>\n\n<p>This talk is a follow-up to \"Open Design, Libre Graphics: Why terminology matters\", that we presented at Libre Graphics Meeting 2019. This was the starting point of a discussion we feel the need to bring to the table, along with other designers that share the love for F/LOSS.</p>",
    "description": "<p>As designers working with Libre/Open Source software, we have a strong opinion on how tools shape practice. And as designers, in general, we care deeply about our methods, workflows, principles and licenses. For some time now we've been debating the issue of terminology: how we describe our practice to others and how free/libre software terms and ideas can be integrated into design methods.</p>\n\n<p>In this talk, we want to propose an exercise in labeling. Building up from conversations we had with other designers moving in the sphere of Libre Software and Libre Culture, we want to:\n- present terms and try to define their scope in the hopes of improving the ways in which open design can be explained to others\n- distinguish between possible stances inside the sphere of open design, such as using libre tools vs. proprietary toolchains, designing for free software vs. designing with free software\n- explore how we understand other designers/studios/communities' ideas and practices\n- better understand how we can present our views and work vis-à-vis other design approaches -- especially to \"traditional\" and proprietary-oriented audiences</p>\n\n<p>This talk is a follow-up to \"Open Design, Libre Graphics: Why terminology matters\", that we presented at Libre Graphics Meeting 2019. This was the starting point of a discussion we feel the need to bring to the table, along with other designers that share the love for F/LOSS.</p>",
    "persons": [
      "Manufactura Independente"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 20,
    "room": "H.2213",
    "title": "Some Excerpts from the Theory of Design in Architecture",
    "subtitle": "",
    "track": "Open Source Design",
    "abstract": "<p>This talk discusses some theories from architectural discourse which attempt to solve problems which required critical and creative thinking. It is interesting to see the overflows and overlaps of theories between design(architecture) and construction(engineering) to solving similar underlying problems. The lengthy history of the field throws up some interesting references and may expose some ideas which might be applicable in new age design+engineering problems.</p>",
    "description": "<p>An architect as a professional in today's day and age represents someone involved in the construction industry tasked with a responsibility for conceptualizing physical spaces and bringing them into reality. Etymologically the word finds its origins in ancient Greek with the word being comprised of 'arkhi' and 'tekton' which loosely translate to 'master' and 'builder' correspondingly.</p>\n\n<p>Historically architectural design and construction was carried out by artisans such as stone masons and carpenters, who were coordinated by a \"master builder\". There was no clear distinction between architect and engineer. This has created a body of theoretical knowledge which spans across different industry.</p>\n\n<p>This created a field of study spanning the spectrum of creative thinking and logical thinking. The talk sheds light on some interesting examples of how 'architects' have dealt with problems in such a space.</p>",
    "persons": [
      "Amit Nambiar"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 20,
    "room": "H.2213",
    "title": "UI/UX Tips & Tricks for developers",
    "subtitle": "",
    "track": "Open Source Design",
    "abstract": "<p>I will present some general UI/UX tips &amp; tricks that will help you design better. Everyone should know the basic principles and patterns of design, and once you understand them you will naturally integrate them in your work.</p>",
    "description": "<p>UI/UX is a craft. The more you practice it, the better you are at it. Some people argue that you need to have 'good taste' in order to be a designer, to be the 'artsy type'. While this might be true for Graphic Design, Branding and Visual Arts in general, when it comes to Interface, Interaction and Product Design, the focus is more on practicality and 'common sense'.</p>",
    "persons": [
      "Ecaterina Moraru"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 20,
    "room": "H.2213",
    "title": "Accessibility in MuseScore",
    "subtitle": "Our experience with Qt and QML",
    "track": "Open Source Design",
    "abstract": "<p>MuseScore is the world's most popular sheet music program. It is used by millions of musicians around the world, including many who are blind, partially sighted, or who struggle to use a traditional mouse-based interface. In this presentation, we share our experience in making a popular open source program accessible to keyboard and screen reader control.</p>",
    "description": "<p>A screen reader is a \"robot voice\" that describes what happens on the screen, which enables people who are blind to use a computer. Screen readers cannot \"see pixels\", so it is up to the programmer to tell the screen reader what is going on. We give tips on how to do this in Qt’s traditional C++ framework as well as it’s JavaScript-based QML language.</p>\n\n<p>Screen readers know how to deal with buttons, menus and text, but they have no idea what a treble clef is! As a music notation program, many parts of MuseScore's interface are custom widgets that do not have a counterpart in traditional UI design. This poses additional challenges when it comes to implementing accessibility. We share the thinking behind our design decisions in MuseScore, and how these may be applied to other programs.</p>\n\n<p>Keyboard navigation is a key aspect of accessibility, not only for people who are blind, but also for people who are motor-impaired, or ordinary users who find it quicker to use the keyboard than the mouse. There is more to keyboard navigation than shortcuts and getting around with the Tab key! We talk about how we have introduced groupings and hierarchy into MuseScore’s design to improve the experience for keyboard users.</p>\n\n<p>Our accessibility work is undertaken in partnership with <a href=\"https://www.ukaaf.org/\">UKAAF</a> and <a href=\"https://www.rnib.org.uk/\">RNIB</a>, two leading accessibility organisations based in the UK.</p>\n\n<p>MuseScore is written in C++ / Qt, with some Javascript / QML. It is available for Windows, macOS and Linux under GPL version 2.</p>",
    "persons": [
      "Peter Jonas",
      "Marc Sabatella"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 20,
    "room": "H.2213",
    "title": "Gitflow Design",
    "subtitle": "A git workflow for design in open-source projects",
    "track": "Open Source Design",
    "abstract": "<p>Gitflow design as mentioned in the description is a git workflow for designers and design work. It's meant to be open, platform-agnostic and help minimise dependencies on proprietary software and help to increase collaboration.</p>\n\n<p>By using git, we get to take advantage of a lot of useful features available such as controlled access, review process, feedback system, version control files, preview changes in context with side-by-side diffs, detailed history of changes and more, something developers had for years, but that designers never really took advantage o</p>",
    "description": "<p>Gitflow Design exists as a way to mitigate issues commonly found in design workflows of open-source software projects where the work created never comes into git. Which means such work is not being tracked and doesn't have an auditable history. Files also might not be stored in a common place accessible to everyone, so its always a hit and miss on how to gain access to them, and sometimes due to these being created with proprietary software they can sit behind a closed gateway and as contributors come and go from the project their access can be lost.</p>\n\n<p>By using git we make things easier and open for anyone wanting to collaborate and hopefully streamlining the work process by connecting the development and design repositories together. For this to work a success we need to adopt a design workflow that focuses on open-source ideals, so that no one is restricted by proprietary software and gatekeepers. We introduce such workflow bellow which we have been testing, It's called Gitflow Design.</p>",
    "persons": [
      "Diogo Sergio"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 20,
    "room": "H.2213",
    "title": "UXBOX, the time for an open source online prototyping platform has arrived",
    "subtitle": "Vision and short demo",
    "track": "Open Source Design",
    "abstract": "<p>Hello World UXBOX! This will be our first public announcement of the coming of UXBOX, the open source prototyping online platform based on SVG. We will share our vision and the 2020 product roadmap, explaining the resources that are committed to them. We will perform a quick demo and hope to start a productive conversation with the Open Source Design Community.</p>",
    "description": "<p>After 2 years of painstakingly slow development, Kaleidos (UXBOX sponsor, also Taiga creator) has finally decided to devote full-time resources and relevant investment to once and for all create a modern open source UX/UI prototyping online platform. With open standards (SVG) as a core feature and value, we hope to contribute a much needed platform to the Open Source Design community, which we consider to be instrumental to make UXBOX the best online prototyping tool out there, period.\nThis short talk would share both the vision of the product, its current state and the 2020 roadmap. Moreover, we will want to nurture a healthy and welcoming community around the development and usage of UXBOX and we will give some hints on how we plan to do this so we can already take great feedback from FOSDEM attendees.\nUXBOX is being developed by open source enthusiasts coming from the tech and UX/UI trenches that have already built 20+ products for startups and launched Taiga in the past. Kaleidos, its umbrella company, has raised seed money and is devoting funds to make sure there is an amazing multidisciplinary team focused on UXBOX, also able to engage with a newly born community of users and contributors.</p>",
    "persons": [
      "Pablo Ruiz-Múzquiz"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 20,
    "room": "H.2213",
    "title": "Using biometric gadgets for express-tests in the UX/UI research",
    "subtitle": "",
    "track": "Open Source Design",
    "abstract": "<p>Estimating the user’s physical and mental state with a set of special measuring devices can be helpful in detecting bottlenecks of the human-computer interaction. Until recent years, evaluating cognitive and physical load by biometrical parameters (heart rate, galvanic skin response, brain waves, gaze direction, etc.) was too expensive to be widely adopted for FLOSS. However contemporary consumer-grade gadgets targeted at fitness and entertainment are much more affordable and precise enough to be used in the UX/UI comparison. Still, their different primary goal often complicates their usage for the  research.  The talk will highlight which devices are the most suitable ones for the research purposes in the open-source world (the ones having open-source and GNU/Linux frameworks to access biometric data).  Gadgets covered with the talk are fitness-trackers, EEG headsets, and eye-trackers. Patterns of getting data, problems with cyphering and licensing will be discussed, as well as brief biometry usage scenarios and examples of the UI express-testing.</p>",
    "description": "",
    "persons": [
      "Dmitriy Kostiuk"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 20,
    "room": "H.2213",
    "title": "Beyond the Pile of Knobs: Usability and Design for Privacy, Security, Safety & Consent",
    "subtitle": "Privacy and security shouldn't be a privilege or inaccessibly complex. We will share what we've learned working with projects that center security and privacy to support vulnerable populations.",
    "track": "Open Source Design",
    "abstract": "<p>Simply Secure will share examples of how we can design, centering the needs of the most vulnerable. We will present the problems, e.g. why the UX patterns that make consenting or refusing consent so difficult in practice and why open source security tools are often associated with bulky user interfaces and inaccessible jargon, and share findings from our 5 years of working with projects in the Internet Freedom, Digital Rights, Media Justice, Translation, Training, Civic Tech and Development communities.</p>",
    "description": "<p>Security and privacy are usually handed off to development teams as technical challenges, with the design and user experience as an after thought — meaning that as designers, we are building off of limited examples and a small research field. With security risks posing a real threat on the internet, design and usability are critical to building safer more trustworthy technology because users will work around poorly design experiences.</p>\n\n<p>In this talk, we will share examples of how we can design, centering the needs of the most vulnerable. We will present the problems, e.g. why the UX patterns that make consenting or refusing consent so difficult in practice and why open source security tools are often associated with bulky user interfaces and inaccessible jargon, and share findings from our 5 years of working with projects in the Internet Freedom, Digital Rights, Media Justice, Translation, Training, Civic Tech and Development communities.</p>\n\n<p>Simply Secure is a nonprofit that supports practitioners by putting people at the center of trustworthy technology. Launched in 2014, our work focuses on building technology that enhances and protects human dignity by centering the needs of vulnerable populations. We use a human-centered approach because we believe that the user experience of a device, program, or application plays a critical role in building trustworthy technology. At a minimum, responsible user experience (UX) offers timely, comprehensible, and actionable information to users — it gives them genuine agency in interacting with the system. Fundamentally our goal is to support practitioners in developing the skills needed to work on the wicked problems presented by technology today.</p>",
    "persons": [
      "Georgia Bullen"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 20,
    "room": "H.2213",
    "title": "Jumpstarting your business with Odoo",
    "subtitle": "",
    "track": "Open Source Design",
    "abstract": "<p>In this talk I will describe the process of discovering the wonders of Odoo when I got the project to write a book about it. My writing tools are Vim, Git, Asciidoctor-pdf and Inkscape. I will describe both the technical as the organisational challenges during the writing process.</p>",
    "description": "",
    "persons": [
      "Jeroen Baten"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 20,
    "room": "H.2213",
    "title": "File sharing & storage for human rights organizations",
    "subtitle": "A design research case study",
    "track": "Open Source Design",
    "abstract": "<p>Least Authority is presenting a design research project that looks at open source file storage and sharing solutions for human rights organizations. We will present the project, the first phase of our research process, and outlook on next steps that involve adapting our open source tools.</p>",
    "description": "<p>Least Authority's mission is to build and strengthen ethical and usable technology solutions that advance digital security and support privacy as a fundamental human right. We support open source projects, such as Tahoe-LAFS, Gridsync, Magic Folders and Magic Wormhole. This session presents a design research project, where we investigate file storing/file sharing needs of human rights organisations. As an outcome of this research, we are exploring how we can adapt our open source tools to best meet use cases and usability requirements of human rights organisations. This presentation will focus on the research process and the development of findings with the purpose of making open source tools available to meet human rights organizations’ needs. The project is funded by the Open Technology Fund.</p>",
    "persons": [
      "Allon Bar",
      "Abigail Garner"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 20,
    "room": "H.2213",
    "title": "Design contributions to OSS: Learnings from the Open Design project at Ushahidi",
    "subtitle": "Structuring in-person and remote workshops for open source design contributions.",
    "track": "Open Source Design",
    "abstract": "<p>Ushahidi builds OSS humanitarian tools, remotely for some of the most marginalized people across the globe. To tackle these systemic problems with how to ‘open source’ a design effort and bring the community along with the ‘on-staff’ Ushahidi designers, we’ve been piloting a series of design events on our OSS crisis communication tool TenFour with our partners Designit and Adobe. Together, we’re looking to solve the problems with how open source design can work by engaging through meaningful technology that makes a difference in the world.</p>\n\n<p>We’re here to take you through that journey and what we’ve learnt about design contributions to OSS.</p>",
    "description": "<p>In this session, we'll briefly cover the history of the project and the main problems we attempted to solve and we'll present the learning and adaptions to our workshop framework and methodology that aims to engage design teams and individuals that are not yet 'on-board' with OSS as an ethos or movement.</p>\n\n<p>Looking into some the abstract deeper motivations for design professionals to contribute but also some practical tips on structuring issues, labelling and maintaining design (and extended functions like research, UX and product management) you'll leave with a set of tools and methods you can apply to your OSS to engage with designers.</p>",
    "persons": [
      "Eriol Fox"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 20,
    "room": "H.2213",
    "title": "Designing to change it all",
    "subtitle": "Designing processes and designing some products on the way...",
    "track": "Open Source Design",
    "abstract": "<p>At the ‘SamenBeter’ project we firmly believe a good product is a product that improves how people work. So we start with designing the change we want before we even start thinking about a product. How that looks in practice? Come and see, but be prepared for a fast ride!</p>",
    "description": "<p>Is it possible to design a radical change in the healthcare system? Why is the first product we build for that an authorization standard? Why are we talking about the ‘personality of a system’? How come the people designing it ended up in community centres, playing a game about privacy with the visitors? What is that ‘developer journey’ we are talking about? why are we making such a big fuss about a license nobody seems to read?</p>\n\n<p>The ‘SamenBeter’ project has a moderate goal: to change healthcare, all of it. And because we are fuelled with design thinking, we are making deliberate design choices about everything. About what we want to change in healthcare, about the process of getting there, about the role of technology, about the values the technology should adhere, about the way we should develop the technology, about the way the technology will be adapted and about how the community should be developed.</p>\n\n<p>In this talk I will showcase the SamenBeter project as example of how design is about much more then user interfaces. It is about designing processes, user interactions and designing the adaptation of the product itself. And last but not least: why did we take the effort to design everything and not just a product? Yes, it is too much to fit in one talk, so fasten your seatbelts!</p>",
    "persons": [
      "Winfried Tilanus"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 20,
    "room": "H.2213",
    "title": "Pitch your project",
    "subtitle": "",
    "track": "Open Source Design",
    "abstract": "<p>If you contribute to a free / open source project in need of design contributions, come and pitch it to the designers in the room.</p>",
    "description": "<p>In this session, FOSS projects as given time to present and ask for contributions to the designers in the room. Each project is given 3 minutes to present. In those 3 minutes, they should briefly introduce the project, explain what design help they need, and provide contact details so designers can reach them after FOSDEM.</p>\n\n<p>The Open Source Design collective will use the information to submit a \"job\" for each project to the Open Source Design \"jobs board\" (https://opensourcedesign.net/jobs/), so that the request for design help reaches not just the designers in the room, but also the wider design community.</p>",
    "persons": [
      "Bernard Tyers",
      "Amit Nambiar"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 50,
    "room": "H.2214",
    "title": "Fibonacci Spirals and 21 Ways to Contribute to Postgres—Beyond Code",
    "subtitle": "",
    "track": "PostgreSQL",
    "abstract": "<p>Postgres is growing like gangbusters: in popularity, in adoption, and in the size of the ecosystem. And over 400 developers contribute code to Postgres today: their expertise, design chops, and skill are big factors in the increasing popularity of Postgres. But what if you’re not a developer? Are there things you can do to help grow the usage and popularity of Postgres? And are these non-code ways to contribute to Postgres important? Valued? Will they make a real difference?</p>\n\n<p>If you love Postgres and want to help drive Fibonacci growth of the Postgres community, this talk is for you. I’ll walk through 21 different (and important) ways to contribute to Postgres—along with tips and resources for getting started.</p>",
    "description": "",
    "persons": [
      "Claire Giordano"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 50,
    "room": "H.2214",
    "title": "Find your slow queries, and fix them!",
    "subtitle": "",
    "track": "PostgreSQL",
    "abstract": "<p>Where, oh where, is all that time going? What in the world is that database thing doing?! This talk will help you understand what's happening (and why) and how to analyze poor query performance. We'll also go over steps and strategies to take to improve them and get the performance (and scalability!) you need.</p>\n\n<p>It all starts with figuring out what queries are slow, we'll do that by going into the various PostgreSQL configuration options for logging queries and a few helpful modules for getting even more information about ongoing queries. Next we'll go over EXPLAIN and EXPLAIN ANALYZE output for select queries, what the EXPLAIN output means in terms of how the query is being executed. Lastly (this is the good part- you have to stay til the end to get it!) we'll go over ways to improve the queries, including index creation, rewriting the query to allow PG to use a different plan, and how to tune parameters for specific queries.</p>",
    "description": "",
    "persons": [
      "Stephen Frost"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 50,
    "room": "H.2214",
    "title": "A Deep Dive into PostgreSQL Indexing",
    "subtitle": "",
    "track": "PostgreSQL",
    "abstract": "<p>Indexes are a basic feature of relational databases, and PostgreSQL offers a rich collection of options to developers and designers. To take advantage of these fully, users need to understand the basic concept of indexes, to be able to compare the different index types and how they apply to different application scenarios. Only then can you make an informed decision about your database index strategy and design. One thing is for sure: not all indexes are appropriate for all circumstances, and using a ‘wrong’ index can have the opposite effect that you intend and problems might only surface once in production. Armed with more advanced knowledge, you can avoid this worst-case scenario! We’ll take a look at how to use pg<em>stat</em>statment to find opportunities for adding indexes to your database. We’ll take a look at when to add an index, and when adding an index is unlikely to result in a good solution. So should you add an index to every column? Come and discover why this strategy is rarely recommended as we take a deep dive into PostgreSQL indexing.</p>",
    "description": "",
    "persons": [
      "Ibrar Ahmed"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 50,
    "room": "H.2214",
    "title": "PostgreSQL on K8S at Zalando: Two years in production",
    "subtitle": "",
    "track": "PostgreSQL",
    "abstract": "<p>Many DBAs avoid any kind of cloud offering and prefer to run their databases on dedicated hardware. At the same time companies demand to run Postgres at scale, efficiently, automated and well integrated into the infrastructure landscape. The arrival of Kubernetes provided good building blocks and an API to interact with and with it solve many problems at the infrastructure level.</p>\n\n<p>The database team at Zalando started running highly-available PostgreSQL clusters on Kubernetes more than two years ago. In this talk I am going to share how we automate all routine operations, providing developers with easy-to-use tools to create, manage and monitor their databases, avoiding commercial solutions lock-in and saving costs, show open-source tools we have built to deploy and manage PostgreSQL cluster on Kubernetes by writing short manifests describing a few essential properties of the result.</p>\n\n<p>Operating a few hundred PostgreSQL clusters in a containerized environment has also generated observations and learnings which we want to share: infrastructure problems (AWS), how engineers use our Postgres setup and what happens when the load becomes critical.</p>",
    "description": "",
    "persons": [
      "Alexander Kukushkin"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 50,
    "room": "H.2214",
    "title": "An ultimate guide to upgrading your PostgreSQL installation",
    "subtitle": "",
    "track": "PostgreSQL",
    "abstract": "<p>Even an experienced PostgreSQL DBA can not always say that upgrading between major versions of Postgres is an easy task, especially if there are some special requirements, such as downtime limitations or if something goes wrong. For less experienced DBAs anything more complex than dump/restore can be frustrating.</p>\n\n<p>In this talk I will describe why we need a special procedure to upgrade between major versions, how that can be achieved and what sort of problems can occur. I will review all possible ways to upgrade your cluster from classical pg_upgrade to old-school slony or modern methods like logical replication. For all approaches, I will give a brief explanation how it works (limited by the scope of this talk of course), examples how to perform upgrade and some advice on potentially problematic steps. Besides I will touch upon such topics as integration of upgrade tools and procedures with other software — connection brokers, operating system package managers, automation tools, etc. This talk would not be complete if I do not cover cases when something goes wrong and how to deal with such cases.</p>",
    "description": "",
    "persons": [
      "Ilya Kosmodemiansky"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 50,
    "room": "H.2214",
    "title": "The State of (Full) Text Search in PostgreSQL 12",
    "subtitle": "",
    "track": "PostgreSQL",
    "abstract": "<p>How to navigate the rich but confusing field of (Full) Text Search in PostgreSQL. A short introduction will explain the concepts involved, followed by a discussion of functions, operators, indexes and collation support in Postgres in relevance to searching for text. Examples of usage will be provided, along with some stats demonstrating the differences.</p>",
    "description": "",
    "persons": [
      "Jimmy Angelakos"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 50,
    "room": "H.2214",
    "title": "RTFM",
    "subtitle": "",
    "track": "PostgreSQL",
    "abstract": "<p>Reading the manual before starting a new work is always a good practice.</p>\n\n<p>However some situations like pressure for delivery or lack of attention may lead to wrong assumptions that cause unpredictable results or even disasters.</p>\n\n<p>The talk, in a semi serious way, will walk the audience through some of corner cases caused by the lack of the good practice of RTFM.</p>",
    "description": "",
    "persons": [
      "Federico Campoli"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 90,
    "room": "H.3242",
    "title": "NGI Meetup",
    "subtitle": "Meetup for Next Generation Internet",
    "track": "BOFs (Track B - in H.3242)",
    "abstract": "<p>The Next Generation Internet initiative is one of the most substantial efforts in recent years to move the state of technology forward. It consists currently of over 200 projects, ranging from open hardware, middleware, web services, ActivityPub and cryptography to more fair search technology and decentralised internet tools. More projects are being added through open calls regularly. There are over twenty different talks related to this programme at FOSDEM 2020! This Birds of a Feather is for anyone interested in or involved with the Next Generation Internet initiative.</p>",
    "description": "",
    "persons": [
      "Michiel Leenaars"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 60,
    "room": "J.1.106",
    "title": "Weblate meetup",
    "subtitle": "Weblate users meetup",
    "track": "BOFs (Track A - in J.1.106)",
    "abstract": "<p>Weblate future, features, bugs, collaboration between users and other related topics.</p>",
    "description": "",
    "persons": [
      "Michal Čihař"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 30,
    "room": "AW1.120",
    "title": "Free Software Radio Devroom Introduction and Hackfest Review",
    "subtitle": "",
    "track": "Free Software Radio",
    "abstract": "<p>Greetings and plans for the day and future</p>",
    "description": "",
    "persons": [
      "Philip Balister"
    ]
  },
  {
    "start": 1577871000000,
    "duration": 30,
    "room": "AW1.120",
    "title": "Modernizing Distribution of SDR Tools and Libraries with Conan",
    "subtitle": "What does cmake have to do with SNR?",
    "track": "Free Software Radio",
    "abstract": "<p>There are so many great open source libraries and tools that people have written that make up the software defined radio ecosystem, but we have unfortunately created a high bar for consumption of this software, and an even higher bar for using modern versions. In this presentation we look at how we can use modern C/C++ package management with Conan to simplify the lives of our users who want to use the latest versions without living in dependency hell.</p>",
    "description": "<p>Users of SDR software have spent far too much time staring at cmake builds and wondering what how to get the missing build time dependency.  The package feeds from major distributions contain a lot of the popular software, but they tend to lag development significantly and lock users into specific versions.  If you want to run the latest code you are back to compiling it.  It does not have to be this way.  Using Conan these applications and their dependencies can be managed in a modern way where users can pull binary packages when available and easily rebuild packages when needed all in a sandboxed environment.</p>\n\n<p>This talk cover the following points:\n* What is Modern C++ Package Management\n* How this can simplify consumption from a SDR user perspective (I just want to play with my hardware...)\n* Demo of using this workflow with some packaged SDR applications\n* Example of how this packaging works for a library\n* What we can do better as developers to simplify this task</p>",
    "persons": [
      "Brennan Ashton"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 30,
    "room": "AW1.120",
    "title": "Block Header Parsing Tool",
    "subtitle": "Implement a block parsing library",
    "track": "Free Software Radio",
    "abstract": "<p>GNU Radio's value comes from many things, but one of them is a vast library of blocks, both in-tree and out-of-tree. Right now, it is hard to use these blocks in a different context than GNU Radio itself or interface with blocks automatically or programmatically.</p>",
    "description": "<p>A python-based tool that can interact with GNU Radio block headers written in C++, to automatically parse them and extract information about them such as which getters/setters they have, their I/O signatures, factory signatures, etc. GRModtool can be extended with the parsing tool as one of its utilities. The parsed information can be further used for creating YAML files for the GRC.</p>",
    "persons": [
      "Arpit Gupta"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 30,
    "room": "AW1.120",
    "title": "How to evolve the GNU Radio scheduler",
    "subtitle": "Embracing and breaking legacy",
    "track": "Free Software Radio",
    "abstract": "<p>GNU Radio is the widest used software radio stack for research and development on PC-style hardware, having enabled hundreds of high-rate applications. I'll discuss where its limits are, where we need to stick to GNU Radio's recipe for SDR success, and where to disruptively address its architectural shortcomings</p>",
    "description": "<p>Today's GNU Radio hits hard limits when it comes to a few things that\nare absolutely crucial for modern communication stacks: It doesn't make any\nguarantees on latency, and its architecture doesn't allow for tight integration\nwith hardware accelerators. And whilst most communications are packet-based,\npacketed data is a second-class citizen in the kingdom of sample streams that is\nGNU Radio.</p>\n\n<p>In this talk, we'll discuss why that is the case, and what can be remedied\nwithin the current framework, and what not. We'll try to assess what usage\nparadigms are worth keeping for the future of GNU Radio, and what needs to\nchange.</p>",
    "persons": [
      "Marcus Müller"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 30,
    "room": "AW1.120",
    "title": "A Rose by Any Other Name Would Run Just as Long",
    "subtitle": "Understanding Computational and Hardware Complexity in Software Defined Radio Framework",
    "track": "Free Software Radio",
    "abstract": "<p>Abstract: Radio based communication systems and imagers operate under real-time constraints. Off-loading computes to an FPGA seems like a solution to speeding-up your application but comes with many pitfalls.  Specifically, software-oriented implementations fail to achieve the required interface bandwidths or computational throughput required to see a speed-up.  In this talk, we will discuss the organization of common compute motif's in software-defined-radio and their complexity in time and resources for FPGAs.</p>",
    "description": "<p>Rough goals of talk:\n1) Communicate why FPGA acceleration would be attractive\n2) Discuss common pitfalls\n2a) A behaviorally oriented accelerator\n2b) Starving the beast, failing to provide the required data bandwidths\n2c) Processor oriented runtime, creates execution overheads\n3) Thinking about accelerators, what do they look like\n3a) FFT\n3b) Correlators\n3c) Matrix-Vector Multiply\n4) Building an off-load model</p>",
    "persons": [
      "John Brunhaver"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 30,
    "room": "AW1.120",
    "title": "gr-satellites latests developments",
    "subtitle": "",
    "track": "Free Software Radio",
    "abstract": "<p>gr-satellites is a GNU Radio out-of-tree module with the goal of decoding every Amateur satellite. Currently it supports more than 80 different satellites. After GNU Radio 3.8 was released last summer, gr-satellites is seeing a lot of development and important changes. A refactored version, which will be released as gr-satellites 3.0 is on the works. This version brings more modularity to avoid code duplication, more flexibility in the input and output that the user can employ, and the idea to improve its integrability with other tools. Satellites are defined using a YAML file and the GNU Radio flowgraph is constructed on the fly by a Python script by connecting so called \"component\" blocks. Advanced users can also use the components directly in their own flowgraphs. This talk gives an overview of the gr-satellites 3.0 development progress.</p>",
    "description": "",
    "persons": [
      "Daniel Estévez"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 30,
    "room": "AW1.120",
    "title": "r2cloud - Decode satellite signals on Raspberry PI",
    "subtitle": "",
    "track": "Free Software Radio",
    "abstract": "<ol>\n<li>Java for digital signal processing\n\n<ul>\n<li>why java?</li>\n<li>how to do digital signal processing in Java. Some examples</li>\n<li>decoding LRPT (with images), BPSK (with real data)</li>\n</ul>\n</li>\n<li> Working base station network\n\n<ul>\n<li>how it differs from satnogs</li>\n<li>testing, code coverage. Enterprise approach for building communication software</li>\n</ul>\n</li>\n<li> Plans. Q&amp;A</li>\n</ol>",
    "description": "",
    "persons": [
      "Andrey Rodionov"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 30,
    "room": "AW1.120",
    "title": "Platform independent CPU/FPGA co-design: the OscImp-Digital framework",
    "subtitle": "G. Goavec-Merou, P.-Y. Bourgeois, J.-M. Friedt",
    "track": "Free Software Radio",
    "abstract": "<p>Combining the flexibility of FPGA hardware configuration with the high abstraction level of an operating system running on a general purpose central processing unit (CPU) requires mastering a broad range of knowledge, from low level hardware configuration to kernel drivers to libraries and userspace application. While some vendor specific frameworks tackle the challenge, we focus on a vendor independent solution applicable to current FPGA Systen on Chip providers: the OscImp Digital framework provides a comprehensive set of FPGA IP, associated Linux driver, library and userspace examples based on GNU Radio running on the embedded CPU. We demonstrate its use on the Redpitaya platform processing baseband signals as well as the Zynq, most significantly associated with the AD9363 radiofrequency frontend on the PlutoSDR board. In both cases, the FPGA is not only used to stream I/Q coefficients but pre-process the datastream in order to reduce bandwidth and efficiently feed the CPU: we demonstrate embedded FM broadcast radio reception as well as GPS decoding on the PlutoSDR custom bitstream. The framework is available at https://github.com/oscimp/oscimpDigital</p>",
    "description": "",
    "persons": [
      "Jean-Michel Friedt"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 30,
    "room": "AW1.120",
    "title": "Striving for Performance Portability of Software Radio Software in the Era of Heterogeneous SoCs",
    "subtitle": "",
    "track": "Free Software Radio",
    "abstract": "<p>Future heterogeneous DSSoCs will be extraordinarily complex in terms of processors, memory hierarchies, and interconnection networks. To manage this complexity, architects, system software designers, and application developers need design and programming technologies to be flexible, accurate, efficient, and productive. Recently, our team has started to explore the mapping of GnuRadio to various heterogeneous SoCs in order to understand how programming technologies can support this goal of making this SDR framework performance portable. Using our software stack, we are porting several SDR applications to GPUs from NVIDIA, AMD, and ARM, and to NVIDIA Xavier SoCs, Qualcomm Snapdragon, and Xilinx Zynq devices. Our current approach uses a directive-based programming model and a new intelligent runtime scheduler to port and execute the workflows. We are evaluating several open programming models to enable performance portability; initially, they include directive-based compilers, OpenCL, and SYCL. All of these approaches will generate tasks that are then queued and scheduled by our open-source intelligent runtime scheduler, which is a critical component of our approach. Initial performance results appear promising; however, more automation will further broad deployment. Also, we have developed a host of tools to examine and profile SDR workflows and modules. Specifically, these analysis tools enable automated characterization of the behavioral and computational features of GNU Radio blocks and workflows. The static tools in GR-tools help developers to create ontologies and queries to classify GR modules based on custom scenarios. The dynamic toolset provides automated profiling capabilities of GR workflows and presents detailed statistics on how components in a given software defined radio application perform. GR-tools also produces a graph-based representation of the analyzed data and provides powerful visualization options to filter and display the information obtained from the static and dynamic tools. Our software is available as open-source software and will be made available to the community.</p>",
    "description": "",
    "persons": [
      "Jeffrey Vetter"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 30,
    "room": "AW1.120",
    "title": "Cooperative Perception in Future Cars using GNU Radio",
    "subtitle": "",
    "track": "Free Software Radio",
    "abstract": "<p>Title: Cooperative Perception in Future Cars using GNU Radio</p>\n\n<p>Speaker: Augusto Vega, IBM Research (NY, USA)</p>\n\n<p>Abstract:\nThe phenomenon of self-driving (autonomous) vehicles is a symbol of the grand re-emergence of artificial intelligence and robotics as a promising technology. The most general model of future vehicular transportation is that of artificially intelligent, connected, autonomous vehicles (CAVs) [1].</p>\n\n<p>In this talk, we present a representative open-source application for CAVs operating as a collaborative swarm and communicating via GNU Radio. The application, called ERA [2], incorporates local sensing, creation of occupancy grid maps, vehicle-to-vehicle (V2V) communication of grid maps between neighboring vehicles using GNU Radio-based dedicated short-range communication (DSRC), and map fusion to create a joint higher-accuracy grid map [3]. Specifically, each vehicle in ERA uses its onboard sensors to generate local occupancy grid maps, which it communicates to other nearby vehicles using DSRC. When a vehicle receives occupancy maps from nearby cars, it merges the received ones with the locally-generated occupancy maps, expanding the scope and increasing the accuracy of this vehicle's perception. The DSRC transceiver adopted in ERA is an open-source GNU Radio implementation of the IEEE 802.11p standard by Bastian Bloessl [4]; while perception and map creation is implemented using ROS (Robot Operating System) [5]. We created an appropriate software interface between GNU Radio and ROS which enables proper execution and interaction of both frameworks.</p>\n\n<p>In addition to presenting a deep dive into ERA's code, we will also show performance analysis results of ERA (including its GNU Radio components) and discuss potential acceleration opportunities for performance and efficiency improvement -- including optimizations of Viterbi decoding and complex exponential through hardware acceleration. We believe that ERA can help to fill the gap between the fast-growing CAV R&amp;D domain and GNU Radio, specifically when it comes to the wireless communication aspect of future vehicles.</p>\n\n<p>[1] A. Vega, A. Buyuktosunoglu, P. Bose, “Towards \"Smarter\" Vehicles Through Cloud-Backed Swarm Cognition,” Intelligent Vehicles Symposium 2018: 1079-1086.</p>\n\n<p>[2] ERA. URL: https://github.com/IBM/era</p>\n\n<p>[3] E. Sisbot, A. Vega, A. Paidimarri, J. Wellman, A. Buyuktosunoglu, P. Bose, D. Trilla, “Multi-Vehicle Map Fusion using GNU Radio,” Proceedings of The GNU Radio Conference 2019, 4(1).</p>\n\n<p>[4] B. Bloessl, “IEEE 802.11 a/g/p transceiver for GNU radio,” URL: https://github.com/bastibl/gr-ieee802-11</p>\n\n<p>[5] ROS. URL: https://www.ros.org</p>\n\n<p>Desired slot time: 30 mins</p>",
    "description": "",
    "persons": [
      "Augusto Vega"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 30,
    "room": "AW1.120",
    "title": "srsLTE project update",
    "subtitle": "",
    "track": "Free Software Radio",
    "abstract": "",
    "description": "",
    "persons": [
      "Andre Puschmann"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 30,
    "room": "AW1.120",
    "title": "Channel Equalization using GNU Radio",
    "subtitle": "compensating for impairments in the wireless channel, and extensions to existing GNU Radio functionality",
    "track": "Free Software Radio",
    "abstract": "<p>We examine the use of equalizers in wireless communication systems, how these are implemented in GNU Radio, and how the existing GR equalizer functionality can be extended with a new OOT using training-based adaptation.  The theory of multipath channels, ISI, and how to overcome with adaptive equalization will be reviewed and shown with interactive flowgraphs.</p>",
    "description": "",
    "persons": [
      "Josh Morman"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 30,
    "room": "AW1.120",
    "title": "Task Scheduling of Software-Defined Radio Kernels in Heterogeneous Chips: Opportunities and Challenges",
    "subtitle": "",
    "track": "Free Software Radio",
    "abstract": "<p>Title: Task Scheduling of Software-Defined Radio Kernels in Heterogeneous Chips: Opportunities and Challenges</p>\n\n<p>Speaker: Augusto Vega, IBM Research (NY, USA)</p>\n\n<p>Abstract:\nThe proliferation of 'heterogeneous' chip multiprocessors in recent years has reached unprecedented levels, especially in the context of IoT and distributed edge computing (e.g. connected and autonomous vehicles). By combining the right set of hardware resources (cores, accelerators, chip interconnects and memory technology) along with an adequate software stack (operating system and programming interface), heterogeneous chips have become an effective high-performance and low-power computing alternative.</p>\n\n<p>However, heterogeneous architectures come with new challenges. Fundamentally, the complexity derived from the design's heterogeneous nature challenges the effective scheduling of tasks (processes), a scenario that becomes even more critical when real-time execution deadlines must be met. This is particularly important in the context of GNU Radio, given that its underlying scheduler is completely unaware of chip heterogeneity today. Early stage prototyping and evaluation of GNU Radio scheduling policies in heterogeneous platforms becomes a valuable asset in the design process of a future GNU Radio scheduler.</p>\n\n<p>In this talk, we present a new open-source simulator for fast prototyping of task scheduling policies, called STOMP (Scheduling Techniques Optimization in heterogeneous Multi-Processors) [1]. It is written in Python and implemented as a queue-based discrete-event simulator with a convenient interface that allows users and researchers to \"plug in\" new scheduling policies in a simple manner. We also present a systematic approach to task scheduling in heterogeneous platforms through the evaluation of a set of progressively more \"intelligent\" scheduling policies using STOMP. We rely on synthetic kernels representative of a GNU Radio application [2], including functions like Viterbi decoding and fast Fourier transform (FFT) that have to be scheduled across general-purpose cores, GPUs or hardware accelerators to meet the application's real-time deadlines. We will show results indicating that relatively simple scheduling policies can satisfy real-time requirements when they are properly designed to take advantage of the heterogeneous nature of the underlying chip multiprocessor.</p>\n\n<p>[1] STOMP. URL: https://github.com/IBM/stomp\n[2] ERA. URL: https://github.com/IBM/era</p>\n\n<p>Desired slot time: 30 mins</p>",
    "description": "",
    "persons": [
      "Augusto Vega"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 30,
    "room": "AW1.120",
    "title": "SDR4IoT - Using SDR for IoT Device Fingerprinting and Localization",
    "subtitle": "A project part of the FED4FIRE+ Open Calls",
    "track": "Free Software Radio",
    "abstract": "<p>This talk will present the result of our experimentation done at i.Lab Wireless Testbed in Ghent, in the context of <a href=\"https://www.fed4fire.eu/\">FED4Fire+ H2020 project</a>. Our project aims to collect raw radio frequency (RF) signals of widely used radio protocols for Internet of Things (IoT) devices in the 2.4GHz ISM bandwidth, such as Bluetooth Low Energy and LoRa, using software-defined radio (SDR).\nThis will allow us collecting a large, reliable and reproducible dataset of RF fingerprint. This dataset will be further used to develop deep learning algorithms for IoT device fingerprinting and localization. Our use case is the authentication of autonomous vehicles or robots in a building according to their localization, without any over-the-air key exchange algorithm.</p>",
    "description": "",
    "persons": [
      "Alexis DUQUE"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 30,
    "room": "AW1.120",
    "title": "openwifi",
    "subtitle": "Opensource \"Wi-Fi chip design\" and Linux driver",
    "track": "Free Software Radio",
    "abstract": "<p>An open source \"Wi-Fi chip design\"(Will be AGPLv3) will be presented and a live demo will be shown in the room! The design is based on SDR (Software Defined Radio) and offers full-stack 802.11a/g/n capabilities on FPGA and ARM Linux (Xilinx Zynq SoC + AD9361 RF front-end).It conforms with Linux mac80211 framework and behaves just like COTS Wi-Fi chip under Linux. The main components of the design are: RF front-end control; PHY; low-MAC; interfacing (DMA, register) with ARM; mac80211 compliant Linux driver; high-MAC (mac80211 framework); Linux user space tools (ifconfig, iwconfig, dedicated tools via netlink).​ Since it is a SDR based \"white box\" design instead of commercial “black box” chip, you can do Wi-Fi research and customization without any reverse engineering efforts.</p>\n\n<p>Why does it fit FOSDEM?</p>\n\n<p>It will be the 1st open source project for full-stack Wi-Fi SDR implementation. Lots of people, especially wireless network/security researchers, SDR researchers and hackers, will be interested in. We are eager to show the demo in the room and hear feedback from people/community. Potential contributors are also very welcomed, and we will be glad to offer help.</p>",
    "description": "<p>Overall architecture: Please find attachment.</p>\n\n<p>Features:</p>\n\n<ul>\n<li>802.11a/g; 802.11n MCS 0~7; 20MHz</li>\n<li>Mode: Ad-hoc; Station; AP</li>\n<li>DCF (CSMA/CA) low MAC layer in FPGA</li>\n<li>Configurable channel access priority parameters:</li>\n<li>duration of RTS/CTS, CTS-to-self</li>\n<li>SIFS/DIFS/xIFS/slot-time/CW/etc</li>\n<li>Time slicing based on MAC address</li>\n<li>Easy to change bandwidth and frequency:</li>\n<li>2MHz for 802.11ah in sub-GHz</li>\n<li>10MHz for 802.11p/vehicle in 5.9GHz</li>\n<li>On roadmap: 802.11ax</li>\n</ul>\n\n\n<p>Supported SDR platforms:</p>\n\n<ul>\n<li>zc706 (Xilinx) + fmcomms2 (Analog Devices)</li>\n<li>On roadmap: ADRV9361-Z7035/ADRV9364-Z7020 + ADRV1CRR-BOB (Analog Devices)</li>\n<li>On roadmap: zcu102 (Xilinx) + fmcomms2/ADRV9371 (Analog Devices)</li>\n</ul>",
    "persons": [
      "Xianjun Jiao"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 30,
    "room": "AW1.120",
    "title": "Software Defined Radio based scientific instrumentation",
    "subtitle": "using SDR frontends and oscilloscopes for fast measurements",
    "track": "Free Software Radio",
    "abstract": "<p>Software Defined Radio is best known for receiving and processing radiofrequency signals transmitted over the ether. However, many scientific experiments benefit from the flexibility, stability and reconfigurability of digital signal processing even when handling radiofrequency signals. In this presentation, we address two demonstrations of this concept. First, readily available SDR hardware is used to replace general purpose laboratory instruments (spectrum analyzer, lock in amplifier)\nfor characterizing radiofrequency processing acoustic transducers (filters, resonators). The benefit of SDR lies in communication bandwidth: while general purpose\ninstrument communication protocols (GPIB, VXI11 over Ethernet) require hundreds of milliseconds or seconds to transfer data, SDR platforms stream at high bandwidth I/Q coefficients collected on the fly on a ZeroMQ socket by the (GNU/Octave) processing software. We demonstrate a 10000 fold bandwidth gain when converting a general purpose instrument experiment to a SDR approach. Another approach is to address high bandwidth radiofrequency oscilloscopes as radiofrequency source for time of flight measurement. The gr-oscilloscope GNU Radio source demonstrates how to communicate between GNU Radio and laboratory grade equipment, here oscilloscopes, for processing discontinuous data streams using GNU Radio.</p>",
    "description": "",
    "persons": [
      "Jean-Michel Friedt"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 5,
    "room": "AW1.121",
    "title": "Welcome to the BSD devroom",
    "subtitle": "",
    "track": "BSD",
    "abstract": "<p>Welcome to the BSD devroom</p>",
    "description": "",
    "persons": [
      "Rodrigo Osorio"
    ]
  },
  {
    "start": 1577869800000,
    "duration": 45,
    "room": "AW1.121",
    "title": "Orchestrating jails with nomad and pot",
    "subtitle": "A container-based cloud computing platform for FreeBSD",
    "track": "BSD",
    "abstract": "<p>Docker and Kubernetes are changing the way to deploy services and applications in the Linux world.\nWhat about FreeBSD?\n2 years ago we presented pot, another jail abstraction framework. In time, the pot framework has developed to provide features containers-alike.\nThe plugin interface provided by nomad (a container orchestrator), allowed us to develop a driver for pot, enabling nomad to orchestrate pot jails.\nIn this talk, we'd like to present this FreeBSD-based ambitious alternative to Docker-Kubernetes</p>",
    "description": "",
    "persons": [
      "Luca Pizzamiglio"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 60,
    "room": "AW1.121",
    "title": "OpenSMTPD over the clouds",
    "subtitle": "the story of an HA setup",
    "track": "BSD",
    "abstract": "<p>OpenSMTPD has gained filters support in its latest version and it is a now an smtp server that can compete with other better known mail servers and can be used to handle both incoming and outgoing mail flows in a secure way.\nIts simple configuration and its \"secure by design\" approach makes it one of the best candidates for a mail server software.</p>",
    "description": "<p>Now that OpenSMTPD has gained filter support it can be used as a mail server with all features of other email servers.\nThis is the story of how OpenSMTPD can be setup in an high availability environment and how other OpenBSD tools can be used to achieve the goal.</p>",
    "persons": [
      "Giovanni Bechis"
    ]
  },
  {
    "start": 1577876700000,
    "duration": 30,
    "room": "AW1.121",
    "title": "NetBSD - Not Just For Toasters",
    "subtitle": "",
    "track": "BSD",
    "abstract": "<p>NetBSD may seem like an exotic choice for an operating system. But it is actually a decent desktop OS for developers and a rock-solid server OS, in the cloud as well as on old hardware. Come for the freedom from systemd, stay for the great packages, modern features and enthusiastic community!</p>",
    "description": "<p>In this talk, I will give reasons why adopting NetBSD makes sense, show some cool hardware that you can run NetBSD on, and talk about new features in the upcoming NetBSD 9 release.</p>",
    "persons": [
      "Benny Siegert"
    ]
  },
  {
    "start": 1577878800000,
    "duration": 40,
    "room": "AW1.121",
    "title": "FreeBSD Around the World!",
    "subtitle": "",
    "track": "BSD",
    "abstract": "<p>The FreeBSD Foundation has been supporting the FreeBSD Project and community for 20 years! In this talk, I'm going to share a little history of FreeBSD and the Foundation, how the Project works, and why you should get involved. I'll continue to share how the Foundation has been advocating for FreeBSD around the world, and what you can do to help bring on new users and contributors. Finally, I'll cover significant projects we are supporting to keep FreeBSD relevant, stable, and secure.</p>",
    "description": "",
    "persons": [
      "Deb Goodkin"
    ]
  },
  {
    "start": 1577881500000,
    "duration": 40,
    "room": "AW1.121",
    "title": "FreeBSD and LLVM support",
    "subtitle": "What is LLVM all about and how it integrates FreeBSD system",
    "track": "BSD",
    "abstract": "<p>We will explain what is LLVM project all about and how central it is under the FreeBSD operating system, as it serves to basically build it ; first we ll go through its major components and what is supported..</p>",
    "description": "",
    "persons": [
      "David Carlier"
    ]
  },
  {
    "start": 1577884200000,
    "duration": 60,
    "room": "AW1.121",
    "title": "Break your BSD kernel",
    "subtitle": "Fuzzing BSD kernel",
    "track": "BSD",
    "abstract": "<p>Fuzzing is an efficient technique to find bugs and vulnerabilities in the software.\nTodays BSD based operating systems allows using such techniques to test the kernel code easily.\nThis talk is designated to be a starting point for everyone who would like to start his journey with fuzzing his BSD kernel as well provide all necessary information needed.</p>",
    "description": "<p>The kernel is a central part of most of the modern operating systems. This place where hardware meets software controls main subsystems like Networking Stack (and other communication stacks), File Systems, Security and many other.\nDue to this fact security of overall system relay on the safety of the kernel.\nOne of the well-proven techniques to test software security is fuzzing.\nFor the last couple of years, researchers found a long list of vulnerabilities in many popular Open Source projects thanks to the efficiency of this technique.\nKernel fuzzing was always more complicated than userspace programs. Nevertheless, that is constantly improving and today's entry barrier is much lower than it used to be, thanks to the improvement made in recent years.\nFor the last couple of years, NetBSD became strong with new security features in the BSD world, as Sanitizers or Fuzzers.\nDue to the work of the community, it grew to an attractive target for people interested in operating systems and security.\nFuzzing can be also a very beneficial technique for kernel and drivers developers who want to improve or test the security of their code.</p>",
    "persons": [
      "Maciej Grochowski"
    ]
  },
  {
    "start": 1577888100000,
    "duration": 15,
    "room": "AW1.121",
    "title": "KDE on FreeBSD",
    "subtitle": "",
    "track": "BSD",
    "abstract": "<p>The state of KDE (the Plasma desktop and applications) on FreeBSD, what works, what needs better support lower in the stack. How do we get rid of HAL?</p>",
    "description": "",
    "persons": [
      "Adriaan de Groot"
    ]
  },
  {
    "start": 1577889300000,
    "duration": 45,
    "room": "AW1.121",
    "title": "NetBSD audio - a userland perspective",
    "subtitle": "Discussing the usage of NetBSD's audio API in third party software",
    "track": "BSD",
    "abstract": "<p>NetBSD has a rather interesting and unique native audio API, distinct from OSS, inherited from an early version of the Solaris API with extensions and improvements made over the years.</p>\n\n<p>In this talk, Nia describes the advantages of using NetBSD's native audio API in comparison to other alternatives, and discusses her improvements to third-party software to encourage usage and adoption of the API.</p>",
    "description": "",
    "persons": [
      "Nia Alarie"
    ]
  },
  {
    "start": 1577892300000,
    "duration": 30,
    "room": "AW1.121",
    "title": "X11 and Wayland: A tale of two implementations",
    "subtitle": "Implementing the hikari window manager/compositor",
    "track": "BSD",
    "abstract": "<p>In this talk I will outline my journey implementing my X11 window manager\n<code>hikari</code> and the corresponding Wayland compositor shortly after. <code>hikari</code> is a\nstacking window manager/compositor with some tiling capabilities. It is still\nmore or less work in progress and currently targets FreeBSD only but will be\nported to Linux and other operating systems supporting Wayland once it has\nreached some degree of stability and feature completeness.</p>",
    "description": "<p>This talk covers:</p>\n\n<ul>\n<li>a brief explanation regarding differences between X and Wayland</li>\n<li>some of <code>hikari</code>'s design goals and motivation</li>\n<li>choice of programming language</li>\n<li>an overview of libraries that were used</li>\n<li>tools for ensuring code quality and robustness</li>\n<li>obstacles</li>\n<li>resources that helped me to implement the whole thing</li>\n</ul>",
    "persons": [
      "raichoo"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 60,
    "room": "AW1.121",
    "title": "Graphing FreeBSD disk utilization with Prometheus",
    "subtitle": "Writing a Prometheus gstat_exporter",
    "track": "BSD",
    "abstract": "<p>All in a days work: How to write a Prometheus gstat_exporter and integrate it in a Grafana Dashboard</p>",
    "description": "",
    "persons": [
      "Thomas Steen Rasmussen"
    ]
  },
  {
    "start": 1577871000000,
    "duration": 20,
    "room": "AW1.125",
    "title": "Making poetry with Racket",
    "subtitle": "Come and see how to make Poems that are also Code!",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>Racket allows us to create languages on the fly. It's extremely practical for making DSLs (domain specific languages), but can it also be used to make art?\nThat's what we'll see in this talk, making (executable) poetry with Racket!</p>",
    "description": "<p>I've always been fascinated by languages, may it be programming languages, \"natural\" , or constructed ones.\nThe way streams of words convey meaning, information, requests, but also emotions, thoughts, feelings, stories, mood...\nHumans have been using languages for thousands of years. It's one of our most important creation and the one we use the most.\nFor a long time, we only had two sorts of language: natural and constructed. But since the invention of computing, and the concept of \"software\", we now have third sort: programming languages.\nWe've been writing programming languages for a century now, but, can we consider programming languages to be languages human could also talk with?\nCan we consider making art with them? Poetry?\nLet's try out using the wonderful language forge that is Racket, and see if we can learn something on the way, and have fun!</p>",
    "persons": [
      "Jérôme Martin"
    ]
  },
  {
    "start": 1577872200000,
    "duration": 20,
    "room": "AW1.125",
    "title": "A small, FRP DSL for distributed systems",
    "subtitle": "Mgmt Config: More about our language",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>Mgmt is a next gen config management tool that takes a fresh look at existing automation problems.\nThe tool has two main parts: the engine, and the language.\nThis presentation will have a large number of demos of the language.\nThe language is a minimalistic, functional, reactive DSL.\nIt was designed to both constrain the user with safe types, and no core looping constructs, but also to empower the user to build powerful real-time distributed systems.\nThis year we will expand on last years talk by showing more of the core language features like classes, functions, closures and more!\nFinally we'll talk about some of the future designs we're planning and make it easy for new users to get involved and help shape the project.</p>",
    "description": "",
    "persons": [
      "James Shubin"
    ]
  },
  {
    "start": 1577873400000,
    "duration": 20,
    "room": "AW1.125",
    "title": "XL, an extensible programming language",
    "subtitle": "A language that grows with Moore's law instead of being killed by it",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>XL is an extensible programming language, designed to grow with Moore's law instead of being killed by it.\nExtensible means that programmers can add features and notations much like they would add functions or classes in existing languages.\nThe mechanisms are based on meta-programming, and are a bit similar to the macros that gave Lisp its longevity, but with interesting twists.\nAs a proof of this extensibility, basic arithmetic (addition, multiplication, etc) or control statements (if-then-else, loops, etc) are implemented by libraries in XL, yet offer similar performance and usability as built-in features in C++.\nAnother validation of the extensibility is Tao3D, an XL-based interactive graphic language that will be used to give the presentation.</p>",
    "description": "<p>XL is an extensible programming language, designed to grow with Moore's law instead of being killed by it. Every 5 year or so, a major paradigm arrives, that makes earlier languages obsolete. Past examples include object-oriented programming, distributed programming or application extension languages. The inability of classical programming languages to integrate such changes also leads to a number of niche languages implementing minority paradigms, ranging from Prolog to make to yaml.</p>\n\n<p>Extensible in XL means that programmers can add features and notations much like they would add functions or classes in existing languages. XL is quite minimalist, since all this is done using a single operator written \"is\".\nThis operator can be used to define variable (<code>X is 0</code>), functions (<code>is_even X is X mod 2 = 0</code>), multi-operator expressions (<code>X in Y..Z is X &gt;= Y and X &lt; Z</code>), or programming constructs. Loops are defined in XL as follows:</p>\n\n<pre><code>loop Body is\n    Body\n    loop Body\n</code></pre>\n\n<p>This extensibility mechanisms is therefore based on meta-programming, and are in that way similar to the macros that gave Lisp its longevity. Lisp was first to normalize object-oriented programming with CLOS. But XL has interesting twists.\nFor starters, there is a strong focus on making notations match concepts. For example, XL will let you write (<code>1 + 2 * 3</code>) or program if statements that look normal. This is actually important.</p>\n\n<p>This talk will give three proofs of this extensibility:</p>\n\n<ul>\n<li>The standard library of XL provides things that have to be put in the compiler in other languages, like basic arithmetic (addition, multiplication, etc), basic control statements (if-then-else, while loops, etc), the module system, and so on. The compiler is actually in the library (or rather, it will be, one day).</li>\n<li>The Tao3D language turned XL into a functional reactive 3D document description language. That would have been a stretch for C, and maybe even for Lisp, for reasons that will be discussed.</li>\n<li>ELIoT, later renamed ELFE, and now integrated in trunk XL, turned XL into a distributed language, where you write one program and it executes on several machines, dispatching code around and exchanging data transparently.</li>\n</ul>\n\n\n<p>Tao3D will actually be used to give the presentation.</p>",
    "persons": [
      "Christophe de Dinechin"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 20,
    "room": "AW1.125",
    "title": "Forth - The New Synthesis",
    "subtitle": "Growing Forth with preForth and seedForth",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>The \"new synthesis\" of Forth is an ongoing effort in spirit of the Forth Modification Laboratory workshops. Its aim is to identify the essentials of Forth and to combine them in a new way to build systems that can scale-down as Forth always did\nand can scale-up to large applications and development projects.</p>\n\n<p>The new synthesis is guided by the two principles biological analogy and disaggregation.</p>\n\n<p>We scrutinize many aspects of traditional and modern Forth implementations trying to separate techniques that are normally deeply intertwined. After isolating the techniques we thrive to combine them in new ways.</p>\n\n<p>The talk describes two mile stones of the ongoing project:</p>\n\n<ul>\n<li><p>preForth (&lt; 500 LOCs) a minimalistic non-interactive Forth kernel that can bootstrap itself and can be used as an easy-to-port basis for a full Forth implementation or implementing other programming languages. It is an open ended language that inherits functionality from the target platform's development tools.</p></li>\n<li><p>seedForth (&lt;500 LOCs) a minimal stack based extensible programming system accepting tokenized source code. seedForth can be extended in various ways: as stand alone applications, as fully interactive systems, as umbilical target system for embedded system's programming</p></li>\n</ul>\n\n\n<p>We try to use Forth wherever possible in order to minimize semantic and formalism mismatches. Everything should be readily available - no hidden secrets.</p>\n\n<p>Of course many of the subjects we are looking at have been used by others in the Forth community and outside - we are dwarfs standing on the shoulders of giants - however we believe our new synthesis to be original.</p>",
    "description": "<p>Our findings in the new synthesis so far can be summarized:</p>\n\n<ul>\n<li><p>high level inner interpreter (EuroForth 2016, [1]) We showed that a traditional Forth indirect threaded code virtual machine can implemented in high level Forth bringing threaded code manipulation tricks to any Forth implementations.</p></li>\n<li><p>stacks for structured data (Forth Tagung (convention) 2017, german. [2]) Stores and handles structured items (strings, queues, lists, stacks) on stack and return stack. No memory required.\nShows how terminal input and number output can work without random accessible memory.</p></li>\n<li><p>handler based outer interpreter (EuroForth 2017, [3]) This demonstrates a very simple modular architecture for the Forth text interpreter separating interpretation and compilation\nactions for parsed tokens by handlers that possible consume and process a token text or pass it on unprocessed.</p></li>\n<li><p>preForth, simpleForth, Forth (Forth Tagung (convention) 2018, german, [4]) Presents preForth, a minimalistic non-interactive Forth kernel that can bootstrap itself, simpleForth, still non-interactive, which adds memory and control structures and \\textsf{Forth} a simple interactive Forth bootstrapped from preForth/simpleForth. See below for details.</p></li>\n<li><p>String Descriptors (EuroForth 2018, [5]) We revise different Forth string manipulation facilities and present string descriptors, an intermediate string representation balancing utility and ease of implementation.</p></li>\n<li><p>Regex (part of string descriptors paper, EuroForth 2018, [5]) Presents a simple implementation of regular expressions extended for Forth's demand to detect space separated tokens and intended to be used in the token detection part of handler based outer interpreters.</p></li>\n<li><p>seedForth  a minimal stack based extensible programming system accepting tokenized source code. seedForth can be extended in various ways (EuroForth 2018 [6]).</p></li>\n</ul>\n\n\n<p>This talk will go into the details of preForth and seedForth and will how the source code tokenizing works as well as how to extend seedForth to become a modern interactive yet minimal programming environment (&lt;1000 LOCs).</p>\n\n<p>References</p>\n\n<p>[1] Implementing the Forth Inner Interpreter in High Level Forth, Ulrich Hoffmann, EuroForth Conference 2016, Reichenau, 2016\n[2] Stack of Stacks,\\newline Ulrich Hoffmann, Forth Tagung 2017, Karlkar, 2017\n[3] A Recognizer Influenced Handler Based Outer Interpreter Structure,\\newline EuroForth 2017, Bad Vöslau, 2017\n[4] Bootstrapping Forth,\\newline Forth Tagung 2018, Linux Hotel,\\newline Essen, 2018\n[5] A descriptor based approach to Forth strings,\\newline Andrew Read and Ulrich Hoffmann, EuroForth conference, Edinburgh, 2018\n[6] String descriptors on GitHub\\newline \\url{https://github.com/Anding/descriptor-based-strings}\n[7] preForth and seedForth on GitHub\\newline \\url{https://github.com/uho/preForth}</p>",
    "persons": [
      "Ulrich Hoffmann"
    ]
  },
  {
    "start": 1577875800000,
    "duration": 20,
    "room": "AW1.125",
    "title": "A minimal pur object-oriented reflective language",
    "subtitle": "A minimal pur object-oriented reflective language",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>Pharo is a minimalist reflective dynamically typed object-oriented language. Pharo is inspired from Smalltalk: Its full syntax fits on a postcard.\nIts model is simple: everything is an object instance of a class, methods are all public virtual, attributes are first class objects and are protected. There is single inheritance and\ntraits. And nothing else! (see <a href=\"http://mooc.pharo.org\">http://mooc.pharo.org</a>). Still Pharo is a real language that is started to be used in industry <a href=\"http://pharo.org/success\">http://pharo.org/success</a> and <a href=\"http://consortium.pharo.org\">http://consortium.pharo.org</a>. The entire Pharo stack is MIT.\nPharo reflective core is bootstrapped from source code.  Experiences shows that we can have down to 11k (adding 2 smallint) and that a simple web app can be\ntrim down to 500 kb.</p>",
    "description": "<p>In this talk I will present Pharo in a nutshell: Syntax, model, but also user stories.\nI will show the vision of the project and where we want to go. I will present some key architectural choices.\nI will show some powerful features such as stack on the fly reification and their application: contextual breakpoint, on the fly program transformation.</p>\n\n<p>Bio: I'm one of the core developer of Pharo, head of the consortium and helping consortium engineers.</p>",
    "persons": [
      "Stephane Ducasse"
    ]
  },
  {
    "start": 1577877000000,
    "duration": 20,
    "room": "AW1.125",
    "title": "Bootstrapping minimal reflective language kernels",
    "subtitle": "Bootstrapping minimal reflective language kernels",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>In this talk, we present a series of tools to bootstrapping smaller crafted kernel languages.\nSmaller kernels allow us to run applications in resources limited environments (IoT).\nAlso, it allows us to modify and study language modifications and extensions easing the evolution of new and existing languages.\nThese experiments are performed in a fully debuggable simulated environment, allowing us to overcome common mistakes and problems.\nThis is not only an experimental environment as it allows us to generate production-ready language kernels.</p>\n\n<p>We use Pharo to generate language kernels that are intended to run on top of the Pharo VM.\nThese tools are also used to bootstrap bigger systems as Pharo itself.</p>",
    "description": "<p>The current explosion of embedded systems (i.e., IoT, Edge Computing) implies the need for generating tailored and customized software for them. Different approaches have been taken for building, deploying, updating and debugging these systems, although there is still no standard way to do this.\nThe goal of this talk is to present the tools and techniques necessary for building, debugging, and deploying custom small language kernels.</p>\n\n<p>Kernel languages are generated from a combination of language definition and the description of the elements and processes to generate the runtime.\nKernel languages must be consistent internally and in relation with the environment where they run (e.g. the VM, the OS)\nLanguages that are bootstrapped from their source code are not new.\nHowever, correctly defining a working consistent language kernel is a complex manual task without support from tools to debug or test before the generation of the language and its deployment.\nThis complexity limits the study of new languages, the creation of prototypes and the evolution of existing ones.</p>\n\n<p>We present a set of tools to overcome the different difficulties that bootstrapping a reflective language kernel presents. Allowing us to simulate the kernel, debug it, validate it and generate it.\nMoreover, our proposed approach offers tools to detect many common mistakes and causes of error.</p>\n\n<p>We center our solution in reflective object-oriented languages that run on top of a VM.\nOur tool uses Pharo and generates languages to run on top of its VM.</p>",
    "persons": [
      "Pablo Tesone"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 20,
    "room": "AW1.125",
    "title": "Universal package & service discovery with Guix",
    "subtitle": "Α universal functional package manager and operating system which respects the freedom of computer users.",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>GNU Guix is a universal functional package manager and operating system which\nrespects the freedom of computer users. It focuses on bootstrappability and\nreproducibility to give the users strong guarantees on the integrity of the full\nsoftware stack they are running. It supports atomic upgrades and roll-backs\nwhich make for an effectively unbreakable system.</p>",
    "description": "<p>I'll present how I intend to leverage the Guile programming language to boost\nsearchability of packages and services via intuitive user interfaces and semantics.</p>\n\n<p>Guix, like many other package managers, suffers from usability\nissues when it comes to the explorability of the content (that is,\npackages and services), facing the exorbitant amount of software there\nis out there.  Users should be able to discover the programs they need\nfor a specific task, without having to know them in advance.  It should\nbe easy to specify build options for the packages, e.g. \"build without\nsound\" or \"add IPFS support.\"  They should not have to waste time\nwriting their own hacks and scripts when ready-to-use services already\nexist and are just waiting to be discovered.</p>\n\n<p>So how do we fix the issue of improving this discoverability, with ease\nof use?</p>\n\n<p>In the context of the Next Generation Internet initiative, I've started\nworking on enhancing search, discovery and reusability of packages and\nservices.</p>\n\n<p>\"A universal software navigator on steroids -- for everyone.\"</p>",
    "persons": [
      "Pierre Neidhardt"
    ]
  },
  {
    "start": 1577879400000,
    "duration": 20,
    "room": "AW1.125",
    "title": "GNU Mes",
    "subtitle": "Scheme-only bootstrap and beyond",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>Last year GNU Mes brought the Reduced Binary Seed bootstrap to GNU Guix: gcc, glibc and binutils were removed and the size of the bootstrap binaries went down from 250MB to 130MB.  This year we introduce the Scheme-only bootstrap: Awk, Bash, Core Utilities, Grep, Gzip, Make, Sed, Tar are replaced by Gash and Gash Core Utils, halving the size of the Guix bootstrap seed again, to 60MB.</p>",
    "description": "",
    "persons": [
      "Jan Nieuwenhuizen (janneke)"
    ]
  },
  {
    "start": 1577880600000,
    "duration": 30,
    "room": "AW1.125",
    "title": "Lisp everywhere!",
    "subtitle": "Gurudom is around the corner",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>Minimalism matters in computing. Minimalism allows for smaller systems\nthat take less resources and consume less energy. More importantly,\nfree and open source minimalism allows for secure systems that are\neasy to understand. Minimalism is also educational and brings back the\nfun of the early days of computing where people learn to understand\nsystems from the ground up. As a co-organizer of this devroom I will\ntalk about my journey through many programming languages and ending up\nwith Scheme (a minimal Lisp). Lisp is the second oldest language in\nuse today and growing. I'll show you that once you master Lisp you can\nuse it everywhere from software deployment, the shell, the editor and\ndebugging and for programming systems and in the browser. As a matter\nof fact, Lisp is everywhere!</p>",
    "description": "",
    "persons": [
      "Pjotr Prins"
    ]
  },
  {
    "start": 1577882400000,
    "duration": 30,
    "room": "AW1.125",
    "title": "Celebrating Guile 2020",
    "subtitle": "Lessons Learns in the Last Lap to Guile 3",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>Guile maintainer Andy Wingo shares his thoughts on the last lap of the race to Guile 3.  We'll go over ways that Guile got faster, more capable, and more minimal at the same time.</p>",
    "description": "<p>New languages are often lovely and minimal but don't have a wide user community.  To the extent that an old language has a community, it also has a legacy burden of supporting that community's old code.  How should these be balanced?  Is there a balance?</p>\n\n<p>In this talk, Andy Wingo takes the opportunity of the Guile 3 release to reflect on change and continuity: how can a language stay minimal over time, and how is Guile working towards this goal?  We cover cases in which things have gone well, not so well, as well as ongoing challenges and opportunities.</p>",
    "persons": [
      "Andy Wingo"
    ]
  },
  {
    "start": 1577884200000,
    "duration": 30,
    "room": "AW1.125",
    "title": "Introduction to G-Expressions",
    "subtitle": "Introduction to G-Expressions",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>This talk will present an overview of G-Expressions and how the GNU Guix project uses them.</p>",
    "description": "<p>The GNU Guix project invented G-Expressions to make it easier to stage S-Expressions for later manipulation or evaluation.  They are similar to S-Expressions (hence the name) but provide useful code staging features beyond what can be easily accomplished with just \"quasiquote\" and \"unquote\".  A high-level object (such as a Guix package) can be included in a G-Expression; the transitive dependencies of that high-level object will then be automatically carried along with the G-Expression.  When the G-Expression is converted to an S-Expression and stored on disk for later manipulation or evaluation, the high-level object will be automatically \"lowered\" to an appropriate representation (such as the package's output path) via a \"compiler\".  Compared to direct manipulation of S-Expressions, G-Expressions provide a simpler and more intuitive way to stage code that refers to such high-level objects.</p>\n\n<p>The Guix project uses G-Expressions to accomplish a wide variety of tasks, including:</p>\n\n<ul>\n<li>Executing the \"liberation\" procedure to convert Mozilla Firefox's source code into GNU IceCat's source code</li>\n<li>Building Docker containers from scratch</li>\n<li>Executing activation actions during system boot</li>\n<li>...and more!</li>\n</ul>",
    "persons": [
      "Christopher Marusich"
    ]
  },
  {
    "start": 1577886000000,
    "duration": 20,
    "room": "AW1.125",
    "title": "Let me tell you about Raku",
    "subtitle": "On why syntax is not so important, with an introduction to the emerging language Raku",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>Most languages steadily incorporate new programming concepts in new releases, and new languages have these concepts already baked in. These concepts are related to how functions work and are considered and invoked, different data structures and working with things like Unicode. There's a language, Raku, that incorporates most of the new concepts that have appeared in this century. This talk is an introduction to the language by way of the concepts it uses.</p>",
    "description": "<p>Known as Perl 6 until October 14th this year, and released in Christmas 2015, Raku (https://raku.org) was designed as \"The language for the next 100 years\", and as such, it was created with the intention of incorporating most modern programming concepts. With the same motto as its (kind-of) predecessor, Perl, \"There are many ways to do it\", Raku is a multi-paradigm language that is functional, asynchronous, object-oriented, and with new interesting features like grammars.\nIn this talk I'll take a look at a dozen of features of modern languages; I'll exemplify every feature with examples from different languages, trying to get through the different concepts of Raku by way of how they are implemented in other languages.\nFinally, we'll see a few examples of Raku, showing how its rich feature set makes it ideal for learning new programming concepts, as well as putting them to good use to solve your own problems.</p>",
    "persons": [
      "Juan Julián Merelo"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 30,
    "room": "AW1.125",
    "title": "Minimalistic typed Lua is here",
    "subtitle": "Minimalistic typed Lua is here",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>In this talk I will present a typed dialect of Lua with a minimalistic implementation. I will discuss the design choices that went into the design, implementation and development approach. We will also discuss whether Lua's minimalism is retained and ponder on the nature of the resulting dialect. This is a sequel for last year talk in which I discussed the challenges on typing dynamic languages and Lua in particular, presenting the results achieved since then.</p>",
    "description": "",
    "persons": [
      "Hisham Muhammad"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 20,
    "room": "AW1.125",
    "title": "RaptorJIT: a fast, dynamic systems programming language",
    "subtitle": "Forking LuaJIT to target heavy-duty server applications",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>RaptorJIT is a Lua implementation suitable for high-performance low-level system programming. With the project scope reduced to the systems programming domain we want RaptorJIT fit one use case and excel at it, and we’re not afraid of radical change.</p>\n\n<p>This talk will be about our efforts to reduce the project’s complexity to improve maintain-ablility and pave the way for new features. A story about porting the LuaJIT interpreter from assembly to C, ruthless trade-offs, and ambitious performance targets in an expressive language.</p>\n\n<p>Topics include: predictable performance in JIT compilers, always-on profilers, memory safety in low-level programming</p>",
    "description": "",
    "persons": [
      "Max Rottenkolber"
    ]
  },
  {
    "start": 1577890200000,
    "duration": 30,
    "room": "AW1.125",
    "title": "The best of both worlds?",
    "subtitle": "Static and dynamic typing in the Crystal programming language",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>Long has raged the war between static and dynamic typing proponents. Dynamic typing promises speedy development, less verbose code, and happier developers. Static typing promises to find bugs earlier, help you fix them when they're found, and ease refactoring. Crystal is a statically typed language, but with several novel features aimed in a different direction: the perfect compromise between the two. In this talk I will cover the history and basics of Crystal, and explore the type system which makes Crystal unique.</p>",
    "description": "",
    "persons": [
      "Steph Hobbs"
    ]
  },
  {
    "start": 1577892000000,
    "duration": 30,
    "room": "AW1.125",
    "title": "C and JS as intermediary languages",
    "subtitle": "Running Nim on everything from microcontrollers to web-sites",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>Nim is an interesting new language whose design is focused around the concept of a small core and great extensibility through a powerful macro system and multiple compilation targets. In this talk I want to showcase how Nim compiles to both C/C++ and JavaScript, and what this means for how easy interoperability and targeting many different platforms can be. Showcasing how the same language can be used for programming anything from the tiniest resource scarce microcontrollers to web-sites or web-technology based desktop applications (like Electron) to normal desktop applications and server applications.</p>",
    "description": "",
    "persons": [
      "Peter Munch-Ellingsen"
    ]
  },
  {
    "start": 1577893800000,
    "duration": 30,
    "room": "AW1.125",
    "title": "Move semantics in Nim",
    "subtitle": "Deterministic Memory Management",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>This talk explains Nim's move semantics and their connection to reference counting, how Nim's model differs from C++ and why move semantics can offer superior performance. Nim with deterministic memory management never has been easier.</p>",
    "description": "",
    "persons": [
      "Andreas Rumpf (Araq)"
    ]
  },
  {
    "start": 1577895600000,
    "duration": 20,
    "room": "AW1.125",
    "title": "Designing an ultra low-overhead multithreading runtime for Nim",
    "subtitle": "Exposing fine-grained parallelism for 32+ cores hardware via message passing",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>While multithreading abstractions are consolidating over a couple of basic primitives around the notion of tasks and futures, under the hood implementations are vastly differing.\nThe abstraction \"details\" are significant in the current era as developers now have to find parallelism opportunities for 16+ cores on consumer CPUs.</p>\n\n<p>We go over the design space of task-parallel and data-parallel multithreading runtime library and present an unique, scalable approach\nbased on message passing.</p>",
    "description": "",
    "persons": [
      "Mamy Ratsimbazafy"
    ]
  },
  {
    "start": 1577896800000,
    "duration": 20,
    "room": "AW1.125",
    "title": "Async await in Nim",
    "subtitle": "A demonstration of the flexibility metaprogramming can bring to a language",
    "track": "Minimalistic, Experimental and Emerging Languages",
    "abstract": "<p>The most basic API for async IO that is high level uses callbacks, but working with those becomes convoluted very quickly. A great solution is <a href=\"https://en.wikipedia.org/wiki/Async/await\">async await</a>, but implementing it in a language is a complex endeavour. That is unless your language is flexible enough with strong enough metaprogramming support to make it possible to implement it without modifications to the compiler. Nim is one such language and its async await implementation is entirely implemented inside the standard library. In this talk I will describe how async await in Nim works, both at the syntax level and the event loop level.</p>",
    "description": "",
    "persons": [
      "Dominik Picheta"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 15,
    "room": "AW1.126",
    "title": "istsos3: Data Analysis and statistical tools and unit conversions",
    "subtitle": "GSoC 2017, 18 at OSGeo(istSOS)",
    "track": "Geospatial",
    "abstract": "<p>Project Name: Data Analysis and statistical tool suite (GSoC 2017)\nThe primary goal of my project was to create OAT(Data analysis and statistics) extension in RESTFul Web API and OAT extension having data analysis and statistical tools for istSOS which is being used to automate the creation of statisticate documents using OAT library (FREEWAT) and harvesting the data from an istSOS server.</p>\n\n<p>Project Name:  istSOS - Support of unit of conversion in istsos3 (GSoC 2018)\nThe aim of my project primarily is to add plugins conversion of the unit of measure in istSOS3. The user can convert a unit in another specified unit. For Unit of measure conversion in istSOS3 we added postgresql-unit and pint libraries which has a powerful feature of unit conversion along with many specified functions like unit conversion function instantly and all types of operations support to istsos3.</p>",
    "description": "<p>Project Name: istSOS (OSGeo) - Data Analysis and statistical tool suite (GSoC 2017)</p>\n\n<p>This section comprises of following parts:\n1. OAT installation\n2. Implemented OAT methods</p>\n\n<p>OAT is a Python package that is integrated in the FREEWAT environment through an interface exposing its features to modelers and non-programmer users. OAT library implements two main classes: the Sensor class that is designated to handle time­series data and metadata and the Method class which is designated to represent a processing method. The library applies the behavioral visitor pattern which allows the separation of an algorithm from the object on which it operates: thanks to this design pattern it is possible to add a new processing capability by simply extending the Method class without the need to modify the Sensor class. From a dependency point of view, OAT takes advantage of the PANDAS (McKinney, 2010), NUMPY and SCIPY (Van der Walt et. al. 2011) packages.</p>\n\n<p>Project Name:  istSOS (OSGeo) - Support of unit of conversion in istsos3 (GSoC 2018)\nThe aim of my project primarily is to add plugins conversion of the unit of measure in istSOS3. The user can convert unit in another specified unit. For Unit of measure conversion in istSOS3 we added postgresql-unit and pint libraries which has a powerful feature of unit conversion along with many specified functions like unit conversion function instantly and all types operations supports to istsos3 data like add, subtraction, multiplication, and division with magnitude and units.</p>",
    "persons": [
      "Rahul Chauhan"
    ]
  },
  {
    "start": 1577870400000,
    "duration": 20,
    "room": "AW1.126",
    "title": "Introduction to SpatioTemporal Asset Catalogs (STAC)",
    "subtitle": "Introducing the new cloud-native cataloging specification for geospatial data: STAC",
    "track": "Geospatial",
    "abstract": "<p>The talk introduces STAC, the SpatioTemporal Asset Catalog specification. It aims to enable a cloud-native geospatial future by providing a common layer of metadata for better search and discovery. It is an emerging open standard to catalog and expose geospatial data from different sources either in a static or dynamic way.</p>\n\n<p>We’ll cover the core set of metadata fields for STAC Catalogs, Collections, and Items first, along with available extensions for describing different types of data (EO, SAR, Point Cloud, etc.). With the basics of STAC in hand, the talk will go through the Open Source ecosystem for working with STAC metadata: validators, graphical user interfaces and client command line tools and libraries for search, access, and exploitation.</p>",
    "description": "<p>The SpatioTemporal Asset Catalog (STAC) specification is an emerging standard to catalog and expose geospatial data from different sources. It aims to enable a cloud-native geospatial future by providing a common layer of metadata for better search and discovery.</p>\n\n<p>This talk gives a detailed overview of STAC and the way it allows for static and dynamic implementations at the same time. The simple concept of static catalogs living alongside the data on cloud file storage (e.g., AWS S3, GCS) by adding small JSON files is highlighted before talking through the dynamic searchable APIs built on top of the new OGC API – Features standard.</p>\n\n<p>The talk will cover the core set of metadata fields for STAC Catalogs, Collections, and Items, along with available extensions for describing different types of data (EO, SAR, Point Cloud, etc.). With the basics of STAC in hand, the talk will go through the Open Source ecosystem for working with STAC metadata: validators, graphical user interfaces and client command line tools and libraries for search, access, exploitation and API generation.</p>\n\n<p>The specification is an open standard developed on GitHub by a wide range of organizations with a strong focus on extensibility to support various domains. It encourages interested parties to extend the specification for their needs for a future of interoperable discovery and work with geospatial data. An ecosystem of Open Source tooling is evolving around the specification.</p>",
    "persons": [
      "Matthias Mohr"
    ]
  },
  {
    "start": 1577871600000,
    "duration": 20,
    "room": "AW1.126",
    "title": "Introduction to openEO",
    "subtitle": "openEO develops an open API to connect R, Python, JavaScript and other clients to big Earth observation cloud back-ends in a simple and unified way",
    "track": "Geospatial",
    "abstract": "<p>openEO is a new API specification for Earth Observation data cubes that supports data extraction, processing and viewing.</p>\n\n<p>Both the standard and its implementations are Open Source projects, which itself rely on Open Source libraries under the hood, such as GRASS GIS, GDAL, Geotrellis, Rasdaman, or provide a standardized interface to proprietary systems such as Google Earth Engine. Client implementations are available for JavaScript, R, Python, QGIS and web browsers.</p>\n\n<p>This talk will show an overview of the main capabilities, and available client and backend implementations.</p>",
    "description": "<p>Earth Observation data are becoming too large to be downloaded locally for analysis. Also, the way they are organised (as tiles, or granules: files containing the imagery for a small part of the Earth and a single observation date) makes it unnecessary complicated to analyse them. The solution to this is to store these data in the cloud, on compute back-ends, process them there, and browse the results or download resulting figures or numbers. Unfortunately, data and APIs are too often proprietary solutions and lock-in to a service provider happens easily so an interoperable standard across service providers is much needed.</p>\n\n<p>The aim of openEO is to develop an open API to connect R, Python, JavaScript and other clients to big Earth observation cloud back-ends in a simple and unified way. With such an API, each client can work with every service provider, and it becomes possible to compare them in terms of capacity, cost, and results (validation, reproducibility).</p>\n\n<p>The specification is centered around Earth Observation data cubes that supports data extraction, processing and viewing. It specifies a set of common processes to be used so that switching between service providers is less of a problem. Both the standard and its implementations are Open Source projects. Under the hood, the client and backend implementations rely on Open Source libraries, such as GRASS GIS, GDAL, Geotrellis, Rasdaman, but also provide a standardized interface to proprietary systems such as Google Earth Engine.</p>",
    "persons": [
      "Matthias Mohr"
    ]
  },
  {
    "start": 1577873100000,
    "duration": 20,
    "room": "AW1.126",
    "title": "GeoServer Basics",
    "subtitle": "",
    "track": "Geospatial",
    "abstract": "<p>GeoServer Basics\nWelcome to GeoServer, a popular web service for publishing your geospatial data using industry standards for vector, raster and mapping.</p>\n\n<p>Are you just getting started with GeoServer, or considering it for the first time?</p>\n\n<p>This presentation is here to help, introducing the basics of:\nUsage: Concepts used to connect to your data and publish as a spatial service.\nContext: What role GeoServer plays in your organization and what value the application provides.\nCommunity: How the project is managed, and a discussion of the upcoming activities.</p>\n\n<p>Attend this presentation to get a running start on using GeoServer in your organization.</p>",
    "description": "",
    "persons": [
      "Jody Garnett"
    ]
  },
  {
    "start": 1577874300000,
    "duration": 20,
    "room": "AW1.126",
    "title": "GeoNetwork Basics",
    "subtitle": "",
    "track": "Geospatial",
    "abstract": "<p>GeoNetwork Basics\nWelcome to GeoNetwork, a leading web service for keeping track of the spatial information used by your organization.</p>\n\n<p>Jody is an experienced open source community member, digging into what this technology offers, and how it is used. This presentation shares these findings with you, and touches on what makes GeoNetwork succeed:</p>\n\n<p>We look at what GeoNetwork is for, the business challenge it is faced with, and the amazing technical approach taken by the technology.\nFor context we look at the core layer publishing workflow to see what is required\nWe peek under the hood at how the editor works, and discover the central super-power of GeoNetwork\nLook at examples of how GeoNetwork has been extended by organizations to see what is possible with this technology</p>\n\n<p>Attend this presentation for an informative tour of the GeoNetwork ecosystem.</p>",
    "description": "",
    "persons": [
      "Jody Garnett"
    ]
  },
  {
    "start": 1577875800000,
    "duration": 20,
    "room": "AW1.126",
    "title": "Getting inspired by open software for a web site: g3n.fyi",
    "subtitle": "",
    "track": "Geospatial",
    "abstract": "<p>So you are here at FOSDEM in Brussels. Also sightseeing? Geocaching? Tried to optimize your way along the sights or to find many caches without making it a hike? Then you've got the traveling salesman problem! Famous in computer science because finding the optimum is extremely difficult and finding good approximations can be done easily.</p>\n\n<p>Last year we had a talk about 3geonames.org where the Hilbert curve was mentioned to be used in name generation. When researching about this space curve it turned out that such space filling curves give good approximations for the traveling salesman problem. This has already been evaluated scientifically. Route finding using thees curves is extremely simple. Other algorithms need much more computational effort. Using a space filling curve to find a route proposal and improving it with 2-Opt optimization algorithm gives the quality of 2-Opt at high speed. Even so fast that it keeps track with interactive changes of the waypoints on a moving map display.</p>\n\n<p>This mechanism gives short routes for your sightseeing or geocaching planning and can also be used professionally if you have to visit several locations on a single tour as in package delivery, meals on wheels, or elderly care.</p>",
    "description": "",
    "persons": [
      "Thomas Bremer"
    ]
  },
  {
    "start": 1577877300000,
    "duration": 20,
    "room": "AW1.126",
    "title": "Arabesque: a geographic flow visualization application",
    "subtitle": "",
    "track": "Geospatial",
    "abstract": "<p><em>Arabesque</em> is a FLOSS web application dedicated to flow mapping and analysis. Using web technologies, it provides tools to the user to load O/D data. The user can easily modify filters parameters or add new ones. A default symbology is proposed but the user can define is own.</p>\n\n<p><em>Arabesque</em> aims to provides a fast, lightweight framework to visualize and explore flow data with a special attention on graphics and symbology in order to produce beautiful and just flow maps.</p>\n\n<p>This presentation will be a short presentation of the project and a quick demo of the application. It is the extension of the presentation made at FOSS4G 2019: release, code publication, updates and a live demo.</p>",
    "description": "<p>Geographic flow visualization (gFlowiz) is an interdisciplinary project dedicated to flows and networks in the geoweb.\nIt is led by a team of French researchers and engineers in Geography, Cartography and Computer Science from both IFSTTAR and CNRS.</p>\n\n<p>A state of the art on current issues of flows and movement analysis on the geoweb has been produced through the compilation of around 70 applications in a thematic dashboard, and a 200 respondants survey on flow map usages and needs has been realized.</p>\n\n<p>The results of this were combined to create the specifications of the <em>Arabesque</em> application.\n<em>Arabesque</em> is a FLOSS web application dedicated to flow mapping and analysis. Using web technologies, it provides tools to the user to load Origin/Destination data in robust and well documented formats (CSV or geojson). <em>Arabesque</em> will display the most significant data at launch but the user can easily modify filters parameters or add new ones on nodes and/or links. A default symbology is proposed but color scales, size, transparency and shape of objects can be modified as well.</p>\n\n<p><em>Arabesque</em> aims to provides a fast, lightweight framework to visualize and explore flow data.\nA special attention has been provided on graphics and use of correct symbology in order to produce\nbeautiful and just flow maps.</p>\n\n<p>This presentation will be a short presentation of the <a href=\"https://geoflowiz.hypotheses.org/\">Gflowiz project</a> and a quick demo of the <a href=\"https://arabesque.ifsttar.fr\"><em>Arabesque</em> application</a>.</p>\n\n<h3>Speaker bio</h3>\n\n<ul>\n<li>Nicolas Roelandt works at the <a href=\"https://www.ifsttar.fr/en/the-institute/ifsttar/who-are-we/\">IFSTTAR research institute</a> as a GIS engineer. He is an OSGeo charter member and an OSGeoLive PSC member.</li>\n<li>Françoise Bahoken is researcher in geography at the <a href=\"https://www.ifsttar.fr/en/the-institute/ifsttar/who-are-we/\">IFSTTAR research institute</a>. Her subject is flow and movements geographical patterns through cartography</li>\n<li>Laurent Jégou is a cartographer and geographer at the <a href=\"https://www.univ-tlse2.fr\">Toulouse-Jean Jaurès University</a></li>\n<li>Marion Maisonobe is a geographer at the <a href=\"http://www.parisgeo.cnrs.fr/spip.php?article8513&amp;lang=es\">CNRS research institute</a>. She studies scientific networks and their geography.</li>\n<li>Etienne Côme is researcher at the <a href=\"https://www.ifsttar.fr/en/the-institute/ifsttar/who-are-we/\">IFSTTAR research institute</a>.  His research interests include probabilistic graphical models, data-science and visualisation and their use to solve transportation problems.</li>\n<li>Grégoire Le Campion works at the [CNRS research institute] (http://http://www.passages.cnrs.fr/) as a data science/statistical engineer.</li>\n</ul>",
    "persons": [
      "Nicolas Roelandt"
    ]
  },
  {
    "start": 1577878800000,
    "duration": 20,
    "room": "AW1.126",
    "title": "Boost.Geometry R-tree - speeding up geographical computation.",
    "subtitle": "",
    "track": "Geospatial",
    "abstract": "",
    "description": "",
    "persons": [
      "Adam Wulkiewicz"
    ]
  },
  {
    "start": 1577880300000,
    "duration": 20,
    "room": "AW1.126",
    "title": "Testing Navit using Device Farms",
    "subtitle": "",
    "track": "Geospatial",
    "abstract": "",
    "description": "",
    "persons": [
      "Patrick Höhn"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 20,
    "room": "AW1.126",
    "title": "Reverse Geocoding is not easy",
    "subtitle": "IGN 10/10 makes you feel like a geocoder",
    "track": "Geospatial",
    "abstract": "<p>Having seen a dozen of different OpenStreetMap-based geocoders, I did not expect to find myself writing another one. But here I am, tasked with making a reverse geocoder better than the industry-standard Nominatim. Turns out it is a fun and not so straightforward task. Let’s see what can go wrong.</p>",
    "description": "",
    "persons": [
      "Ilya Zverev"
    ]
  },
  {
    "start": 1577883300000,
    "duration": 20,
    "room": "AW1.126",
    "title": "Working with spatial trajectories in Boost Geometry",
    "subtitle": "",
    "track": "Geospatial",
    "abstract": "",
    "description": "",
    "persons": [
      "Vissarion Fysikopoulos"
    ]
  },
  {
    "start": 1577884800000,
    "duration": 20,
    "room": "AW1.126",
    "title": "MobilityDB",
    "subtitle": "Managing mobility data in PostGIS",
    "track": "Geospatial",
    "abstract": "<p>MobilityDB is an open source moving object database system (https://github.com/ULB-CoDE-WIT/MobilityDB). Its core function is to efficiently store and query mobility tracks, such as vehicle GPS trajectories. It is engineered up from PostgreSQL and PostGIS, providing spatiotemporal data management via SQL. It integrates with the postgreSQL eco-system allowing for complex architectures such as mobility stream processing and cloud deployments.</p>\n\n<p>The presentation will explain the architecture of MobilityDB, its database types, indexes, and operations. An end to end example will be demonstrated, starting with data preparation, loading, transformation, querying, until visualization. This presentation will be of special interest to the PostgreSQL community, and to professionals in the transportation domain.</p>\n\n<p>This presentation will build on our talks in PGConf.ru 2019, and FOSS4G Belgium 2019.</p>",
    "description": "<p>MobilityDB is an open source PostgreSQL extension that adds support for temporal and spatio-temporal objects to the PostgreSQL and PostGIS. MobilityDB implements the Moving Features specification from the Open Geospatial Consortium (OGC).</p>\n\n<p>Features:\n- Time types: Period, PeriodSet, and TimestampSet which, in addition of the the TimestampTz type provided by PostgreSQL, are used to represent time spans.\n- Temporal types: tbool, tint, tfloat, and ttext which are based on the bool, int, float, and text types provided by PostgreSQL and are used to represent basic types that evolve on time.\n- Spatio-temporal types: tgeompoint and tgeogpoint which are based on the geometry and geography types provided by PostGIS (restricted to 2D or 3D points) and are used to represent points that evolve on time.\n- Range types: intrange and floatrange which are used to represent ranges of int and float values.</p>\n\n<p>All these types have associated an extensive set of functions and operators. GiST and SP-GIST index support for these types are also provided.</p>",
    "persons": [
      "Mahmoud  Sakr"
    ]
  },
  {
    "start": 1577886300000,
    "duration": 20,
    "room": "AW1.126",
    "title": "Geo-spatial queries on multi-petabyte weather data archives",
    "subtitle": "",
    "track": "Geospatial",
    "abstract": "<p>Geo-spatial queries on multi-petabyte weather data archives\nJohn Hanley, Nicolau Manubens, Tiago Quintino, James Hawkes, Emanuele Danovaro</p>\n\n<p>Weather forecasts produced by ECMWF and environment services by the Copernicus programme act as a vital input for many downstream simulations and applications. A variety of products, such as ECMWF reanalyses and archived forecasts, are additionally available to users via the MARS archive and the Copernicus data portal. Transferring, storing and locally modifying large volumes of such data prior to integration currently presents a significant challenge to users. The key aim for ECMWF effort in H2020 Lexis project is to provide tools for data query and pre-processing close to data archives, facilitating fast and seamless application integration by enabling precise and efficient data delivery to the end-user.</p>\n\n<p>ECMWF aims to implement a set of services to efficiently select, retrieve and pre-process meteorological multi-dimensional data by allowing multi-dimensional queries including spatio-temporal and domain-specific constraints. Those services are exploited by Lexis partners to design complex workflows to mitigate the effect of natural hazards and investigate the water-food-energy nexus.</p>\n\n<p>This talk will give a general overview of Lexis project and its main aims and objectives. It will present the pilot applications exploiting ECMWF data as the main driver of complex workflows on HPC and cloud computing resources. In particular, it will focus on how ECMWF's data services will provide geospatial queries on multi-dimensional peta-scale datasets and how this will improve overall workflow performance and enable access to new data for the pilot users.</p>\n\n<p>This work is supported by the Lexis project and has been partly funded by the European Commission's ICT activity of the H2020 Programme under grant agreement number: 825532.</p>",
    "description": "",
    "persons": [
      "Emanuele Danovaro"
    ]
  },
  {
    "start": 1577887800000,
    "duration": 20,
    "room": "AW1.126",
    "title": "actinia: geoprocessing in the cloud",
    "subtitle": "",
    "track": "Geospatial",
    "abstract": "<p>With a rapidly increasing wealth of Earth Observation (EO) and geodata, the demand for scalable geoprocessing solutions is growing as well. Following the paradigm of bringing the algorithms to the data, we developed the cloud based geoprocessing platform actinia (<a href=\"https://actinia.mundialis.de\">https://actinia.mundialis.de</a> and <a href=\"https://github.com/mundialis/actinia_core\">https://github.com/mundialis/actinia_core</a>). This free and open source solution is able to ingest and analyse large volumes of data in parallel. actinia provides a HTTP REST API around GRASS GIS functionality, extended by ESA SNAP and user scripts written in Python. Core functionality includes the processing of raster and vector data as well as time series of satellite images. The backend is connected to the full Landsat and Copernicus Sentinel archives. actinia is an OSGeo Community Project since 2019 and a backend of the <a href=\"https://openeo.org\">openEO.org</a> API (EU H2020 project).</p>",
    "description": "",
    "persons": [
      "Markus Neteler"
    ]
  },
  {
    "start": 1577889300000,
    "duration": 20,
    "room": "AW1.126",
    "title": "RoboSat.pink: Deep Learning Computer Vision patterns extraction at scale",
    "subtitle": "",
    "track": "Geospatial",
    "abstract": "<p>RoboSat.pink, a Deep Learning Computer Vision framework for GeoSpatial Imagery,\nallow you to perform at scale:</p>\n\n<ul>\n<li>DataSet Quality Analysis</li>\n<li>Change Detection highlight</li>\n<li>Features extraction</li>\n</ul>\n\n\n<p>This presentation will focus on the latests enhancements of RoboSat.pink,\nand mainly on:</p>\n\n<ul>\n<li>How we increase again pattern recognition accuracy, even with unconsistents labels</li>\n<li>How we speed up the whole process, and allow to scale even on large areas</li>\n<li>New hot features</li>\n</ul>",
    "description": "",
    "persons": [
      "Olivier Courtin"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 20,
    "room": "AW1.126",
    "title": "Apache Spark on planet scale",
    "subtitle": "Using Apache Spark to process OpenStreetMap data",
    "track": "Geospatial",
    "abstract": "<p>Apache Spark is an open-source distributed general-purpose cluster-computing framework with implicit data parallelism. OpenStreetMap is a huge database of features, found on Earth surface. Working with that database is hard, so Spark is a natural solution to solve OSM size-caused processing issues. I'm going to show how to load OSM data to Spark, run processing algorithms like extract/merge or render and how using Spark improves development process and cuts processing times greatly.</p>",
    "description": "<p>Will show, how to use Spark OSM DataSource to load data to the Spark DataFrame and how to use Spark for OSM data merge/extract, simple analysis, rendering etc. Talk will also mention multithreaded OSM PBF parser, that can be used independently of Spark or other processing library.</p>",
    "persons": [
      "Denis Chaplygin"
    ]
  },
  {
    "start": 1577892300000,
    "duration": 20,
    "room": "AW1.126",
    "title": "Creating GPX tracks from cycle routes in OpenStreetMaps",
    "subtitle": "Using the OverpassAPI to download and process cycle routes from OpenStreetMaps",
    "track": "Geospatial",
    "abstract": "<p>Across Europe, there are many regional, national and international cycle routes; they provide safe ways for families and friends to travel and explore by bike. They can however, be hard to follow: overgrown vegetation can hide directions, signs are subject to vandalism and sometimes it is just easy to miss a turn.</p>\n\n<p>Having freely available GPX tracks for cycle routes means people can better plan their journey and avoid wrong turns when following the route. OpenStreetMaps is the best source of information for cycle routes and these relations can easily be downloaded using the OverpassAPI.</p>\n\n<p>In this talk I will present an Open Source tool to download GPX tracks of cycle routes, and a website for people to download the generated GPX files. I will discuss some of the nuances of how cycle routes are stored as relations and what processing needs to be performed in order to create a continuous route. In addition, I will speak about how the tool can be used to identify inconsistencies in OpenStreetMaps data.</p>",
    "description": "",
    "persons": [
      "Henry Miskin"
    ]
  },
  {
    "start": 1577893800000,
    "duration": 20,
    "room": "AW1.126",
    "title": "Divide and map. Now.",
    "subtitle": "damn project",
    "track": "Geospatial",
    "abstract": "<p>There is a Tasking manager by the Humanitarian OpenStreetMap Team (HOT). We use it heavily during mapathons (mapping for developing countries). The Tasking Manager serves one primary purpose: take some great area to be mapped and split it to squares a human can map in a few minutes. With this divide and map approach, we can map a lot.</p>\n\n<p>There are some issues with the Tasking Manager, however. The main problem is performance -- it is slow and failing when loaded by requests.</p>\n\n<p>The next is the architecture of the Tasking Manager. It is wrong, in my opinion.</p>\n\n<p>The last but not least is that the Tasking Manager is not community-driven. The Tasking Manager is a product of HOT for which you can download the source code.</p>\n\n<p>In the talk, I want to introduce <strong>Divide and map. Now.</strong> -- damn project. It is an alternative to the Tasking Manager that tries to fix the issues noted above.</p>",
    "description": "<p>I am going to publish the full description between Dec 2019 and Jan 2020 as it is not ready yet. The damn project is not fully released yet, although parts of it are developed as opensource:\n- https://gitlab.com/qeef/damn<em>server\n- https://gitlab.com/qeef/damn</em>client\n- https://gitlab.com/qeef/damn<em>plugin\n- https://gitlab.com/qeef/damn</em>manager</p>\n\n<p>EDIT: The project is released:\n- https://www.openstreetmap.org/user/qeef/diary/391778\n- https://www.damn-project.org/</p>",
    "persons": [
      "Jiri Vlasak"
    ]
  },
  {
    "start": 1577895300000,
    "duration": 20,
    "room": "AW1.126",
    "title": "Integration Processes",
    "subtitle": "Data flowing the easy way",
    "track": "Geospatial",
    "abstract": "<p>To run our software we need a flow of data going through it. Usually we write scripting pieces to make that workflow of data moving from one component to the next. Integration Processes are the \"glue\" between these software pieces. Automating the data flows, adding conditional steps, handling credentials on a secure way,... That's usually a tedious and repetitive task lots of developers do again and again. With Integration Processes frameworks we can make it simpler and reuse expertise from other developers. We could even build entire workflows without throwing a single line of code.</p>",
    "description": "<p>Apache Camel is an open source integration framework that empowers you to quickly and easily integrate various systems consuming or producing data. Based on Enterprise Integration Patterns to help you solve your integration problem by applying best practices out of the box. Camel is one of the the most active project in the Apache Foundation and is the base of many other FOSS projects.</p>\n\n<p>Syndesis is an open source project that helps non-developers create complex integrations easily on a graphic interface.</p>",
    "persons": [
      "María Arias de Reyna"
    ]
  },
  {
    "start": 1577896800000,
    "duration": 20,
    "room": "AW1.126",
    "title": "The Wallonian GeoChallenge Invitation",
    "subtitle": "",
    "track": "Geospatial",
    "abstract": "<p>During the first months of 2020, the Geoportail of Wallonia,  and some partners will organise a public event called « The GeoChallenge ».</p>\n\n<p>In a few words, the idea is to ask  participants to solve public services or citizens expectations by using Wallonia’s geographic information and ressources.</p>\n\n<p>More than a hackathon, the event will last a few weeks with the idea to create results that translate into concrete benefits.</p>\n\n<p>The first call for proposals will be launched in january and will last until the end of february.</p>",
    "description": "",
    "persons": [
      "Emmanuel Jauquet"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 55,
    "room": "K.3.201",
    "title": "Do Linux Distributions Still Matter with Containers?",
    "subtitle": "",
    "track": "Distributions",
    "abstract": "<p>In the beginning there was compiling and static linking. My first programs when I was 10 years old worked like that. Then, we discovered dynamic linking. This was great because we could now patch one library and all of the programs would pick up the change on restart. But we created another problem - dependencies. Throughout the history of computing we have solved one problem and created another. Containers are no different. This talk will walk through why we invented Linux distros and why we should continue to appreciate them in a world full of container images...</p>",
    "description": "<p>In the beginning there was compiling and static linking. My first programs when I was 10 years old worked like that. Then, we discovered dynamic linking. This was great because we could now patch one library and all of the programs would pick up the change on restart. But we created another problem - dependencies. Throughout the history of computing we have solved one problem and created another. Containers are no different. This talk will walk through why we invented Linux distros and why we should continue to appreciate them in a world full of container images.</p>\n\n<p>The short answer is yes. Yes, they still matter because of several reasons:</p>\n\n<ol>\n<li><p>A linux distribution is a bunch of human beings that work together to create a dependency tree of software packages. This dependency tree is still convenient in container images</p></li>\n<li><p>There is a TON of knowledge embedded in systemd unit files on how to properly start/stop commonly used daemons</p></li>\n<li><p>Mapping CVEs to packages with meta data is still useful in a container</p></li>\n<li><p>Standardized C libraries like glibc are used by binaries, interpreters like Python, and even virtual machines managers like the JVM</p></li>\n<li><p>Critical libraries like libssl, openssl, and hardware accelerated bridges, are useful to everyone</p></li>\n<li><p>Linux distros are a connection point with gravity which builds community. Community is what solves problems</p></li>\n<li><p>Host and container image portability (glibc actually can take different code paths depending on what hardware is made available by the kernel. Also, glibc has a min/max kernel version that it supports well when compiled)</p></li>\n</ol>",
    "persons": [
      "Scott Mccarty"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 25,
    "room": "K.3.201",
    "title": "What's up on Haiku?",
    "subtitle": "R1/beta2, packaging, porting and contributing.",
    "track": "Distributions",
    "abstract": "<p>What are the new features in the upcoming R1/beta2? How did the packaging system work out? How to make your software easier to port to it, and how to contribute?</p>",
    "description": "<p>Haiku is a Free Software Operating System, inspired by the BeOS, which focuses on personal computing.</p>\n\n<p>It's been in the making for more than 18 years now. We'll see what's coming up for the R1/beta2 release.</p>\n\n<p>The packaging system has been integrated for some years now, as a different approach to software distribution. Did it live up to its promise? How well does it scale with the growing number of available packages?</p>\n\n<p>What are the specifics of Haiku that you should care about when writing portable software?</p>\n\n<p>How to contribute to various parts of the system?</p>",
    "persons": [
      "François Revol (mmu_man)"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 25,
    "room": "K.3.201",
    "title": "Homebrew: Features and Funding",
    "subtitle": "",
    "track": "Distributions",
    "abstract": "<p>A talk about the Homebrew package manager and how we've been working with our users to introduce new features to subsets of users, encouraging users to donate to the project and communicating both these to as many users as possible without being annoying.</p>",
    "description": "",
    "persons": [
      "Mike McQuaid"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 25,
    "room": "K.3.201",
    "title": "GNU Guix as an alternative to the Yocto Project",
    "subtitle": "",
    "track": "Distributions",
    "abstract": "<p>This talk demonstrates how to use GNU Guix to build a root filesystem for an\nembedded device (Pine A64 LTS).  I will also try to answer the question:\ncould GNU Guix be a viable alternative to the Yocto project and what would be\nthe benefits of using GNU Guix?</p>",
    "description": "<p>Working as an embedded software engineer, I've been using Yocto and Buildroot\nprojects to create root filesystems for embedded devices. While Buildroot is\nonly suitable for small embedded systems, Yocto does scale well, but is a\nreally complex tool.</p>\n\n<p>Plus, both tools are difficult to handle for developers without a strong\nunderstanding of Linux system integration, and on the other hand, do not\nprovide APIs and introspection tools for integrators.</p>\n\n<p>In this talk, I want to explore the possibility of using GNU Guix as an\nalternative to the Yocto project to generate embedded root filesystems.</p>\n\n<p>With 7 years of existence, more than 10000 packages and 4 supported\narchitectures, GNU Guix can be used as a transactional package manager and an\nadvanced distribution of the GNU operating system running on the Linux kernel.</p>\n\n<p>What would be missing to cover all Yocto features? How could the embedded\ndeveloper benefit from GNU Guix features such as its high level Scheme API,\npackage substitution mechanism, strong reliability and reproducibility?</p>\n\n<p>To provide some real world application, I'll compare the process of adding\nsupport for a new board (Pine A64 LTS) on Yocto and GNU Guix. Then I'll\ncompare how to configure, build and flash a small root filesystem for\nthat same board, on the two tools.</p>",
    "persons": [
      "Mathieu Othacehe"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 25,
    "room": "K.3.201",
    "title": "Software distribution: new points of failure",
    "subtitle": "In a censored world",
    "track": "Distributions",
    "abstract": "<p>There is a multitude of software or code ecosystems: Linux distribution packages, language-specific (e.g. Python or node.js) modules, third-party desktop themes, git repositories, and recently also Flatpak and Snap. Users thus obtain software and code mainly from the network. This talk explores what can go wrong in such code delivery mechanisms, and what actually went wrong when a new threat has materialized: networks in certain countries started to be unreliable \"thanks\" to the governments (classical example: https://isitblockedinrussia.com/?host=7-zip.org == true). And what technical steps can be done in order for the said ecosystems to survive when censorship and overblocking spreads over the globe even more.</p>",
    "description": "<p>The focus will be on how mirror networks and CDNs operate (and what's the difference and why it matters), illustrated by examples of Debian mirrors and NPM. Both availability and integrity concerns regarding code delivery will be discussed.</p>",
    "persons": [
      "Alexander E. Patrakov"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 55,
    "room": "K.3.201",
    "title": "Reinventing Home Directories",
    "subtitle": "Let's bring the UNIX concept of Home Directories into the 21st century",
    "track": "Distributions",
    "abstract": "<p>The concept of home directories on Linux/UNIX has little changed in the last 39 years. It's time to have a closer look, and bring them up to today's standards, regarding encryption, storage, authentication, user records, and more.</p>\n\n<p>In this talk we'll talk about \"systemd-homed\", a new component for systemd, that reworks how we do home directories on Linux, adds strong encryption that makes sense, supports automatic enumeration and hot-plugged home directories and more.</p>",
    "description": "",
    "persons": [
      "Lennart Poettering"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 55,
    "room": "K.3.201",
    "title": "Using systemd security features to build a more secure distro",
    "subtitle": "",
    "track": "Distributions",
    "abstract": "<p>Systemd provides a bunch of features which can be used to contain and secure services,\nmaking security and isolation primitives provided by the kernel accessible to system programs.\nThis allows service authors to write much simpler code, and often to avoid any integration\nwith the operating system for security purposes.\nUnfortunately, those features are still not widely used, possibly because developers\nwant to maintain compatibility with a wide range of systems.</p>\n\n<p>I'll talk about the features that are the most useful,\nhow they can be used in practice, and how this could be used\nto make a noticeable change in security at the distribution level.</p>",
    "description": "<p>The number of security features that systemd provides is long and growing:\nFirst, it performs setup like creating runtime directories and opening sockets, so the service doesn't need privileges.\nSecond, it makes it easy to run services as unprivileged users, removing a whole set of problems.\nThird, it uses kernel features like mount and network namespaces, capabilities, resource limits, to constrain services.\nFourth, it implements additional filters using BPF (per-service firewalls, devices controller).\nFifth, it does resource cleanup after the service is done, removing the need for privileges again.</p>\n\n<p>We could use this to vastly simplify services and to provide an additional level of security for system services.\nSome distributions are making use of this, but not nearly enough.\nFedora is probably at the forefront, but the common case is still to run as root will full access to everything the service doesn't need.\nDebian is now discussing a General Resolution to drop SysV Init compatibility and empower packagers to use all systemd features.\nFull support in the two biggest distro families would motivate upstreams to make systemd their \"baseline\" and build more secure services.\nNew features like \"dynamic users\" could be used to make Linux systems take more modern approaches to system security.</p>\n\n<p>I want the talk to serve as a prompt for a general discussion how we could modernize service packaging in distros,\nto avoid reimplementing security features in individual daemons, and how to stop the 90's mentality of running everything as root.</p>",
    "persons": [
      "Zbigniew Jędrzejewski-Szmek"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 25,
    "room": "K.3.201",
    "title": "Introducing libeconf",
    "subtitle": "Bringing systemd-like configuration layering to everything else",
    "track": "Distributions",
    "abstract": "<p>systemD has a very distribution-friendly way of providing it's configuration, with distributions providing defaults in /usr and users being able to override things either selectively or entirely with their own files in /etc.\nThis is especially nice for distributions wishing to be in some way stateless, support a read-only root filesystem, or provide some kind of factory-reset.\nlibeconf is a newly written C library to ease the adoption of similar configuration layering in other programs across the Linux ecosystem.</p>",
    "description": "<p>This talk will give a brief introduction to libeconf, how to use it in your existing programs and demonstrate some examples that have already adopted libeconf (eg. PAM, util-linux, rebootmgr, etc).</p>\n\n<p>The session will also share some future plans and welcome suggestions for future contributions, especially for additional features, language bindings, etc.</p>\n\n<p>The target audience is primarily developers of 'low level' distribution plumbing (eg. core daemons &amp; services, package managers, etc) that are most likely to benefit from libeconf, but might be of interest to anyone developing any service for linux distributions.</p>",
    "persons": [
      "Richard Brown"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 25,
    "room": "K.3.201",
    "title": "GRUB upstream and distros cooperation",
    "subtitle": "",
    "track": "Distributions",
    "abstract": "<p>The presentation will discuss current state of GRUB upstream development and cooperation with distributions.</p>",
    "description": "<p>The first half of presentation will be focusing on last year, current and future development efforts. The second half will discuss cooperation between GRUB upstream and distros. In general it will show current progress in the project and main pain points. One of the goals of the presentation is to solicit some help from the community. Maintainers are quite busy and they are not able to solve all issues themselves. So, help from others is greatly appreciated. At the end of presentation Q&amp;A session is planned.</p>",
    "persons": [
      "Daniel Kiper"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 55,
    "room": "K.3.201",
    "title": "Integrating new major components on fast and slow moving distributions",
    "subtitle": "How latest GNOME desktop was integrated into latest SUSE / openSUSE releases",
    "track": "Distributions",
    "abstract": "<p>Upgrading big components in Linux distributions is hard. But integrating them while minimizing regressions (for stable distributions) and not slowing down release pace (for rolling releases) requires a lot of process and tooling.</p>\n\n<p>Let's deep dive in those.</p>",
    "description": "<p>Over the previous months, openSUSE and SUSE teams have worked together on integrating latest GNOME release (3.34) in their various distributions, while minimizing duplicated work and sharing as much code as possible.</p>\n\n<p>We'll describe how it was done for the 3 differents flavors of distributions:\n- openSUSE Tumbleweed (rolling release)\n- SUSE Linux Enterprise 15 SP2 (Enterprise release)\n- openSUSE Leap 15.2 (stable release)</p>",
    "persons": [
      "Frederic Crozat"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 55,
    "room": "K.3.201",
    "title": "Fedora rawhide packages gating, for real!",
    "subtitle": "How we have implemented gating rawhide packages in Fedora",
    "track": "Distributions",
    "abstract": "<p>Rawhide, the, rolling, development version of Fedora has often been considered has too broken and too unstable. Sometime to the detriment of the development of stable releases as well.\nIn a near future, this should no longer be happening as now every change made to every package landing in Fedora rawhide can be gated based on test results.</p>\n\n<p>This talk will walk you through the processes and changes that Fedora landed to make of this idea a reality.</p>",
    "description": "<p>Rawhide is the development version of Fedora. It is the version from which stable Fedora releases branch from and thus every change made to it will trickle down to the next stable release. This also means that there are time in the development of Fedora where changes landing in rawhide can (and do!) have a detrimental effect on the next stable release.</p>\n\n<p>With the rawhide package gating initiative, Fedora has gained mechanisms to test and gate packages based on the results of these tests.</p>\n\n<p>In this talk we will go through the mechanisms built to allow this gating, how it works, how to debug if there are issues with it.\nWe will also gladly receive feedback from Fedora contributors who have interacted with it.</p>",
    "persons": [
      "Pierre-Yves Chibon"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 25,
    "room": "K.3.401",
    "title": "Discover dependency license information with ClearlyDefined",
    "subtitle": "License discovery and record-keeping for crates",
    "track": "Rust",
    "abstract": "<p>Complying with license obligations can incur a lot of hurdles. This results in developers skipping compliance or failing to achieve correct compliance. To compound this, package authors sometimes fail to describe the license of their package clearly or omit license information of included files. ClearlyDefined is a community curated repository of discovered license information for Crates packages, among other types.</p>\n\n<p>This talk will cover: the importance of the license obligations of the dependencies of your Rust package, tool, or application. How to discover the license information of those packages. And demonstrate some Cargo compatible tooling that allows automated license recordkeeping and notice file generation as a part of your CI system.</p>",
    "description": "",
    "persons": [
      "Jeff Mendoza"
    ]
  },
  {
    "start": 1577871000000,
    "duration": 25,
    "room": "K.3.401",
    "title": "cargo deny",
    "subtitle": "Fearlessly update your dependencies",
    "track": "Rust",
    "abstract": "<p>A talk about https://github.com/EmbarkStudios/cargo-deny, why we created it, and how it helps us manage our dependencies in the long term.</p>",
    "description": "",
    "persons": [
      "Jake Shadle"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 25,
    "room": "K.3.401",
    "title": "Packaging Rust programs in GNU Guix",
    "subtitle": "Build reproducibility and dependency management",
    "track": "Rust",
    "abstract": "<p>Rust is a language with a healthy ecosystem and a strong developer base. With built-in dependency management it's easy to build and install new programs even for those who have never used the language. But how is its adoption among Linux distros?\nCome with me as we figure out how best to package rust libraries and binaries in Linux distributions which demand total control over dependency management.</p>",
    "description": "",
    "persons": [
      "Efraim Flashner"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 20,
    "room": "K.3.401",
    "title": "rustdoc: beyond documentation",
    "subtitle": "All the goodies packed in rustdoc, and more",
    "track": "Rust",
    "abstract": "<p>Rust compiler comes with a few tools, rustdoc is one of them. It is THE standard rust tool to generate documentation for your crates.</p>",
    "description": "<p>Rust compiler comes with a few tools, rustdoc is one of them. It is THE standard rust tool to generate documentation for your crates.</p>\n\n<p>You can write documentation using \"///\" or \"//!\" patterns (which are syntaxic sugar over <code>#[doc = \"...\"]</code>).</p>\n\n<p>It generates HTML which can used locally without a need for internet connection. The documentation search is running in JS directly in your browser. You have a source code viewer integrated. You can pick different themes (and even add one yourself). It works with javascript disabled. It provides settings to make your docs browsing more comfortable. You can generate docs with extra content (take a look at https://docs.rs/pwnies for a good example!).</p>\n\n<p>But not only it generates documentation, it also adds things for each type that you didn't know was available thanks to the \"Auto-traits implementation\" and \"Blanket implementation\" sections.</p>\n\n<p>In addition to generate documentation, it provides functionalities such as an integrated documentation's test runner (which themselves can be quite customized!). It also provides lints that can you deny (missing<em>docs, missing</em>doc_example).</p>\n\n<p>With just all this, rustdoc is already a quite complete tool. But more will come in the future:</p>\n\n<ul>\n<li>more interactive source code viewer</li>\n<li>automatic link generation based on the item name</li>\n<li>more output formats supported (json would be the first)</li>\n<li>cfgs (doctest and doc)</li>\n<li>doc aliases for search (for instance, \"*\" proposes pointer in the std lib)</li>\n</ul>",
    "persons": [
      "Guillaume Gomez"
    ]
  },
  {
    "start": 1577875800000,
    "duration": 45,
    "room": "K.3.401",
    "title": "Rusty instruments",
    "subtitle": "Building Digital Musical Instruments with Rust and friends",
    "track": "Rust",
    "abstract": "<p>This talk will introduce the Muses project, which applies programming language theory and practice, physical computing, networking, and musicial theory to design and implementation of Digital Musical Instruments. Rust is a key ingredient in the Muses project, providing a robust and performant foundation for cross platform, desktop, and embedded system development.</p>\n\n<p>The talk will give a brief introdution to Muses project as a whole and then focus on the use of Rust in developing a selection of very different components\nin the system and its benefits for these wildy varing use cases.</p>\n\n<p>Demos of the Digitial Musical Instruments with Rust at their heart will shown through out the talk.</p>",
    "description": "<p>Controller and gesture interaction with audio and/or visual media is today ubiquitous, requiring the development of intuitive software solutions for interaction design. Designing and building these interfaces often require extensive domain expertise in audio and visual media creation, e.g. the musician, but additionally in engineering and software development. In this talk we focus on custom controller-based interactive systems for sound and musical performance, with a focus on an intuitive and simple design process that is accessible to artists.</p>\n\n<p>A large part of the software developed for these systems is low-level system code, where direct access to hardware and understandable performance are hard requirements for these systems. Historically, these systems are written in C/C++ and in the case of embedded systems C is still the language of choice. With the emergence of the system programming language Rust an alternative for developing these systems is now with us with its support for high-level features such as traits, type inference, pattern matching, and of course it's affine based type system for pointers.</p>\n\n<p>This talk will introduce the Muses project, which applies programming language theory and practice, physical computing, networking, and musical theory to design and implementation of Digital Musical Instruments. Rust is a key ingredient in the Muses project, providing a robust and performant foundation for cross platform, desktop, and embedded system development.</p>\n\n<p>A high-level overview of the schedule is:</p>\n\n<ul>\n<li> Introduction to the Muses project</li>\n<li> 100 foot view of the Muses architecture</li>\n<li> Experience using Rust for audio and interface development</li>\n<li> Demonstration</li>\n</ul>\n\n\n<p>The demonstration will include the following physical components:</p>\n\n<ul>\n<li><p> Custom interface</p>\n\n<ul>\n<li>Hardware encoders + arcade buttons</li>\n<li>Sensel touch interface with custom interface</li>\n<li>STM32 based embedded hardware platform, all running Rust</li>\n</ul>\n</li>\n<li><p> Raspberry PI for Sound</p>\n\n<ul>\n<li>Pure Data for sound synthesis</li>\n<li>Rust based driver for communicating with custom hardware</li>\n<li>Rust based Open Sound Control (OSC) server for custom control messages</li>\n</ul>\n</li>\n</ul>\n\n\n<p>The framework also includes an approach to automatically generating interfaces from a DSL for SVG interfaces, written largely in Haskell, but with a tessellation pipeline written in Rust. However, while this will be mentioned in passing it is not the intention of this talk to cover this aspect of the system in detail. (For  more information on this, see the provided link for the project website and associated papers, also linked from the site.)</p>\n\n<h2>Expected prior knowledge / intended audience</h2>\n\n<p>Knowledge of programming will be expected and prior use of C/C++, Rust, or other systems programming language would be useful.</p>\n\n<p>Audio topics will be introduced through out the talk and it is not expected that audience members have a musical background.</p>\n\n<h2>Speaker bio</h2>\n\n<p>Dr Benedict R. Gaster is an Associate Professor at University of West of England, he is the director of the Computer Science Research Centre, which within he also leads the Physical Computing group. He research focuses on the design embedded platforms for musical expression and more generally the IoT, he is the co-founder of Bristol LoRaWAN a low power wide area network for Bristol city, is the technical lead for city wide project on city pollution monitoring for communities, having developed UWE Sense a hardware platform for cheap sensing. Along with his PhD students and in collaboration with UWE's music tech department, is developing a new audio platform based on ARM micro-controllers using the Rust programming language to build faster and more robust sound!</p>\n\n<p>Previously Benedict work at Qualcomm and AMD where he was a co-designer on the programming language OpenCL, including the lead developer on AMD's OpenCL compiler. He has a PhD in computer science for his work on type systems for extensible records and variants. He has published extensively, has given numerous presentations, including ones at FOSDEM on Rust and LoRaWAN.</p>\n\n<h2>Links to some previous talks by the speaker</h2>\n\n<p>Below are are some examples of recent talks:</p>\n\n<ul>\n<li> <a href=\"https://bgaster.github.io/farm19/\">Fun with Interfaces (SVG Interfaces for Musical Expression).</a> FARM'19: 7th International Workshop on Functional Art, Music, Modeling and Design (FARM).</li>\n<li> <a href=\"https://archive.fosdem.org/2018/schedule/event/rustyarm_rust_on_embedded_platforms/\">Rustarm AKA A project looking at Rust for Embedded Systems.</a> FOSDEM’18.</li>\n<li> <a href=\"https://archive.fosdem.org/2017/schedule/event/lorawan/\">Outside the Block Syndicate: Translating Faust's Algebra of Blocks to the Arrows Framework</a>. International Faust Conference (IFC-18), 2018</li>\n<li> <a href=\"https://archive.fosdem.org/2017/schedule/event/lorawan/\">LoRaWAN for exploring the Internet of Things. Talk Hard: A technical, political, and cultural look at LoRaWAN for IoT.</a> FOSDEM’17.</li>\n</ul>",
    "persons": [
      "Benedict Gaster"
    ]
  },
  {
    "start": 1577878800000,
    "duration": 45,
    "room": "K.3.401",
    "title": "Optimizing rav1e",
    "subtitle": "Effective profiling techniques and optimization strategies",
    "track": "Rust",
    "abstract": "<p>rav1e is a fast AV1 encoder written in rust (and plenty of assembly), released monthly.</p>\n\n<p>Since the 0.1.0 release we try to make sure we provide an adequate speed or quality boost compared to the previous.</p>\n\n<p>This talk is about what tools are available in the rust ecosystem and what are the practices that worked best for us.</p>",
    "description": "<p>The presentation will touch the following topics:\n- Exploring a codebase and profiling it effectively, both for cpu usage and memory usage.\n- Which are the optimization strategies that worked for us better (critical path analysis vs peak consumer)\n- Benchmarking and tracing</p>\n\n<p>I'll provide examples on what tools worked well or not so well and what currently I consider the best and most promising tools for each tasks.</p>",
    "persons": [
      "Luca Barbato"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 25,
    "room": "K.3.401",
    "title": "Rust techniques used in sled",
    "subtitle": "A correctness-critical and performant Rust codebase",
    "track": "Rust",
    "abstract": "<p>The sled high-performance embedded database has been under development for over 4 years. This talk will cover techniques that have been used to dramatically outperform existing databases while optimizing for long term developer happiness in a complex, correctness-critical Rust codebase.</p>",
    "description": "",
    "persons": [
      "Tyler Neely"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 45,
    "room": "K.3.401",
    "title": "RedisJSON",
    "subtitle": "A document DB in Rust",
    "track": "Rust",
    "abstract": "<p>Over the last decade, Redis has become one of the most popular NoSQL DBs delivering on the promise of high throughput and low latency. What started as a pure C code base is gradually being augmented with Rust due to the trifecta of safety, concurrency, and speed. A primary example is thre RedisJSON module which turns Redis into a document DB.</p>\n\n<p>The talk outlines the principal architecture of the re-implementation of RedisJSON, the challenges encountered and the solutions for these. The focus is on the practical aspects rather than conveying theoretical knowledge. A comparison with other open source document DB concludes this presentation, concentrating on latency and throughput aspects.</p>",
    "description": "",
    "persons": [
      "Christoph Zimmermann"
    ]
  },
  {
    "start": 1577886600000,
    "duration": 25,
    "room": "K.3.401",
    "title": "Sharing memories of Python and Rust",
    "subtitle": "The story of a lifetime inside Mercurial",
    "track": "Rust",
    "abstract": "<p>The Mercurial version control system now has an option for running Rust code to improve performance. In this talk we will explore the challenges of using Rust efficiently inside a performance-sensitive Python project with more than 10 years of backwards compatiblity.</p>",
    "description": "",
    "persons": [
      "Raphaël Gomès"
    ]
  },
  {
    "start": 1577888400000,
    "duration": 45,
    "room": "K.3.401",
    "title": "Building WebGPU with Rust",
    "subtitle": "The new foundation for graphics and compute",
    "track": "Rust",
    "abstract": "<p>WebGPU is a new graphics and compute API designed from the ground up by a W3C community group. It's a successor to WebGL, which also has a chance to become a standard on native platforms. We are going to talk about the API itself as well as our Rust-based implementation \"wgpu\".</p>",
    "description": "<p>Expected prior knowledge / intended audience: basic familiarity with one of the graphics APIs is good but not required.\nWe'll explain in details how this is different from existing APIs.</p>\n\n<p>\"wgpu\" is the native implementation of this API in Rust, running on top of Vulkan, Metal, D3D12, D3D11, and potentially OpenGL.\nThis is a talk about the API architecture being designed as well as our implementation of it.\nWe want to share the experience of leveraging the power of Rust ecosystem and language in order to build this level of abstraction.\nWe'll show a few demos and spread excitement about the new API.</p>\n\n<p>The talk is related to the Fosdem 2018 talk about gfx-rs: the old talk mentioned WebGPU as one of the clients for the low-level abstraction.</p>",
    "persons": [
      "Dzmitry Malyshau"
    ]
  },
  {
    "start": 1577891400000,
    "duration": 25,
    "room": "K.3.401",
    "title": "Progress of Rust and WASM in 2019",
    "subtitle": "The year in review",
    "track": "Rust",
    "abstract": "<p>There was a huge progress in Rust tools for WebAssembly in the last year. Let's review some of the most noticeable changes. The talk is mostly about wasm-bindgen.</p>",
    "description": "",
    "persons": [
      "Ilya Baryshnikov"
    ]
  },
  {
    "start": 1577893200000,
    "duration": 25,
    "room": "K.3.401",
    "title": "Rustifying the Virtual Machine Introspection ecosystem",
    "subtitle": "Why Rust is the best language for introspection agents in the future",
    "track": "Rust",
    "abstract": "<p>From stealth malware analysis to OS hardening through fuzzing, virtual machine\nintrospection is expanding the possibilities offered by our hypervisors,\nshifting our view of virtual machines, from opaques containers to fully\ntransparent and instrumentable systems.</p>\n\n<p>Today the VMI ecosystem is made of a multitude of applications, targeting one\nhypervisor or emulator, with their own semantic library. (Examples includes\nDrakvuf, PANDA, PyREBox, icebox, etc...). If we want to make the most out of VMI\nin the future, we need to build the libraries that will unify this ecosystem and\nlet the developers focus on what matters: building quality VMI apps.</p>\n\n<p>This is where libmicrovmi comes into play. It aims to solve this problem, by\nproviding a core, foundation library, written in Rust, to be cross-platform,\nhypervisor-agnostic and emulator-agnostic, on top of which higher-level\nlibraries and apps can rebase.</p>\n\n<p>Rust makes a lot of sense for VMI for 2 main reasons:</p>\n\n<ul>\n<li>Rust is safe: considering that we are processing untrusted input from virtual\nmachines, we cannot allow any crash or exploitation in the introspection\nagent. Also one of our use case is OS hardening, which needs an excellent\nlevel of trust</li>\n<li>Rust is fast: processing an event requires to pause the VCPU. The longer the\npause, the more delayed the guest execution will be, and when scaling to\nthousands of events per second this can dramatically influence how many breakpoints\nyou are willing to put, especially on production systems. Speed matters.</li>\n</ul>\n\n\n<p>Therefore Rust is the de facto choice for VMI apps in the future, and we are\nbuilding it today, by providing libmicrovmi, a new foundation for VMI.</p>\n\n<p>Libmicrovmi has drivers for:</p>\n\n<ul>\n<li>Xen</li>\n<li>KVM</li>\n<li>Hyper-V (in progress)</li>\n</ul>",
    "description": "<h1>What is VMI ?</h1>\n\n<p><em>Vrtual Machine Introspection</em> is a concept born in a 2003 research paper titled\n\"A Virtual Machine Introspection Based Architecture for Intrusion Detection\".\nThe idea resides in inspecting and understanding the real-time high-level state\nof a virtual machine, based on the hardware layer, for security purposes.</p>\n\n<p>Since then the technology has made its way, from research and academic\ndevelopments to being fully integrated and supported into mainstream\nhypervisors, like Xen.</p>\n\n<h1>What are the use-cases ?</h1>\n\n<p>The initial population who adopted VMI has been malware sandbox providers. Since\ncommon malware had a tendency to hide from debuggers, the level of stealth\nreached with this technology made it perfectly suited for this job, alongside\nthe full system view.</p>\n\n<p>Today VMI has grown to be applied in various domains:</p>\n\n<ul>\n<li>Debugging</li>\n<li>Malware Analysis</li>\n<li>Live Memory Analysis</li>\n<li>OS Hardening</li>\n<li>Fuzzing</li>\n</ul>\n\n\n<h1>What is the state of the technology today ?</h1>\n\n<p>As of today, Xen is the leading hypevisor, haivng VMI APIs since 2011. And since\n2017, both KVM and VirtualBox have patches available, and even reviewed on the\nmailing list for KVM.</p>\n\n<p>Regarding the libraries available, LibVMI stands out, as it provides a unified,\nhypervisor-agnostic, VMI API to applications, and a well-known malware analysis\nframework (Drakvuf) is based on it.</p>\n\n<p>However, most of VMI applications today do not share the same common set of core\nlibraries, which makes the ecosystem fragmented and hard to deal with, where a lot\nof efforts is spent solving the same problems everyone has, isolated by their\nown stacks.</p>\n\n<h1>Why Rust ?</h1>\n\n<p>This is where Rust comes into play. The language itself combines 3 important features:</p>\n\n<ul>\n<li>Safety: new VMI applications have a focus on OS hardening, rebasing your trust\non an introspection agent to avoid a kernel compromise is a huge deal,\nespecially when the agent has high-privileges.</li>\n<li>Speed: the amount of hardware events that can you can handle per second will\ndefine how much impact your agent has on the guest execution. This has to be\nkept as low as possible, otherwise the technology's adoption won't go further\nthan private malware analysis systems.</li>\n<li>Cross-platform: Rust's build system and standard library allow to effortlessly\nbuild a cross-plaform library, which is a requirements to bring developers\nusing KVM, Hyper-V or even VirtualBox to share the same library.</li>\n</ul>\n\n\n<p>Building this core library that will unify the ecosystem is the goal of libmicrovmi.</p>\n\n<h1>Related work</h1>\n\n<p>I have been building a hypervisor-level debugger, based on LibVMI. It can\nintrospect a Windows guest and debug a specific process, while providing a GDB\nstub to be plugged into your favorite GDB fronted (IDA, radare2, etc ....)</p>\n\n<h1>Expected knowledge</h1>\n\n<p>The audience will need a bit of familiarity with virtualization concepts, this\nwill be enough to understand the idea of introspection.</p>\n\n<p>They can be totally new to Rust, as I once was a few months ago.</p>",
    "persons": [
      "Mathieu Tarral"
    ]
  },
  {
    "start": 1577895000000,
    "duration": 45,
    "room": "K.3.401",
    "title": "zbus: yet another D-Bus library",
    "subtitle": "The why, how & WTH of creating a pure D-Bus Rust crate",
    "track": "Rust",
    "abstract": "<p>In this talk, I will present zbus, a D-Bus crate written from scratch. D-Bus is an inter-process communication mechanism, available and used on almost all modern Linux desktops and many embedded systems. I will start with why I felt the need to take this huge undertaking on my shoulders, followed by the design goals, the challenges faced and how I overcame them during the development.</p>",
    "description": "",
    "persons": [
      "Zeeshan Ali"
    ]
  },
  {
    "start": 1577873100000,
    "duration": 25,
    "room": "K.4.201",
    "title": "Low-end platform profiling with HawkTracer profiler",
    "subtitle": "",
    "track": "Debugging Tools",
    "abstract": "<p>HawkTracer is low-overhead instrumentation-based profiler built at Amazon Video for platforms with limited capabilities. It's written in C but can be used almost with any other language (we've successfully used it with JavaScript, LUA and Python). It's highly extensible (at compile time) and portable so it can be run on almost any embedded device. In this talk I'll introduce the architecture of the profiler, present it's advantages and limitations, show how can you instrument the code and demonstrate the profiler in action by running it with an example cross-language (C++ and Python) project.</p>",
    "description": "",
    "persons": [
      "Marcin Kolny"
    ]
  },
  {
    "start": 1577874900000,
    "duration": 30,
    "room": "K.4.201",
    "title": "GDB pipelines -- convenience iteration over inferior data structures",
    "subtitle": "Bringing MDB's \"walkers\" to GDB",
    "track": "Debugging Tools",
    "abstract": "<p>We introduce a GDB plugin for working with large data structures in the inferior.</p>\n\n<p>This plugin brings some of the flexibility of Unix pipelines to the GDB command prompt, providing the ability to <em>conveniently</em> run some action on every element in a data structure that matches certain criteria.</p>\n\n<p>One big aim of this plugin is to make it easy and convenient for a user to write their own sub-commands to iterate over the data structures used in their own program.</p>\n\n<p>This is intended for anyone who has found difficulty inspecting large data structures from inside GDB.</p>",
    "description": "<p>MDB -- the debugger on Solaris -- has a feature called \"walkers\" that is used to great effect when inspecting the contents of large data structures in the Solaris Kernel.</p>\n\n<p>We introduce a GDB plugin to provide the same type of functionality.</p>\n\n<p>Similar to Unix pipelines, one can now flexibly write a surprisingly powerful command by combining several \"walkers\".</p>\n\n<p>Some examples are:\n- Search an inferior data structure for nodes that are malformed.</p>\n\n<pre><code> gdb-pipe &lt;mywalker&gt; &lt;startnode&gt; | if ! &lt;some verification test&gt;\n</code></pre>\n\n<ul>\n<li><p>Put breakpoints on member functions of those nodes in a data structure matching some criteria.\n   gdb-pipe &lt;mywalker> &lt;startnode> | if &lt;interesting-check> | show break {}->function-member</p></li>\n<li><p>Put breakpoints on a given function when passed a node that matches some criteria.\n   gdb-pipe &lt;mywalker> &lt;startnode> | if &lt;interesting-check> | show break somefunc if &lt;arg> == {}</p></li>\n</ul>\n\n\n<p>This plugin has a strong aim to make it easy for users to write \"walkers\" over their own data structures, and already has \"walkers\" for the open source projects \"neovim\" and \"GCC\".</p>\n\n<p>We would like to discuss possible future directions for this plugin with regards to speed improvements to work on extremely large data structures, and how there could be a tie-in with pretty-printers.</p>",
    "persons": [
      "Matthew Malcomson"
    ]
  },
  {
    "start": 1577877000000,
    "duration": 30,
    "room": "K.4.201",
    "title": "The GDB Text User Interface",
    "subtitle": "",
    "track": "Debugging Tools",
    "abstract": "<p>GDB has had a curses-based interface for many years.  Come see what new features are available and how it can improve your debugging experience.</p>",
    "description": "<p>This talk will cover GDB's text user interface (the \"TUI\").  In particular we'll discuss the benefits of the rewrite, the new features that are available, and how you can easily extend it yourself.  A fun demo will be included.</p>",
    "persons": [
      "Tom Tromey"
    ]
  },
  {
    "start": 1577879100000,
    "duration": 40,
    "room": "K.4.201",
    "title": "Memcheck Reloaded",
    "subtitle": "dealing with compiler-generated branches on undefined values",
    "track": "Debugging Tools",
    "abstract": "<p>Valgrind's Memcheck tool reports various kinds of errors. One of the most important are those where an if-condition or a memory address uses undefined data.  Detecting that reliably on optimized code is challenging, and recent compiler development has made the problem worse.</p>",
    "description": "<p>Two years ago, at FOSDEM 2018, I did a talk describing the techniques Memcheck uses to achieve a very low false positive rate.  But by 2018 both GCC and Clang were routinely emitting code with branches on uninitialised data. Surprisingly, there are situations where such code is correct.  Unfortunately Memcheck assumes that every conditional branch is important and so emits many complaints when this happens.</p>\n\n<p>The worst thing was, this problem couldn't be solved using the bag of tricks we'd accumulated over Memcheck's decade-plus lifetime.  Our options didn't look good.  But in early 2019 it became clear how to fix this: enhance Valgrind's trace generation machinery to analyse more than one basic block at a time, and use that to recover the source-level &amp;&amp;-expressions, which can then be instrumented precisely.  This talk tells the story.</p>\n\n<p>The implementation (appears to!) work.  If all goes well, it will ship in the upcoming 3.16 release.</p>",
    "persons": [
      "Julian Seward"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 30,
    "room": "K.4.201",
    "title": "Modern strace",
    "subtitle": "",
    "track": "Debugging Tools",
    "abstract": "<p>strace is a diagnostic, debugging and instructional utility for Linux. It is used to monitor and tamper with interactions between processes and the Linux kernel, which include system calls, signal deliveries, and changes of process state. In this talk the maintainer of strace will describe new features implemented since FOSDEM 2018.</p>",
    "description": "<p>Several interesting features were implemented within strace project since FOSDEM 2018, including:</p>\n\n<ul>\n<li>seccomp-assisted system call filtering</li>\n<li>system call return status filtering</li>\n<li>PTRACE_GET_SYSCALL_INFO API support</li>\n<li>new options: -DD, -DDD, -X, -z, -Z</li>\n</ul>\n\n\n<p>In this talk the maintainer of strace will describe these new features and demonstrate what kinds of problems they help to solve.</p>",
    "persons": [
      "Dmitry Levin"
    ]
  },
  {
    "start": 1577883900000,
    "duration": 30,
    "room": "K.4.201",
    "title": "strace: fight for performance",
    "subtitle": "",
    "track": "Debugging Tools",
    "abstract": "<p>The talk gives an overview of various optimisations implemented in strace over the past several years. While most of them are quite trivial (like caching of frequently-used data or avoiding syscalls whenever possible), some of them are a bit more tricky (like usage of seccomp BPF programs for avoiding excessive ptrace stops) and/or target more specific use cases (like the infamous thread queueing patch[1], which was carried as a RHEL downstream patch for almost 10 years).</p>\n\n<p>[1] https://gitlab.com/strace/strace/commit/e0f0071b36215de8a592bf41ec007a794b550d45</p>",
    "description": "",
    "persons": [
      "Eugene Syromyatnikov"
    ]
  },
  {
    "start": 1577886000000,
    "duration": 30,
    "room": "K.4.201",
    "title": "strace --seccomp-bpf: a look hunder the hood",
    "subtitle": "",
    "track": "Debugging Tools",
    "abstract": "<p>strace is known to add significant overhead to any application it traces.\nEven when users are interested in a handful of syscalls, strace will by\ndefault intercept all syscalls made by the observed processes, involving\nseveral context switches per syscall.  Since strace v5.3, the\n<code>--seccomp-bpf</code> option allows reducing this overhead, by stopping observed\nprocesses only at syscalls of interest.  This option relies on seccomp-bpf\nand inherits a few of its limitations.</p>\n\n<p>In this talk, we will describe the default behavior of ptrace and strace,\nto understand the problem <code>--seccomp-bpf</code> addresses.  We will then detail\nthe inner workings of the new option, as seen from ptrace (seccomp-stops)\nand bpf (syscall matching algorithms).  Finally, we'll discuss limitations\nof the new option and avenues for improvement.</p>",
    "description": "<ul>\n<li>Problem addressed and ptrace default behavior</li>\n<li>seccomp-bpf, <code>SECCOMP_RET_TRACE</code>, and the new behavior</li>\n<li>cBPF syscall matching algorithms</li>\n<li>Main limitations: working together with <code>-p</code> and <code>-f</code></li>\n<li>Avenues for improvements</li>\n</ul>\n\n\n<p>Part of this talk is covered in the following blog post:\n<a href=\"https://pchaigno.github.io/strace/2019/10/02/introducing-strace-seccomp-bpf.html\">https://pchaigno.github.io/strace/2019/10/02/introducing-strace-seccomp-bpf.html</a>.</p>",
    "persons": [
      "Paul Chaignon"
    ]
  },
  {
    "start": 1577888100000,
    "duration": 40,
    "room": "K.4.201",
    "title": "Tools and mechanisms to debug BPF programs",
    "subtitle": "",
    "track": "Debugging Tools",
    "abstract": "<p>By allowing to safely load programs from user space and to execute them in the kernel, eBPF (extended Berkeley Packet Filter) has brought new possibilities to the Linux kernel, in particular in terms of tracing and network processing.</p>\n\n<p>But when a program fails to load, or when it does not return the expected values, what tools do we have to examine, inspect and debug eBPF objects? This talk focuses on the different tools and mechanisms available to help eBPF developers debug their programs, at the different stages of the workflow. From bpftool to test runs, let's find the best way to track bugs!</p>",
    "description": "<p>I am not sure how long the time slots for the debugging devroom are, so the\ncontent would be adapted according to the duration of the talk. The idea is to:</p>\n\n<ol>\n<li>Very briefly introduce the existing tools/mechanisms for debugging BPF\nprograms at each stage of the workflow</li>\n<li>Spend some time on a more in-depth introduction to bpftool, which can be\nused to perform a variety of operations on eBPF programs, maps, or BTF\nobjects</li>\n<li>Mention leads for future work</li>\n</ol>\n\n\n<p>In more details, this would look like:</p>\n\n<ol>\n<li>Different tools to debug</li>\n</ol>\n\n\n<p>1.1 Compilation time (make sure program is generated as intended)</p>\n\n<ul>\n<li>LLVM backend</li>\n<li>llvm-objdump</li>\n<li>eBPF assembly</li>\n</ul>\n\n\n<p>1.2 Load time / Verifier / JIT compile time (make sure program loads successfully)</p>\n\n<ul>\n<li>libbpf / ip / tc</li>\n<li>verifier logs, kernel logs, extack messages</li>\n<li>Documentation</li>\n<li>bpftool</li>\n</ul>\n\n\n<p>1.3 Runtime (make sure program return as expected)</p>\n\n<ul>\n<li>bpftool</li>\n<li>bpf<em>trace</em>printk() helper / perf events</li>\n<li>(user space BPF machines)</li>\n<li>BPF<em>PROG</em>TEST_RUN</li>\n<li>bpftool prog run</li>\n<li>perf annotations</li>\n<li>BTF debug information</li>\n<li>BPF trampolines: spy on BPF with BPF</li>\n</ul>\n\n\n<p>1.4 User space (develop programs that manipulate BPF objects)</p>\n\n<ul>\n<li>strace, valgrind support for bpf()</li>\n<li>probing kernel with bpftool</li>\n<li><p>using tools to generate BPF (bcc, bpftrace, libkefir, ...)</p></li>\n<li><p>bpftool introduction (with examples)</p></li>\n<li><p>listing objects</p></li>\n<li>loading programs, attaching programs, creating maps</li>\n<li>performing “test runs”</li>\n<li>probing kernel for existing features</li>\n<li><p>taking over the world</p></li>\n<li><p>In the future?</p></li>\n<li><p>checking a program loads: bpftool prog probe object_file.o</p></li>\n<li>debugger: break points, dump for registers/stack/context?\n(Complete the support (hooks exist) in GDB? Extend BPF<em>PROG</em>TEST_RUN\ninfrastructure?)</li>\n</ul>",
    "persons": [
      "Quentin Monnet"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 15,
    "room": "K.4.201",
    "title": "Support for mini-debuginfo in LLDB",
    "subtitle": "Debugging without installing debug-symbols, or how to read the .gnu_debugdata section.",
    "track": "Debugging Tools",
    "abstract": "<p>The \"official\" mini-debuginfo man-page describes the topic best:</p>\n\n<blockquote><p>Some systems ship pre-built executables and libraries that have a\nspecial <code>.gnu_debugdata</code> section. This feature is called MiniDebugInfo.\nThis section holds an LZMA-compressed object and is used to supply extra\nsymbols for backtraces.</p>\n\n<p>The intent of this section is to provide extra minimal debugging information\nfor use in simple backtraces. It is not intended to be a replacement for\nfull separate debugging information (see Separate Debug Files).</p></blockquote>\n\n<p>In this talk I'll explain what it took to interpret support for mini-debuginfo\nin LLDB, how we've tested it, and what to think about when implementing this\nsupport (e.g. merging <code>.symtab</code> and <code>.gnu_debugdata</code> sections).</p>",
    "description": "<p>If the <code>.symtab</code> section is stripped from the binary it might be that\nthere's a <code>.gnu_debugdata</code> section which contains a smaller <code>.symtab</code> in\norder to provide enough information to create a backtrace with function\nnames or to set and hit a breakpoint on a function name.</p>\n\n<p>My change looks for a <code>.gnu_debugdata</code> section in the ELF object file.\nThe <code>.gnu_debugdata</code> section contains a xz-compressed ELF file with a\n<code>.symtab</code> section inside. Symbols from that compressed <code>.symtab</code> section\nare merged with the main object file's <code>.dynsym</code> symbols (if any).\nIn addition we always load the <code>.dynsym</code> even if there's a <code>.symtab</code>\nsection.</p>\n\n<p>For example, the Fedora and RHEL operating systems strip their binaries\nbut keep a <code>.gnu_debugdata</code> section. While gdb already can read this\nsection, LLDB until my patch couldn't. To test this patch on a\nFedora or RHEL operating system, try to set a breakpoint on the \"help\"\nsymbol in the \"zip\" binary. Before this patch, only GDB can set this\nbreakpoint; now LLDB also can do so without installing extra debug\nsymbols:</p>\n\n<pre><code>lldb /usr/bin/zip -b -o \"b help\" -o \"r\" -o \"bt\" -- -h\n</code></pre>\n\n<p>The above line runs LLDB in batch mode and on the \"/usr/bin/zip -h\"\ntarget:</p>\n\n<pre><code>(lldb) target create \"/usr/bin/zip\"\nCurrent executable set to '/usr/bin/zip' (x86_64).\n(lldb) settings set -- target.run-args  \"-h\"\n</code></pre>\n\n<p>Before the program starts, we set a breakpoint on the \"help\" symbol:</p>\n\n<pre><code>(lldb) b help\nBreakpoint 1: where = zip`help, address = 0x00000000004093b0\n</code></pre>\n\n<p>Once the program is run and has hit the breakpoint we ask for a\nbacktrace:</p>\n\n<pre><code>(lldb) r\nProcess 10073 stopped\n* thread #1, name = 'zip', stop reason = breakpoint 1.1\n    frame #0: 0x00000000004093b0 zip`help\nzip`help:\n-&gt;  0x4093b0 &lt;+0&gt;:  pushq  %r12\n    0x4093b2 &lt;+2&gt;:  movq   0x2af5f(%rip), %rsi       ;  + 4056\n    0x4093b9 &lt;+9&gt;:  movl   $0x1, %edi\n    0x4093be &lt;+14&gt;: xorl   %eax, %eax\n\nProcess 10073 launched: '/usr/bin/zip' (x86_64)\n(lldb) bt\n* thread #1, name = 'zip', stop reason = breakpoint 1.1\n  * frame #0: 0x00000000004093b0 zip`help\n    frame #1: 0x0000000000403970 zip`main + 3248\n    frame #2: 0x00007ffff7d8bf33 libc.so.6`__libc_start_main + 243\n    frame #3: 0x0000000000408cee zip`_start + 46\n</code></pre>\n\n<p>In order to support the <code>.gnu_debugdata</code> section, one has to have LZMA\ndevelopment headers installed. The CMake section, that controls this\npart looks for the LZMA headers and enables <code>.gnu_debugdata</code> support by\ndefault if they are found; otherwise or if explicitly requested, the\nminidebuginfo support is disabled.</p>\n\n<p>GDB supports the \"mini debuginfo\" section <code>.gnu_debugdata</code> since v7.6\n(2013).</p>",
    "persons": [
      "Konrad Kleine"
    ]
  },
  {
    "start": 1577892000000,
    "duration": 40,
    "room": "K.4.201",
    "title": "The elfutils debuginfod server",
    "subtitle": "",
    "track": "Debugging Tools",
    "abstract": "<p>Debugging data is a necessary evil. It is necessary for running debuggers in situ, some tracing tools, or for coredump analysis. It is evil because it is big - potentially many times the size of the binaries. Therefore, software distributions have conflicting needs to generate &amp; keep this data but not burden everyone with its storage.</p>\n\n<p>We will review some degrees of freedom for debugging data distribution, across compiled languages and OS distributions, identifying some of the best practices. We will identify the remaining shortcomings that necessitate exploring yet another way of making debuginfo data available needed.</p>\n\n<p>We will present the elfutils debuginfo-server prototype, where a web service offers a lightweight, build-id-indexed lookup of debuginfo-related data on demand. This service is designed to run on a nearby host, private or shared within teams, or even by OS distributions. Clients built into various debugging type tools will be demonstrated.</p>",
    "description": "",
    "persons": [
      "Mark Wielaard",
      "Frank Ch. Eigler"
    ]
  },
  {
    "start": 1577894700000,
    "duration": 30,
    "room": "K.4.201",
    "title": "Debugging apps running in Kubernetes",
    "subtitle": "An overview of the tooling available",
    "track": "Debugging Tools",
    "abstract": "<p>New tools are coming out to make it possible to add breakpoints and debug running code in a Kubernetes Pod. This talk will present an overview of some of these tools. We'll cover tools that make it easy to update the code that's running in a Pod (Skaffold/Ksync/Telepresence). And we'll also cover how to connect your IDE to the code and set breakpoints.</p>",
    "description": "<ul>\n<li>A really brief, high level overview of k8s pods &amp; how to run an application there</li>\n<li>List out some the tools available that make debugging possible</li>\n<li>Overview of how each tool differs</li>\n<li>A demo of 2 different debugging tools</li>\n</ul>",
    "persons": [
      "Jeff Knurek"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 25,
    "room": "K.4.401",
    "title": "BSP generator for 3000+ ARM microcontrollers",
    "subtitle": "",
    "track": "Hardware Enablement",
    "abstract": "<p>For embedded developers using alternative programming languages, but also for anyone using third party driver frameworks such as libopencm3, one of the main pain points to start using a microcontroller is to make a Board Support Package.</p>\n\n<p>Things like linker script or startup code (crt0) not only require skills, but also information that are not always easily accessible.</p>\n\n<p>In this talk we will present a tool that generates linker script, startup code, and low level hardware binding for 3000+ ARM microcontrollers using information extracted from archives provided as part of the ARM Cortex Microcontroller Software Interface Standard (CMSIS).</p>",
    "description": "",
    "persons": [
      "Fabien Chouteau"
    ]
  },
  {
    "start": 1577871000000,
    "duration": 25,
    "room": "K.4.401",
    "title": "On-hardware debugging of IP cores with free tools",
    "subtitle": "",
    "track": "Hardware Enablement",
    "abstract": "<p>An approach to challenges of an on-FPGA debugging of IP cores based on\nfree software tools is demonstrated. Various aspects and related problems\nof an on-hardware debugging are presented along with the tools to\naddress them, such as OpenOCD, sigrok/PulseView, GHDL, etc.  Real-life\nworking configuration and missing bits of software are accompanied by\nthe live debug session demo running on Open-source Hardware.</p>",
    "description": "<p>Debugging of hardware blocks on an FPGA is always challenging and may\nbe frustrating, especially with no reliable tools at hands. Way too\noften the process turns into developing and debugging of the tools,\ninstead of a target design.</p>\n\n<p>Commercial solutions are available (SignalTap, ChipScope, Synopsys\nIdentify RTL Debugger, MicroSemi Smart Debug), at the same time there\nare a lot of well known problems associated with them: vendor lock,\nsingle target, closed source and not always flexible enough, license\nterms and costs.</p>\n\n<p>Owing to free software developers essential tools for\non-hardware debugging of IP cores are available today.  However there\nare problems associated with these tools too.  Among the most notable\nones are a weak integration between separate tools and small bits of\ncode and config files missing here and there. A working combination of\ntools along with explanations of how they may be used together to\ndebug IP cores is provided.  A presentation covers such free\nsoftware as GHDL, sigrok/PulseView, and OpenOCD.  Source code of free\nIP cores, all configuration and script files and presentation slides\nwill be available in a dedicated repository on github.</p>\n\n<p>A live demonstration of the PulseView connected to an in-FPGA\nlogic analyzer via JTAG interface and working in parallel with a gdb\ndebug session on a RISC-V soft-core CPU in the same FPGA with an\nopen and low-cost hardware will be presented.</p>\n\n<p>An outline of the open tasks and possible future development\ndirections concludes the presentation.</p>",
    "persons": [
      "Anton Kuzmin"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 25,
    "room": "K.4.401",
    "title": "Continuous Integration for Open Hardware Projects",
    "subtitle": "",
    "track": "Hardware Enablement",
    "abstract": "<p>While it is standard to deploy every single code commit using CI systems and deploy new code automatically we are only at the beginning of automation for designing hardware. In this talk I will share the experience with continuous integration tools in FOSSASIA hardware projects, and specifically our Pocket Science Lab. I will outline opportunities and challenges for implementing CI processes for hardware.</p>",
    "description": "<p>While it is standard to deploy every single code commit using CI systems and deploy new code automatically we are only at the beginning of automation for designing hardware. In this talk I will share the experience with continuous integration tools in FOSSASIA hardware projects, and specifically our Pocket Science Lab. I will outline opportunities and challenges for implementing CI processes for hardware.</p>\n\n<p>With PSlab apart from the firmware we have connected CI processes to our hardware repository. This means each design change can be automatically build into a digital prototype. Electronics materials are largely standardized and with KiCad we are even able to create package lists and Gerber files automatically. Furthermore we deploy to Kitspace using a yaml file. Here any user can order all components and the board through a one-click process. Every version could easily be built here.</p>",
    "persons": [
      "Mario Behling"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 25,
    "room": "K.4.401",
    "title": "Open Source Firmware Testing at Facebook",
    "subtitle": "If you don't test your firmware, your firmware fails you",
    "track": "Hardware Enablement",
    "abstract": "<p>We talked extensively about LinuxBoot, a Linux-based environment intended to be integrated into the firmware on the boot ROM. This time we want to talk about how do we test LinuxBoot before it goes to production. We will talk about ConTest, an open-source continuous and on-demand system testing framework that we designed to be modular, validating, and infrastructure-agnostic, and how it is helping us validate open source firmware on our datacenter platforms.</p>",
    "description": "<p>With LinuxBoot we became vendors of our own system firmware. In order to go to production we need a reliable quality assurance process, and firmware testing was a necessity. In this talk we are presenting ConTest (short for Continuous Testing), a modular framework aimed at automating system testing workflows, like firmware validation and provisioning. ConTest has several goals in mind: being open source and community-driven; validate as much as possible at compile time and at job submission time, to minimize unnecessary operations and run-time failures; being lightweight and infrastructure-agnostic, so it can run in Facebook’s datacenters as well as on a Raspberry Pi; being composable, thanks to an interface-and-plugins architecture; being user-oriented so that it’s not necessary to know the internals to use it effectively; and being metrics and events driven, so that users can gain valuable insights about their jobs, more than just success rate (e.g. micro-benchmarking and trend analysis).</p>\n\n<p>ConTest is aimed at anyone who need to automate system-level testing. Various plugins are provided out of the box, with examples on how to use them. The users can combine them like building blocks using a simple job description format based on JSON, and test scenarios of variable complexity. When default plugins are not enough, for example in order to talk to a custom service, users can develop new plugins, and plug them just like if they were part of the core framework. Open-sourcing your own plugins is always appreciated!</p>",
    "persons": [
      "Andrea Barberio"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 55,
    "room": "K.4.401",
    "title": "FreedomEV 2.0",
    "subtitle": "From Tesla Hacking to a free mobility stack",
    "track": "Hardware Enablement",
    "abstract": "<p>Last year the FreedomEV project was launched as an addon to the existing software for the older Tesla Model S and Tesla Model X cars.</p>\n\n<p>FreedomEV envisions a future without mass surveillance, with full control over our own car.</p>\n\n<p>Today we launch FreedomEV 2.0.\nMore features, more supported cars and where we are now.</p>\n\n<p>Let us play with our car!</p>",
    "description": "",
    "persons": [
      "Jasper Nuyens"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 25,
    "room": "K.4.401",
    "title": "A free toolchain for 0.01 € - computers",
    "subtitle": "The free toolchain for the Padauk 8-bit microcontrollers",
    "track": "Hardware Enablement",
    "abstract": "<p>The Taiwanese company Padauk makes small 8-bit microcontrollers, the smallest of which are available at 0.01 € even in small quantities. Even the larger ones are just a few cents; a particularly interestign feature is the hardware multithreading support available in larger devices.\nUntil recently, the only available toolchain was Padauk's non-free toolchain based around their \"MINI-C\" IDE (which despite, the name, ist just a bit of C-like syntactic sugar coating for assembler, and in no way a C implementation).</p>\n\n<p>In 2019, an effort to provide a free alternative resulted in a full free toolchain. Documentation beyond that provided by Padauk was created by reverse-engineering. A free design for a programmer along with firmware was created. Assembler, simulator and a Small Device C Compiler (SDCC) backend were written.</p>",
    "description": "<p>The Padauk microcontrollers (µC) are small 8-bit Systems-on-a-Chip (SoC). Their program memory (PROM or Flash) is 0.5 KW to 4 KW with word sizes from 13 to 16 bit. Their data memory is 60 B to 256 B with 8-bit Bytes. The µC only have few peripherals; however, some of the larger devices are barrel processors with support for up to 8 hardware threads. This allows the emulation of even timing-critical peripherals in software. Padauk also supplies a programmer and a non-free \"MINI-C\" IDE. There is a lack of documentation when it comes to aspects not needed for users of MINI-C. In particular, there is no documentation of opcodes and very little information on the programming protocol.\nThere are 4 subarchitectures, which we name by the word size in the program memory pdk13, pdk14, pdk15 and pdk16. There is some variation in the form of optional instructions within the subarchitectures.</p>\n\n<p>In 2019, a full free toolchain for these µC was created.\nThe necessary documentation was reverse-engineered. A free programmer design and firmware was created. For the pdk13, pdk14 and pdk15, we wrote free assemblers and simulators. We also wrote an SDCC backend for these. SDCC is a free C compiler that emphasizes standard-compliance and generating efficient code for small devices; while not up to the level of GCC and LLVM it tends to hold up well against many non-free compilers targetting small devices (see e.g. the FOSDEM 2018 talk \"The free toolchain for the STM8\"). While stack handling on the Padauk µC is much better than on e.g. small Mirochip PIC devices, it is still not efficient (in particular, there is no stack-pointer-relative addressing mode). Thus SDCC does not place local variables on the stack by default, which makes functions non-reentrant and is not standard-compliant, but a common choice in such cases (see e.g. the mcs51 backend in SDCC or Keil for MCS-51). However, this approach does not work well for devices with hardware-multithreading (i.e. few pdk14 and all pdk16).</p>",
    "persons": [
      "Philipp Klaus Krause"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 25,
    "room": "K.4.401",
    "title": "Status of AMD platforms in coreboot",
    "subtitle": "",
    "track": "Hardware Enablement",
    "abstract": "<p>The presentation is about AMD's involvement in coreboot evolution and development. Gives a high-level overview of the engagement of the silicon vendor in the coreboot project history. The presentation may contain a little bit of technical aspects of firmware and BIOS. However, the intended audience is not only firmware and BIOS developers, but also free and libre hardware enthusiasts too. If anybody is interested in the future of famous platforms like Asus KGPE-D16, Lenovo G505S, PC Engines apu1/apu2, please attend the presentation.</p>",
    "description": "<p>The history of AMD cooperation in coreboot projects reaches 2007 where the\nfirst contribution appeared for the Geode LX processors. AMD's open-source\nsupport continued for many years until now (with some break). This presentation\nwill briefly introduce the history of AMD and coreboot, the evolution of the\ncode, processors, creation of CIMX and AGESA and so on. It will also show the\ngradual change in the AMD attitude to open-source and introduction of binary\nplatform initialization. Binary blobs, very disliked by the open-source\ncommunity started to cause problems and raised the needs for workarounds to\nsupport basic processor features. Soon after that AMD stopped supporting the\ncoreboot community. Moreover, recent coreboot releases started to enforce\ncertain requirements on the features supported by the silicon code base. Aging\nplatforms kept losing interest and many of them (including fully open ones) are\nbeing dropped from the main tree. Nowadays AMD released the newest AGESA with\nthe cooperation of hired coreboot developers, but only for Google and their\nChromebooks based on Ryzen processors. These are hard times for open firmware\non AMD platforms. If you are curious about what is the present status of AMD\nboards and hardware (for example famous Asus KGPE-D16, Lenovo G505S, PC Engines\napu1/apu2) in coreboot and what future awaits them, this presentation will give\nyou a good overview.</p>",
    "persons": [
      "Michał Żygowski"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 55,
    "room": "K.4.401",
    "title": "Open Source Hardware for Industrial use",
    "subtitle": "OSHW model has benefits for SOC vendors, industrial manufacturers and end users",
    "track": "Hardware Enablement",
    "abstract": "<p>Olimex is designing Open Source Hardware Linux computers since 2012.\nThey are adopted by hundreds of manufacturers all around the world and prove the Open Source business model is sustainable.\nThe lecture is about the advantages which OSHW bring to the industrial vendors and what drives their decision to use our boards.\nWe will explain the benefits for the SOC vendors to have OSHW designs with their ICs, the end user benefits and how OSHW helps us to excel our products and make them better and better.</p>",
    "description": "",
    "persons": [
      "Tsvetan Usunov"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 55,
    "room": "K.4.401",
    "title": "FOSDEM Video Box",
    "subtitle": "A bespoke HDMI capture device for conferences.",
    "track": "Hardware Enablement",
    "abstract": "<p>A bespoke OSHW HDMI video capture solution is being developed for use at FOSDEM and other open source conferences. This talk will explain the what, why, how and hopefully when.</p>",
    "description": "<p>FOSDEM is unique in that it has 750+ talks on two days, with more than 28 parallel tracks. All are captured and streamed out live, and a sanitised version of each talk is re-encoded for separate viewing after the event. For each track, there are (at least) two capture boxes, one for the speakers' laptop (which also feeds the projector), and at least one for a camera. This means FOSDEM has close to 60 video capture boxes deployed.</p>\n\n<p>The FOSDEM video boxes are also used at several other conferences, and we are in active contact with several more, as each conference basically has the same problem to solve, albeit at a smaller scale than FOSDEM.</p>\n\n<p>Today, our capture boxes are an amalgamation of many different devices tied together, and while the whole is working surprisingly well, it is far from ideal. Some bits are non-free, some bits are hard or even impossible to control, and the result is bulky and, relatively speaking, expensive. Sourcing the exact same components again to expand the current array of boxes is nigh impossible, and the bulk of these boxes means that deployment is more of a hassle than it could be.</p>\n\n<p>To solve almost all of our issues, we are creating a bespoke HDMI capture solution by tying a HDMI-to-parallel-RGB decoder chip to the camera input of the venerable Allwinner A20 SoC. The Allwinner A20 was chosen due to its feature set, large community (linux-sunxi), its advanced upstream support and the availability of OSHW board designs. We were lucky to find a suitable board in the Olimex Lime2 (which is OSHW), which exposes all the necessary IO pins on pinheaders.</p>\n\n<p>Capturing 1280x720@50Hz, encoding it to h.264 for local storage and streaming over the network, while displaying the captured frames directly to HDMI or VGA (projector) and a status LCD, while also capturing (and passing through) audio is a challenge on this cheap, low power but open hardware. It forces us to make use of a wide array of SoC HW blocks, not all of which previously had driver support, and we are close to saturate the available memory and bus bandwidth.</p>\n\n<p>So while a good part of this talk is about describing the bigger problem we are trying to solve, this project very much is an issue of hardware enablement.</p>",
    "persons": [
      "Mark Van den Borre",
      "Luc Verhaegen",
      "Gerry Demaret"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 55,
    "room": "K.4.401",
    "title": "Using OSHW and OSS for building your custom hardware platform",
    "subtitle": "Lessons learned from building a custom hardware platform.",
    "track": "Hardware Enablement",
    "abstract": "<p>The lecture describes a journey (and bunch of bragging stories) of designing and implementing an extendable hardware platform utilizing OSHW and OSS.</p>",
    "description": "<p>We talk about:</p>\n\n<ul>\n<li>business requirements and how they affected the choice of hardware platform (Olimex Lime2)</li>\n<li>hardware bugs how they are related to tight deadlines</li>\n<li>what is actually needed to build a custom shield</li>\n<li>maintaining and administering the whole platform</li>\n</ul>",
    "persons": [
      "Priit Laes"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 55,
    "room": "K.4.401",
    "title": "Extending the lifetime of smartphones with Replicant, a fully free Android distribution",
    "subtitle": "",
    "track": "Hardware Enablement",
    "abstract": "<p>After a very quick introduction on Replicant and the smartphones ecosystem, we will look at what affects smartphones' and tablets' lifetime and how to increase it by making Replicant more sustainable.</p>",
    "description": "",
    "persons": [
      "Denis Carikli (GNUtoo)"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 50,
    "room": "K.4.601",
    "title": "seL4 Microkernel Status Update",
    "subtitle": "",
    "track": "Microkernels and Component-based OS",
    "abstract": "<p>I will give an overview of where seL4 stands today in terms of functionality, verification, ecosystem, deployment and community. The focus will be on what has happened in seL4 land over the past 12 months, which is a lot: seL4 Foundation, RISC-V support and introducing time protection.</p>",
    "description": "<p>The biggest news of the year is that we are in the process of setting up the seL4 Foundation, as an open, transparent and neutral organisation tasked with growing the seL4 ecosystem. It will bring together developers of the seL4 kernel, developers of seL4-based components and frameworks, and those deploying seL4-based systems. Its focus will be on coordinating, directing and standardising development of the seL4 ecosystem in order to reduce barriers to adoption, raising funds for accelerating development, and ensuring clarity of verification claims. I will report on the state of this.</p>\n\n<p>The other big development is that we are closing in on completing verified seL4 on the open RISC-V architecture. This includes the functional correctness proof (that guarantees that the kernel is free of implementation bugs), the binary correctness proof (which guarantees that the compiler did not introduce bugs) and the tranition to the new mixed-criticality scheduling model, which supports the safe co-location of critical real-time software with untrusted components, even if the latter can preempt the former.</p>\n\n<p>Finally, on the research side we have introduced the new concept of <em>time protection</em> (the temporal equivalent of the established memory protection) that allows us to systematically prevent information leakage through timing channels.</p>",
    "persons": [
      "Gernot Heiser"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 35,
    "room": "K.4.601",
    "title": "M³: Taking Microkernels to the Next Level",
    "subtitle": "",
    "track": "Microkernels and Component-based OS",
    "abstract": "<p>Current microkernels have shown to provide advantages in terms of security, robustness, and flexibility of systems. However, in recent years, the hardware added new challenges that need to be addressed as well, demanding approaches that include the hardware into the picture. First, hardware is getting more and more heterogeneous and consists not only of general-purpose cores, but contains also various accelerators. Second, system designers need to integrate untrusted third-party components (e.g., accelerators or modems) to meet today's performance, energy, and development-time demands. And third, security vulnerabilities such as Meltdown, Spectre, and Fallout have shown that today's complex general-purpose cores should not be trusted anymore to properly enforce isolation boundaries between different software components.</p>\n\n<p>In my talk, I will present a new system architecture that takes existing microkernel ideas to the \"next level\" to address the mentioned challenges. We use a hardware/operating system co-design consisting of a small and simple hardware component, called <em>trusted communication unit</em> (TCU), that we add next to each processing element (core, accelerator, modem, etc.) and an operating system, called <em>M³</em>, that takes advantage of it. The TCU provides a uniform interface for all processing elements, simplifying the management and usage of heterogeneous processing elements, and enables secure communication between arbitrary processing elements. M³ is designed as a microkernel-based system and runs its components on different processing elements with TCU-based communication channels between them. To account for the security vulnerabilities in today's cores, M³ places components onto different and physically isolated processing elements by default, but allows sharing of processing elements as a fallback.</p>",
    "description": "",
    "persons": [
      "Nils Asmussen"
    ]
  },
  {
    "start": 1577875200000,
    "duration": 15,
    "room": "K.4.601",
    "title": "HelenOS in the Year of the Pig",
    "subtitle": "",
    "track": "Microkernels and Component-based OS",
    "abstract": "<p>This is going to be an all-encompassing update talk for HelenOS developments that happened in the Year of the Pig (since the last FOSDEM).</p>",
    "description": "",
    "persons": [
      "Jakub Jermář"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 25,
    "room": "K.4.601",
    "title": "Linux Kernel Library",
    "subtitle": "A Library Version of Linux Kernel",
    "track": "Microkernels and Component-based OS",
    "abstract": "<p>LKL (Linux Kernel Library) is aiming to allow reusing the Linux kernel\ncode as extensively as possible with minimal effort and reduced\nmaintenance overhead.  It allows us to link the library with any\nprograms (which wish to call as a function call) containing Linux\nkernel code.  There are many use cases: reading/writing files without\ngeneral system calls, putting experimental protocol implementation\nwithout neither of host kernel update nor kernel module installation,\nusing customized kernel in container instance, building a unikernel\nbased on existing rumprun framework, or testing/fuzzing kernel\nimplementation in userspace execution, etc.</p>\n\n<p>In this talk, we are going to present</p>\n\n<ul>\n<li>why/how/when we started this project,</li>\n<li>share the outcomes using LKL</li>\n<li>fuzzing test with Linux filesystem</li>\n<li>out of tree network protocols on Android</li>\n<li>lkl.js</li>\n<li>macOS port</li>\n<li>docker integration</li>\n<li>future directions including upstreaming to Linux kernel</li>\n</ul>",
    "description": "",
    "persons": [
      "Hajime Tazaki"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 25,
    "room": "K.4.601",
    "title": "Phantom OS",
    "subtitle": "Orthogonal Persistence-based OS Intro and Design Concepts",
    "track": "Microkernels and Component-based OS",
    "abstract": "<p>Phantom OS is an Operating system based on the orthogonal persistence. Application does not feel OS shutdown and restart. Even abrupt restart. It is guaranteed that application will be restarted in consistent state.</p>",
    "description": "<ul>\n<li><p>As long as you have reference to any variable, it’s state is the same between OS reboots. You don’t have (though you can) save program state to files.</p></li>\n<li><p>Managed code. Native Phantom applications are running in a bytecode machine. (But it is worth to mention that Phantom has simple Posix compatibility subsystem too.)</p></li>\n<li><p>Global address space. Phantom OS is an application server. All applications can communicate directly, by sharing objects.</p></li>\n<li><p>Phantom OS persistence is achieved not by serializing data to files, but by running all applications in a persistent RAM. You can (and it will be true) think of Phantom memory subsystem as of a persistent paging engine</p></li>\n</ul>",
    "persons": [
      "Dmitry Zavalishin"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 25,
    "room": "K.4.601",
    "title": "Gneiss: A Nice Component Framework in SPARK",
    "subtitle": "",
    "track": "Microkernels and Component-based OS",
    "abstract": "<p>Gneiss is an abstraction layer for component based environments that aims to\nprovide a foundation for formally provable components. It enables the creation\nof platform independent, asynchronous components in SPARK and provides\nfunction contracts that allow to prove the correct interaction with the\nunderlying platform.</p>",
    "description": "",
    "persons": [
      "Johannes Kliemann"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 25,
    "room": "K.4.601",
    "title": "A Component-based Environment for Android Apps",
    "subtitle": "",
    "track": "Microkernels and Component-based OS",
    "abstract": "<p>With 2.5 billions of active users Android is the most widely deployed mobile operating system in the world. Its vast complexity paired with a monolithic architecture regularly result in severe security issues like the infamous Stagefright bug. In this presentation we talk about an ongoing research project which aims at running Android applications on top of the component-based Genode OS framework and secure them using formally verified components. We discuss how Android applications interact, how well this matches the semantics of Genode and what it takes to support unmodified Android apps.</p>",
    "description": "",
    "persons": [
      "Alexander Senier"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 50,
    "room": "K.4.601",
    "title": "Demonstration of the Sculpt Operating System",
    "subtitle": "",
    "track": "Microkernels and Component-based OS",
    "abstract": "<p>Sculpt OS is a novel general-purpose operating system designed from the ground up and implemented using the building blocks of the Genode OS framework. It started with the vision of a truly trustworthy OS that combines a completely new system structure with microkernels, capability-based security, sandboxed device drivers, and virtual machines. The talk is a live demonstration of the current incarnation of Sculpt.</p>",
    "description": "<p>The Genode OS framework is an operating-system technology created from scratch. Over the past decade, it steadily evolved from a fairly obscure research prototype to a practical day-to-day operating system.</p>\n\n<p>Being a component-based system designed after the principle of least privilege from the very beginning, it breaks with many concepts that we take for granted in traditional operating systems, e.g., the central role of files. Instead, Genode introduces a novel way of composing system scenarios out of building blocks where the building blocks are able to cooperate without ultimately trusting each other. Those building blocks include not only applications but also all classical OS functionalities including kernels, device drivers, file systems, and protocol stacks.</p>\n\n<p>In 2018 - after more than 10 years of developing Genode in a shadowy corner of the open-source community - the project created Sculpt OS, which is a Genode-based general-purpose OS for commodity PC hardware. Since it is not derived from any existing OS, Sculpt re-approaches established concepts like the installation, configuration, and spawning of programs from a new angle. This is reflected by its custom user interface.</p>\n\n<p>Besides presenting the motivation and the fundamental ideas behind Genode, the talk will introduce and demonstrate the current state of Sculpt OS, draw connections to related open-source projects, and give a glimpse on the project's future plans.</p>",
    "persons": [
      "Norman Feske"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 25,
    "room": "K.4.601",
    "title": "A Brief Survey through Genode's ARMv8 Playground",
    "subtitle": "",
    "track": "Microkernels and Component-based OS",
    "abstract": "<p>The Genode project has evolved over the past decade into a versatile toolkit for general-purpose computing. Even though support for ARM embedded devices is an inherent part of it since the very beginning, the focus of the past years was more x86-Architecture centered. Sculpt, the desktop incarnation of Genode, being the prime example. Recently, Genode's ARMv8 port ignited extensive development efforts to support more sophisticated workloads on top of modern embedded and mobile devices.</p>\n\n<p>The talk provides an overview about the current ARMv8 Genode landscape, its ambitions and potential. It will live demonstrate recent achievements from device support up to hardware-assisted virtualization on top of the NXP i.MX8 SoC.</p>",
    "description": "",
    "persons": [
      "Stefan Kalkowski"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 50,
    "room": "K.4.601",
    "title": "NOVA Microhypervisor on ARMv8-A",
    "subtitle": "",
    "track": "Microkernels and Component-based OS",
    "abstract": "<p>NOVA is a modern open-source microhypervisor that can host unmodified guest operating systems next to critical host applications. Although originally developed for the x86 virtualization extensions of Intel and AMD, the internals of the microhypervisor and its external API were designed with flexibility in mind, such that the code could also be ported to other architectures.</p>\n\n<p>In this talk we present the first ever version of NOVA on ARMv8-A.  We will show how the NOVA abstractions map onto the ARM architecture, how modern virtualization features such as GIC and SMMU are being used, discuss the ongoing evolution of the NOVA API and how the ARM port differs from the earlier x86 version.</p>\n\n<p>The talk will conclude with a short demo, an outlook into the NOVA roadmap and the formal verification efforts around the code base, as well as opportunities for collaboration with the NOVA community.</p>",
    "description": "",
    "persons": [
      "Udo Steinberg"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 25,
    "room": "K.4.601",
    "title": "The HIPPEROS RTOS",
    "subtitle": "A Song of Research and Development",
    "track": "Microkernels and Component-based OS",
    "abstract": "<p>HIPPEROS is an upcoming open source RTOS that was developed at ULB and by a former spin-off company of ULB.\nThe talk will be a presentation followed by an open discussion about the main architecture principles of the HIPPEROS kernel and OS, what platforms and architectures we support and our agenda regarding open source.</p>",
    "description": "<p>This multitasking RTOS is specifically designed to take advantage of multi-core platforms for critical, hard real-time applications that must be predictable. It targets high-end embedded platforms that exhibit heterogeneous parallelism. The HIPPEROS kernel is designed from scratch in order to support recent process models present in the real-time systems research literature.\nThe RTOS is based on a micro-kernel with an asymmetric master/slave architecture. It allows to natively support parallel processor architectures, by dedicating one core to the heavy operations of the kernel (scheduling, memory management, etc.) and the other cores can then execute user mode application and serve real-time tasks with very few interferences.</p>\n\n<p>The OS kernel was designed and implemented by a team made of people from the ULB PARTS laboratory (http://parts.ulb.ac.be/).\nThe goal was to create a spin-off company around the topic of Real-Time Operating Systems, including the creation of a new micro-kernel for high-end embedded systems with an innovative software architecture, backed-up by research (both theoretical and applied), designed, developed and maintained with \"good\" (and agile) software design methodology.\nWithin this business, a side objective was to maintain strong links with universities and the research world, by validating the OS design in an academic environment and continuous research activities.</p>\n\n<p>Currently, the company is under liquidation. The state of the project is frozen but we started an ongoing initiative aiming to open source the code base. This would allow external contributions and therefore continuing maintenance and development of new features.\nHIPPEROS could then become a test ground for academics and industrials that aim to try new ideas regarding the reliability and efficiency of their systems. With no such undertaking, the code base might just disappear, therefore cancelling out the 7-year team effort.</p>",
    "persons": [
      "Antonio Paolillo"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 25,
    "room": "K.4.601",
    "title": "Unikraft: A Unikernel Toolkit",
    "subtitle": "",
    "track": "Microkernels and Component-based OS",
    "abstract": "<p>Although unikernels - images containing specialized OS primitives and libraries targeting a specific application - have shown impressive performance potential (e.g., fast I/O of 40 Gbps, fast instantiation in the millisecond range, minimal memory footprints of only KBs and a minimal trusted compute base), creating them has proven to be a complicated and time-consuming process. This is mostly because operating system components have to be individually specialized and developed for each target application and target platform.</p>\n\n<p>In this talk we give an update on the Unikraft open source project. Unikraft is a toolkit for creating specialized unikernels and it aims to remove the need for time-consuming, expert work. In the past two years, the community has put a lot of effort into supporting OS functionality, drivers, and platforms, porting libraries, and providing tools to ease porting of existing applications. We will give an overview of all the exciting achievements and conclude with an outlook of recent project directions: binary compatibility (Linux ABI), support for a wide range of compiled and interpreted languages (e.g., web assembly, Go, Python, Ruby, etc.), enhanced safety features, and the ability to seamlessly produce images ready to run as extremely lean VMs, containers, or directly on bare metal. The aim is that Unikraft will represent a step forward towards wider adoption of unikernels beyond the research community.</p>",
    "description": "<p>We have spent quite a bit of our time over the last years developing unikernels – highly specialized kernels targeting specific applications. We have been originally interested in them for virtualized network functions because of their fantastic performance benefits: tiny memory footprints, boot times comparable to those of processes, and fast I/O performance, to name a few.</p>\n\n<p>Despite the fact that this work and work from several others is proof of their potential, unikernels have yet to see massive adoption. One of the main showstoppers is development time: for instance, developing Minipython, a MicroPython unikernel, took the better part of 3 months to put together and test. ClickOS, a unikernel for NFV, was the result of a couple of years of work. What’s particularly bad about this development model besides the considerable time spent is that each unikernel was basically a “throwaway”: every time we wanted to create a new unikernel targeting a different application and a different platform, we would start more or less from scratch. This comes from the fact that each application has different OS dependencies and benefit from different optimizations and specializations of these layers.</p>\n\n<p>Two years ago, we started Unikraft as an open source incubator project under the umbrella of the Xen Project and the Linux Foundation. Our goal is to build a common pool of decomposed OS functionalities, called libraries, where various Unikernel projects can share implementations and optimizations with others. The project provides Unikernel builders tools that help them to select needed libraries and configurations. Unikraft's build system quickly and automatically creates images tailored to the needs of their specific applications. The users can choose multiple target platforms (e.g., extremely lean VMs, containers, or directly as bare metal) without having to do additional work for each of them.</p>\n\n<p>We are going to present the efforts and achievements done by the community in the last two years. We will also give an outlook of recent project directions: binary compatibility (Linux ABI), support for a wide range of compiled and interpreted languages (e.g., web assembly, Go, Python, Ruby, etc.), and enhanced safety and protection features. With a bit of left time, we will show a live demo to the audience.</p>",
    "persons": [
      "Simon Kuenzer"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 25,
    "room": "K.4.601",
    "title": "VUOS: Give Your Processes a New VU",
    "subtitle": "",
    "track": "Microkernels and Component-based OS",
    "abstract": "<p>VUOS is a different perspective on namespaces, anykernels and related concepts. The main idea behind VUOS is that it is possible to give processes their own \"view\" using partial virtual machines.</p>\n\n<p>A partial virtual machine intercepts the system call requests and operates like a filter: system calls can be forwarded to the kernel of the hosting system or processed by the partial virtual machine hypervisor.\nIn this way processes can see a mix of resources provided by the kernel (on which they have the same view of the other processes) and virtual resource. It is possible to mount filesystems, load networking stacks, change the structure of the file system tree, create virtual devices.</p>\n\n<p>The hypervisor is just a user process so while it gives new perspective for processes, it does not widen the attack surface of the kernel.</p>",
    "description": "",
    "persons": [
      "Renzo Davoli"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "How Firefox upholds its values and keeps up with change",
    "subtitle": "",
    "track": "Mozilla",
    "abstract": "<p>How the Firefox team changed how we thought about shipping features and replaced a process biased towards those who had the loudest voices and the luxury of time with a process that is more inclusive and allows us to reduce risk to Firefox users when we ship.</p>",
    "description": "<p>In this talk you'll learn how the Firefox team changed how we ship new features in the browser, and adopted industry best practices including gradual deployments and feature flags.</p>\n\n<p>Why?</p>\n\n<ul>\n<li>Listening to our most vocal users may bias us towards the needs of an unrepresentative group of users</li>\n<li>When you release a browser to millions of users on a heterogeneous set of platforms, you're testing in production</li>\n<li>Avoiding dot releases</li>\n</ul>\n\n\n<p>I'll talk about what we learned and how it's shaped how we release Firefox at Mozilla.</p>",
    "persons": [
      "Emma Humphries"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Make it accessible",
    "subtitle": "Tips and tricks for create a good accessible frontend",
    "track": "Mozilla",
    "abstract": "<p>This talk focuses on Web Accessibility, namely the practice of ensuring that people with disabilities—be it physical, situational or socio-economic—have access to and can interact with websites and applications.</p>",
    "description": "<p>This talk focuses on Web Accessibility, namely the practice of ensuring that people with disabilities—be it physical, situational or socio-economic—have access to and can interact with websites and applications.</p>\n\n<p>We will begin the talk sharing the concept of an interface, intended as a layer between two parties that do not speak the same language. We’ll continue talking about accessibility devices and myths that surround them. Then we will dive into how to refactor codebases for screen reader optimization, following accessibility guidelines that improve the user experience for all people. We will conclude by sharing some useful tools that help developers build accessible webpages.</p>\n\n<p>In the end we'll see some useful tools for helping developers to make accessible webpages</p>",
    "persons": [
      "Gabriele Falasca"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "You Can't Spell Accessibility Without CSS",
    "subtitle": "",
    "track": "Mozilla",
    "abstract": "<p>In this talk, we'll discuss website accessibility in the context of CSS. Things like when to use an image and when to use a background image, how to ensure proper contrast between text and background colors, maintaining a hierarchical layout in your design, when to use an outline: 0 property (almost never), and a whole bunch of other things.</p>",
    "description": "<p>This talk is suited to beginner and intermedite front end developers.\nSpecific topics covered include:\n- maintaining the hierarchical structure of content,\n- creating non-obtrusive elements\n- ensuring contrast between background and text\n- using devtools to check accessibility</p>",
    "persons": [
      "Jemima Abu"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Is the web rea11y for all?",
    "subtitle": "Firefox DevTools & A11y",
    "track": "Mozilla",
    "abstract": "<p>The web is pretty much present in everyone's life nowdays. But is it really for all? How are we making it a smooth usage for those with disabilities, for those with reduce time and internet connection? Or even more further, for those that just use the web for the first time?\nLet's learn a bit how the devtools from Firefox can help us improve everyone's experience without much of a sacrifice on our end.</p>",
    "description": "<p>A11y and inclusion is getting more traction nowadays but people still think that they address only those with disabilities. As a person without any officially i did find myself in this situation and felt confused about how the web should be ( was about to loose money on ticket conferences because i was not seeing the categories). Some Developers are a bit defensive saying they don't have users for whom to build more a11y in or the time to add these features. Showing the devtools Firefox has for it - will show how fast you can check your page and how no costy is. Also not knowing all best practices won't be a blocker and firefox will give you suggestions. In a way i wanted to bring attention to A11y nd Inclusion in a more technical way.</p>",
    "persons": [
      "Ioana Chiorean"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 40,
    "room": "UA2.114 (Baudoux)",
    "title": "What are the Top 10 Frustrations for Web Developers and Designers?",
    "subtitle": "Lessons from the 2019 MDN Developer Needs Assessment",
    "track": "Mozilla",
    "abstract": "<p>The MDN Web Developer Needs Assessment is the first edition of an annual study providing a prioritized list of designer and developer needs.</p>",
    "description": "<p>As an industry working on the Web, as a platform and set of tools, we recognized a critical voice was missing when it came to making decisions about feature development — that of web designers and developers.</p>\n\n<p>The MDN Web Developer Needs Assessment is the first edition of an annual study providing a prioritized list of designer and developer needs.</p>\n\n<p>We put this report together with the help of more than 30 stakehold- ers from the MDN Product Advisory Board member organizations and the input of more than 28,000 developers and designers from 173 countries who took the twenty minutes necessary to complete the survey entirely. That’s more than 10,000 hours contributed by the community to help us understand their pain points, wants, and needs. With that involvement, we believe the MDN Web DNA is the largest web developer and designer focused research study ever conducted.</p>\n\n<p>Their input now, and in future versions, will influence how browser vendors prioritize feature development so we can address the needs of designers and developers, both on and off the Web. By producing the report annually, we can track needs and pain points over time so we can see the impact of our efforts.</p>\n\n<p>A critical aspect of the report is that it provides a voice for commu- nities of practitioners. We did not tailor it to current assessments and priorities of participating browser vendors. A single browser vendor does not own it.</p>",
    "persons": [
      "Kadir Topal"
    ]
  },
  {
    "start": 1577880900000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Discover the New Firefox Profiler",
    "subtitle": "",
    "track": "Mozilla",
    "abstract": "<p>Using a profiling tool help developers to get detailed information about the execution of their application and allow them to understand the behavior of it.</p>\n\n<p>The Firefox Profiler is a profiler that is built into Firefox. It has tighter integration with Firefox than external profilers. It can provide more information and insight into what the browser is doing. It can also show the memory usage and Firefox internal code execution.</p>\n\n<p>During the talk, I will be explaining, how to capture a good profile and how to analyze profile data. I will be sharing Firefox Profiler specific features like memory tooling, single tab/advanced view and how to use them. I will also be sharing the future of Firefox Profiler!</p>",
    "description": "<p>Web applications are more popular than ever and users expect more from a web application. Unfortunately, they are using applications more frequently from low powered devices, which strains your application’s performance. Managing application performance can be challenging as modern applications have many dependencies and their complexity can hide the issues.</p>\n\n<p>Using profiling tools to look for potential bottlenecks can significantly reduce the number of problems in your application. It helps you to get detailed information about the execution of your application and it allows you to understand the behavior of it.</p>\n\n<p>The Firefox Profiler is a profiler that is built into Firefox and is available at https://profiler.firefox.com/. It has tighter integration with Firefox than external profilers. It can provide more information and insight into what the browser is doing. Aside from understanding the execution of a web page, it can also show the memory usage and Firefox internal code execution.</p>\n\n<p>The intended audience is all web developers and people who want to contribute to Mozilla by helping us analyze the Firefox performance issues by capturing profiles and filing bugs.</p>\n\n<p>The talk will cover topics like:\n- Profiling basics\n- How profile capturing UI works.=\n- How to capture a profile without getting a lot of noise in the profile data\n- Profile data content\n- Firefox Profiler specific features like:\n- Single tab/advanced view\n- Memory profiling\n- Profiling network\n- Profile comparison view\n- Brand new features in Firefox Profiler</p>",
    "persons": [
      "Nazım Can Altınova"
    ]
  },
  {
    "start": 1577882700000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Web compatibility and ML",
    "subtitle": "Improving webcompat issue triaging using ML",
    "track": "Mozilla",
    "abstract": "<p>In 2019, Mozilla's Open Innovation and WebCompat team joined forces to improve the process of gathering web compatibility issues. One of the experiments was to introduce machine learning capabilities in the triaging process and automate some steps. This talk is about the early steps and how we got some hands on experience with machine learning, what we've achieved so far and potential next steps.</p>",
    "description": "",
    "persons": [
      "Nemo"
    ]
  },
  {
    "start": 1577884500000,
    "duration": 40,
    "room": "UA2.114 (Baudoux)",
    "title": "Facilitating distributed deterministic computation with WASI",
    "subtitle": "",
    "track": "Mozilla",
    "abstract": "<p>WebAssembly System Interface (WASI) is the new brilliant community effort at standardising the use of WebAssembly (Wasm) outside the browser environment. Initiated by Mozilla, now under the umbrella of Bytecode Alliance, WASI has the potential to revolutionise the way we think about the \"build once, run anywhere\" in a truly secure manner. But could WASI also lend itself to a task of running any code within a network of distributed, untrusted nodes such as BOINC or Golem Network, and ensuring that the results received are indeed correct? The short answer is yes, if determinism of computations could be enforced which opens many ways at verifying the results. Enforcing determinism is a rather difficult thing to achieve in other platforms such as JVM etc., now possible thanks to Wasm and WASI. This talk will delve deep into the inner workings of the WASI spec, and its goto implementation, the wasi-common library, and explore how and if determinism can be enforced at the WASI syscall level.</p>",
    "description": "",
    "persons": [
      "Jakub Konka"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Loanwords, Agriculture & WebAssembly",
    "subtitle": "",
    "track": "Mozilla",
    "abstract": "<p>In this talk we take a whirlwind tour through untranslatable language jargon and native indigenous agricultural techniques to help us think about where the web ecosystem appears to be headed and how to switch the current course towards a more sustainable future.</p>",
    "description": "",
    "persons": [
      "Andre Garzia"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "A Mozilla IoT Forecast thats Sunny and Clear -- No Clouds!",
    "subtitle": "WebThings by Mozilla",
    "track": "Mozilla",
    "abstract": "<p>WebThings (by Mozilla) is an open source smart home implementation to improve privacy, security, and interoperability.</p>",
    "description": "<p>Want to manage your own private smart home? Want your connected things to be interoperable across brands, securely accessible and controllable over the web? Come see how to run your entire smart home on the edge, in your own home, no clouds required! This talk demonstrates how to run the WebThings Gateway on a Raspberry Pi (or in a Docker container on your favorite platform) to manage IoT devices that you build or buy. You'll also learn how to build your own \"web things\", in minutes, using open source WebThings framework libraries.</p>",
    "persons": [
      "Kathy Giori"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 25,
    "room": "UA2.114 (Baudoux)",
    "title": "Privacy by Design",
    "subtitle": "",
    "track": "Mozilla",
    "abstract": "<p>Most of the websites leak out the user data, and most of the times (~90%) it is not known to the site admins. These leakage happen via tag managers, third party CDNs, embeds, fonts etc. In this talk I would like to discuss the opportunities for developers to avoid these while developing websites. This talk will help them to ensure their websites does exactly what they intend their website to do. The side-effect of this always is better performance 😎</p>",
    "description": "<p>Every other website on the internet uses third party trackers to improve their product and provide better user experience, which eventually leads to a compromise in user privacy. As developers, we are responsible for making our users feel safe and do whatever it takes to secure the privacy of our users.\nIn this talk we will discuss the data leaks which happen while using third party trackers and will walk through the measures we can take to avoid them and ensure the privacy of our users, right from the time of development.</p>",
    "persons": [
      "Trishul Goel"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 30,
    "room": "UA2.114 (Baudoux)",
    "title": "What Makes People Come and What Makes Them Stay",
    "subtitle": "",
    "track": "Mozilla",
    "abstract": "<p>Over the years the tech industry has been trying to change its diversity and inclusion statistics but that seems to have been a hard nut to crack. This is a talk about what makes people come, but then also what makes people stay. Because diversity is inviting people to the dance, but inclusion is enabling them to join it. Let's figure out how you can make people come and want to stay in your organizations, and teams, and let's see one use-case where Mozilla did the same.</p>",
    "description": "",
    "persons": [
      "Gloria Dwomoh"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 10,
    "room": "UA2.220 (Guillissen)",
    "title": "Close lid to encrypt",
    "subtitle": "Hard disk encryption in Linux suspend mode",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>Today, hard disk encryption only protects user's data when their machine is shut down.\n\"Close lid to encrypt\" aims to enhance this protection also to suspend mode.</p>",
    "description": "<p>Hard disk encryption is a necessity for everyone, who fears the physical theft or seizure of their device. However, your data is still only protected while the machine is shut down. But most people rarely shutdown their devices anymore. Usually, you just close the lid of your notebook and you're on your way.</p>\n\n<p>\"Close lid to encrypt\" aims to improve the privacy of your data. When you close the lid of your notebook, it goes into sleep/suspend mode. All  processes are frozen and don't need to access your hard disk anymore. We use this opportunity to clean the keys of your encrypted devices and suspend them as well. Therefore, the data on your hard drive is protected.\nWhen resuming your computer, you must re-enter the password of your encrypted volumes. But then you're just where you've been working before.</p>\n\n<p>To make all this work, we rely on a small kernel patch, the cryptsetup project, initramfs and cgroups2. \"Close lid to encrypt\" right now focuses on Debian and it derivatives and we plan to bring all code upstream. This effort is funded by the German Prototypefund.</p>\n\n<p>In this 20 minute presentation, I will thoroughly explain the problem and our approach to solving it. Of course, I will also explain the limits of our approach.\nFurthermore, I will demonstrate our already working prototype and answer your question.</p>",
    "persons": [
      "Tim Dittler"
    ]
  },
  {
    "start": 1577869800000,
    "duration": 10,
    "room": "UA2.220 (Guillissen)",
    "title": "Open and federated identities with ID4me",
    "subtitle": "An alternative to \"sign in with Facebook\"",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>Online identities are the cornerstone on which data-based capitalism is built - so, Google, Facebook and other OTTs are trying to dominate them and close them into silos. The ID4me platform extends OpenID Connect to create an open and federated architecture that allows any number of providers to interoperate, and gives back control to users, and a role to community service providers.</p>",
    "description": "<p>In the last years, the Internet has been increasingly centralized into the hands of GAFAM and other over-the-top companies that built walled gardens in fields like messaging and social networks. More and more of these companies have user data monetization and targeted advertising as a core revenue stream; thus, tracking people across their Internet activities is necessary to their existence.</p>\n\n<p>This is why they have also built closed identity systems that supply single-sign-on and very easy sign up for new websites and services, at the expense of privacy and user control. As managing hundreds of separate accounts is inconvenient and insecure, and as alternatives such as password managers are not easy enough for the average Internet user, clicking on “sign in with Google” or “sign in with Facebook” has become a very common choice.</p>\n\n<p>We think that this is bad for the Internet in general, and thus we are creating a platform that allows anyone to provide identities, creating an open, public and federated single-sign-on and data management system. We are extending OpenID Connect just a bit, the bit that is necessary to break the silos and allow interoperability; and we are basing the system on the DNS, the widely available and already federated naming directory of the Internet.</p>\n\n<p>The talk will explain how the system works and encourage participation and contributions.</p>",
    "persons": [
      "Vittorio Bertola"
    ]
  },
  {
    "start": 1577870400000,
    "duration": 10,
    "room": "UA2.220 (Guillissen)",
    "title": "Identity Box",
    "subtitle": "Decentralized Web of the Future",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>Society is becoming increasingly more aware of the importance of protecting digital information and it is becoming clear that the current centralized model has came to an end.\nThe future of the Internet is distributed. Unsupervised, unmoderated access, affordable storage, data-replication, and security and privacy built-in are the most important aspects of the Internet of the future.</p>\n\n<p>Unfortunately, a global, reliable, decentralized network cannot be built without actual physical nodes, as the opposite of thousands of nodes in centralized cloud data center. Only by building an open network of physical nodes we can pave our way as a society to the decentralized Internet of the future.</p>\n\n<p>Identity Box is a personal P2P networking device giving you access to a global network of distributed storage, digital identity, and distributed personal apps. It is a community effort of\nbuilding the next-generation, decentralized infrastructure that enables an open platform for privacy-preserving ecosystems.</p>",
    "description": "<p>Most of the data today belong to just a handful of companies. Personal documents, photographs, videos, things that we put online in general, contain lots of sensitive information. Information that we would rather prefer to stay private. Very often the same companies that provide more or less \"complimentary\" storage space for our disposal, also help us managing our whole digital existence. The combination of the data and the identity information is a powerful combination which empowers well-established business models where the user's data or the user itself become a product. Allowing sensitive data to be kept by well-known service providers makes it easier than ever for illegal institutions, but also the state, to gain insights into the data that they have no rights to access.</p>\n\n<p>Our sensitive personal data are kept by the state, healthcare organizations, financial institutions, and corporations. We do not have control over these data and our access to them is limited. Every institution storing the data has not only its own policies, but also uses proprietary technologies to access the data. These data silos make interoperability hard and give institutions almost complete freedom to use the data without consent of the user.</p>\n\n<p>Society is becoming increasingly more aware of the importance of protecting the digital content and it is becoming clear that the current centralized model has came to an end.\nThe future of the Internet is distributed. Unsupervised, unmoderated access, affordable, unlimited storage, security and privacy built-in are the most important aspects of the Internet of the future.</p>\n\n<p>Unfortunately, a global, reliable, decentralized network cannot be built without actual physical nodes, as the opposite of thousands of nodes in centralized cloud data centers. Users need to be re-introduced to the concept of decentralization and learn the advantages of technologies like self-sovereign identity, and content-addressable networks. Only by building an open network of physical nodes we pave our way as a society to the decentralized Internet of the future. Building the decentralized Internet of the future is therefore a community effort, where all participants become the actual owners of the distributed global infrastructure.</p>\n\n<p>To support this community movement, we propose Identity Box: a personal P2P networking device giving you access to the global network of distributed storage, digital identity, and distributed personal apps.</p>\n\n<p>Identity Box is a physical device, but at the same far more than just piece of hardware. Together with the included software and Identity App, Identity Box enables an ecosystem of rich, distributed personal applications. It supports IPFS, Self-Sovereign Identity, and end-to-end encrypted storage. And that's just the beginning.</p>\n\n<p>Join us in building the decentralized Internet of the future!</p>",
    "persons": [
      "Marcin Czenko"
    ]
  },
  {
    "start": 1577871000000,
    "duration": 10,
    "room": "UA2.220 (Guillissen)",
    "title": "Decentralized object storage",
    "subtitle": "An open source decentralized object storage",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>A presentation of the architecture of Storj decentralized object storage.\nStorj is a company that has built a completely open source decentralized object storage written in Go.\nCurrently Storj is in the lat beta release before releasing the production version of the decentralized object storage network.</p>",
    "description": "<p>A presentation of the architecture of Storj decentralized object storage.\nStorj is a company that has built a completely open source decentralized object storage written in Go.\nCurrently Storj is in the lat beta release before releasing the production version of the decentralized object storage network.</p>\n\n<p>The network is formed by a big community of operators, named Storj Node Operators, who offer storage space and network bandwidth to the network.\nStorj incentives the Storj Node Operators for their contributed storage space and network bandwidth.</p>\n\n<p>The network is AWS S3 compatible and is pursuing to achieve the same or even better service layer agreements than AWS S3 at a cheaper price in terms of storage and bandwidth, all thanks to the great community of Storage Node Operators.\n23.-\nIn this event, I will expose the architecture of the network and the challenges that decentralized network storage presents in comparison with the ones that are not exposed to a centralized one.</p>\n\n<p>I will comment how the confidentially, durability, retrievability and upload/download speed is maintained.</p>",
    "persons": [
      "Ivan Fraixedes"
    ]
  },
  {
    "start": 1577871600000,
    "duration": 10,
    "room": "UA2.220 (Guillissen)",
    "title": "Librecast: Privacy and Decentralization with Multicast",
    "subtitle": "IPv6 Multicast and the Next Generation Internet",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>Written in 2001, RFC 3170 states: \"IP Multicast will play a prominent role on the Internet in the coming years.  It is a requirement, not an option, if the Internet is going to scale.  Multicast allows application developers to add more functionality without significantly impacting the network.\"</p>\n\n<p>Nearly two decades later, multicast is still largely ignored and misunderstood.</p>\n\n<p>This talk explains why multicast is the missing piece in the decentralization puzzle, how multicast can help the Internet continue to scale, better protect our privacy, solve IOT problems and make polar bears happier at the same time.</p>",
    "description": "<p>There are many common misconceptions about multicast, including that it is only useful for streaming video and audio.  It does so much more than that.</p>\n\n<p>Multicast is really about group communication.  It is, by definition, the most efficient way to distribute data to groups of nodes.</p>\n\n<p>Multicast brings with it a very different way of thinking about distributed systems, and what is possible on the Internet.  From database replication to chatops, server federation, configuration management and monitoring.</p>\n\n<p>Even applications, such as chat, which are fundamentally multicast in nature are being built on top of unicast protocols.  There is a Better Way.</p>\n\n<p>Unicast networking leads to centralised and inefficient systems that are more open to attack and censorship.  This talk will show how multicast allows for more efficient, decentralized designs, leading to increased efficiency and much-reduced energy consumption.  This is better for our democracy, human rights and our planet.</p>\n\n<p>Multicast lets us do things that would be impossible with unicast.  Imagine sending software updates to a billion IoT nodes simultaeneously, using just one tiny virtual server.</p>\n\n<p>At a time when even the web is moving to UDP with HTTP/3 and WebRTC, it is time we took a serious look at what we're missing by not using multicast at the network layer to underpin our Internet protocols.</p>\n\n<p>We'll discuss how you can start using multicast in your project today, and how multicast design and thinking differs from unicast.  We'll cover some of the different types of IP multicast, the basics of multicast routing, how to build in TCP-like reliability and take a look forward to how improvements in multicast can make a better Internet for the future.</p>",
    "persons": [
      "Brett Sheffield"
    ]
  },
  {
    "start": 1577872200000,
    "duration": 10,
    "room": "UA2.220 (Guillissen)",
    "title": "SCION - future internet that you can use today",
    "subtitle": "",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>Do you know where your internet traffic flows? Does it go through China even if you don't want it to? SCION is a new internet architecture aimed at solving this problem. We will show how you can easily join the already existing worldwide network.</p>",
    "description": "<p>The current Internet was not designed with control and security considerations in mind: incidents such as the hijacking of all traffic for YouTube by a Pakistani ISP in February 2008, the Cloudflare DNS service hijacked by AnchNet in May 2018, or a large chunk of European mobile traffic being rerouted through China in June 2019 show that we cannot quite trust the current Internet. SCION is a proposed future Internet architecture aiming to offer high availability and security, even in the presence of actively malicious network operators and devices.</p>\n\n<p>Designing a new Internet from scratch gives us the opportunity to make it work a lot better: we are aiming to notably improve security, availability, and performance. At the same time, just replacing the Internet would not be feasible, and thus we also emphasise practical concerns, such as incremental deployment and backwards compatibility. Thanks to that, SCION is currently the only clean-slate Internet architecture with a world-wide research network and production deployments in several large institutions in Switzerland; and you can start using it today.</p>\n\n<p>In this lightning talk, we will briefly present the current state of SCION implementation, focusing on how it provides its most important features:</p>\n\n<ul>\n<li>path awareness and path control by end hosts</li>\n<li>geofencing and isolation from untrusted actors</li>\n<li>increased performance by active usage of multiple links</li>\n</ul>\n\n\n<p>We will point you to the resources presenting how easy it is today for the end user to join the network and start using the available services through the world-wide test deployment, SCIONLab, consisting of around 50 different points-of-presence around the globe, many of them connected via direct, BGP-free, links.</p>",
    "persons": [
      "Mateusz Kowalski"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 30,
    "room": "UA2.220 (Guillissen)",
    "title": "RFC 1984",
    "subtitle": "Or why you should start worrying about encryption backdoors and mass data collection",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>In 1996 Brian E. Carpenter of IAB and Fred Baker of IETF wrote a co-statement on cryptographic technology and the internet.  This RFC wasn't a request for a technical standard, it was a statement on their concerns about Governments trying to restrict or interfere with cryptography. They felt that there was a need to offer \"All Internet Users an adequate degree of privacy\"</p>\n\n<p>Since that time successive governments around the world have sought to build back doors into encrypted apps and services to access more citizen and visitor data.  As of July 2019, the AG of the United States William Barr stated: “Some argue that, to achieve at best a slight incremental improvement in security, it is worth imposing a massive cost on society in the form of degraded safety,” i.e For security Americans should accept weakened encryption.  The head of the FBI also claimed that weakened encryption wouldn't break it.  At the moment the US Government is actively trying to stop Facebook implementing end to end encryption across it's suite of apps.</p>\n\n<p>In Australia the metadata retention laws have been abused against journalists with 58 searches carried out by the AFP.  In 2015 ACT police carried out 115 metadata searches.  UK officials have a cavalier attitude to the EU SIS database which tracks undocumented migrants, missing people, stolen cars, or suspected criminals.</p>\n\n<p>The EU isn't immune to this either with France considering implementing Facial Recognition on its government services.</p>\n\n<p>IETF Session 105 mentioned privacy and concerns with the mass collection of data. While the IAB and IESG were worried about US export controls on cryptography there is an argument for RFC 1984 to be updated to include the unnecessary mass collection of data and to use it as a term for IT professionals, privacy advocates and the public to rally behind.</p>\n\n<p>In this talk let's recount a brief history of governments around the world wanting to weaken encryption as RFC 1984 warned us about.</p>\n\n<p>We live in a time where citizens put data into commercial, healthcare and Government systems to access services, some services are only accessible online. From CCTV to Facebook people have little understanding of why mass collection of data is dangerous. There is little scrutiny of who can access that data, from Scotland to the US.</p>\n\n<p>Open Surveillance is only a small part of the picture when profiling citizens.  It still counts as personal data, when combined with metadata and the actual data that people put into social media and services like ancestor DNA test kits.  Businesses who use CCTV have to put up signs to warn the public they are recording.  So called anonymized data still contains identifiers that can tie to individuals.</p>\n\n<p>Let's talk about Ovid and peacocks. Let's explore how to expand the RFC to cover recent developments in surveillance capitalism with governments accessing that data, but not securing it. We need to make it clear weakened encryption, the mass collection and careless retention of data isn't acceptable. RFC1984 became Best Practice in 2015, we need to do more to raise awareness and to implement it in our projects.</p>",
    "description": "<p>Why we need to implement RFC 1984:</p>\n\n<p>\"The Internet Architecture Board (IAB) and the Internet Engineering\n   Steering Group (IESG),[...] are concerned by the need for increased\n   protection of international commercial transactions on the Internet,\n   and by the need to offer all Internet users an adequate degree of\n   privacy.\n\"</p>\n\n<p>I'd like to start by briefly mentioning Ovid and the legend of Io.  Ovid was anti authoritarian during the time of Augustus as he'd been exiled by the Emperor.  He wrote The Metamorphoses; an epic poem about Greek myths with the theme of transformation.   The myth is often used as a metaphor for surveillance.  With Io suffering restriction of liberty and being abused by authority. Being turned into a cow was bad enough, to make things worse she was constantly watched by the agent of Hera another authority Argus (Argus Panoptes) the 100 eyed giant.  Argus is a great name for a security firm in fact there are quite a few firms that use an eye in the logo.</p>\n\n<p>Pop culture like Neil Gamien's American gods on Amazon  have also referenced this legend to show surveillance and how it can convey power to authority.  In the end a modern interpretation of the myth could argue that Hermes sending Argus to sleep to kill him is a good metaphor for opposing actors using exploits to subvert and disable surveillance to access information to Citizens data.   We focus more on Argus the agent of Surveillance rather than Io, who was violated, changed and then incarcerated with surveillance against her will.</p>\n\n<p>Argus Panoptes inspired the idea of the Panopticon. A  building design by English Philospher Jeremy Bentham as a prison that could be observed by a single guard.  Our Internet is in danger of being a virtual panopticon for future citizens.  The EFF already started thinking about this with panopticlick so that you can test who's tracking you through your browser. So who's watching us?</p>\n\n<p>Of course this explanation and the metaphor is from a Western Perspective.  Privacy doesn't mean the same thing to all countries and cultures.  Neither does the symbolism of the Peacock.</p>\n\n<p>Many IT professionals consider RFCs are more like guidelines, see RFC Clueless.org. Popular email services like Me.com, Outlook.com and even gmail.com have been listed on RFC ignorant, then it's successor RFC clueless .  Sadly the giants often ignore RFCs.   Which breaks the idea of interoperable standards and protocols and leaves us in danger of being at the  mercy of large hosting giants.</p>\n\n<p>There is a narrative that threads through the media since that time.  Privacy is dead, you need to give up that freedom to stay safe.  Politicians like the UK Prime Minister David Cameron in 2015 stated:</p>\n\n<p>.\"In our country, do we want to allow a means of communication between people which even in extremis, with a signed warrant from the home secretary personally, that we cannot read? “Up until now, governments have said: ‘No, we must not'.\" \"</p>\n\n<p>Malcolm Turnbull the Australian Prime Minister in 2017 stated that \" the laws of Australia take precedence over the laws of mathematics.\"</p>\n\n<p>With organizations like Palantir providing information to ICE to target illegal immigrants in the US; The UK Home Office deliberately destroying data in the the Windrush scandal;  It's clear that human rights, specifically the right to privacy is in danger.  Recently the EU confirmed that UK Border Force officials had illegally copies Shengen SIS data to third party Organizations based in the US.</p>\n\n<p>That's before I even start on repressive regimes where that data can and will be used to oppress citizens of that regime.</p>\n\n<p>The recent IETF Session 105 this month mentioned privacy and concerns with the mass collection of data. While the IAB and IESG were worried about US export controls on cryptography there is an argument for RFC1984 to be updated to include the unnecessary mass collection of data and to use it as a term for IT professionals, privacy advocates and the public to rally behind.</p>\n\n<p>I propose a brief history of governments around the world wanting to weaken encryption as RFC1984 warned us about:</p>\n\n<p> \" The IAB and IESG are therefore disturbed to note that various\n   governments have actual or proposed policies on access to\n   cryptographic technology that either:</p>\n\n<p>   (a) impose restrictions by implementing export controls; and/or</p>\n\n<p>   (b) restrict commercial and private users to weak and inadequate</p>\n\n<pre><code>   mechanisms such as short cryptographic keys; and/or\n</code></pre>\n\n<p>   (c) mandate that private decryption keys should be in the hands of</p>\n\n<pre><code>   the government or of some other third party; and/or\n</code></pre>\n\n<p>   (d) prohibit the use of cryptology entirely, or permit it only to</p>\n\n<pre><code>   specially authorized organizations.\"\n</code></pre>\n\n<p>RFC 1984 was explicitly named to reference an Orwellian Society that uses mass surveillance.  Let's expand that beyond encryption to the mass collection of data and ask how do we limit this?  How do we limit access to this data?  How do we stop the nightmare?</p>\n\n<p>Addendum: As time goes on with the current political climate, I expect more focus by the media and the IAB and the IETF on this subject.  So while the overall thrust of this presentation will be the same, I plan to keep it as fresh as possible.</p>\n\n<p>I will be updating this talk with more of a focus on Biometric data including Facial Recognition.</p>\n\n<p>I recently presented a version of this talk at UBUCON Europe in October 2019</p>",
    "persons": [
      "Esther Payne"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 30,
    "room": "UA2.220 (Guillissen)",
    "title": "Fixing healthcare data exchange with decentralized FOSS",
    "subtitle": "Building a decentralized Infrastructure to fix medical data exchange in The Netherlands.",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>In The Netherlands we have a interesting problem: in 2011, weeks before going live, the national electronic health record system got shut down by our senate. They decided not to interveine and let the market fix the problem. Now, 9 years later, the market has made a mess out of it: there is no uniform way of exchanging medical data in The Netherlands.\nArchitects write countless of pages with solutions, the government pours  millions into subsidised programs, but the problem is only getting bigger.</p>\n\n<p>So, 2 years ago together with a group of other software vendors we started a foundation called Nuts. The goal is to end this impasse by building an open source decentralised infrastructure that nobody controls and can be used by everyone. It should be cheap to join, privacy by design, and use technology over lawyers.</p>\n\n<p>Our infrastructure allows parties to exchange data \"peer to peer\", only helping them solve four generic problems: user identity, patient consent, discovery of endpoints and logging.</p>\n\n<p>In this talk I would like to show our architecture, explain which choices we made, what we have learned while working with a distributed software and some anecdotes about what happens if you pitch such an idea to the establishment.</p>\n\n<hr />\n\n<p>A little more background: I'm one of the main devs. The system is written mostly in Go and some parts in Java. Every software vendor can spin up a node and join the network. Patient Consents are distributed by a DLT (Corda) and are only stored on the nodes of vendors who already process the patient`s data. No medical data flows through the system, Nuts is only used to connect them and provide a level of trust. Identities are managed by a self-sovereign Identity system called IRMA (irma.app) which is based on IBM idemix.</p>",
    "description": "",
    "persons": [
      "Steven van der Vegt"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 30,
    "room": "UA2.220 (Guillissen)",
    "title": "P2P how and Kademlia",
    "subtitle": "P2P how and Kademlia",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>Recently there has been tremendous interest in P2P systems and their usefulness. However most people are confused about how they work, how nodes are techno-magically able to find each other or the right other node with expected data. This talk would start p2p from basics, it's differences with client-server systems and then different ways p2p systems can operate, dhts as one way. And then end with more specific things as kademlia being a good dht and how does it work. It would be interesting because it (kademlia) powers some of the most popular p2p systems such as bittorrent, IPFS, Ethereum, I2P, Tox etc.</p>",
    "description": "<p>Contents of the talk would be as follows\n- what is a p2p system\n- a bit about client-server\n- finding nodes in client-server vs p2p\n- p2p as overlay networks\n- some earlier p2p systems and their limits\n- distributed hash tables to rescue, dht basics\n- what makes a good dht</p>\n\n<p>since Kademlia is a good dht</p>\n\n<p>Kademlia\n- core ideas (Uniform ID Space, Closeness, Local view)\n- protocol messages\n- joining network\n- locating nodes and resources\n- storing\n- go-libp2p-kad-dht</p>\n\n<p>TADA!</p>",
    "persons": [
      "Kishan Sagathiya"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 30,
    "room": "UA2.220 (Guillissen)",
    "title": "GNUnet: A network protocol stack for building secure, distributed, and privacy-preserving applications",
    "subtitle": "GNUnet basics, the GNU Name System and other applications.",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>In this talk we will give a brief introduction into the GNUnet peer-to-peer framework, its architecture and existing applications.\nThis includes details on the p2p overlay, double-ratchet messaging channels (CADET) the GNU Name System (GNS) and a selection of other applications and features.\nWe present the current status of the project, the roadmap as well as ways to participate and use GNUnet.</p>",
    "description": "<p>GNUnet is a new network protocol stack for building secure, distributed, and privacy-preserving applications. With strong roots in academic research, our goal is to replace the old insecure Internet protocol stack.</p>\n\n<p>GNUnet is typically run as an overlay network on top of the existing Internet infrastructure forming the basis of a hybrid peer-to-peer mesh and relay backbone for applications to run on. It could just as well be run independently of the Internet, over dedicated radio and cable.</p>\n\n<p>GNUnet is made for a free and open society: It's a self-organizing network and it is free software as in freedom. GNUnet puts you in control of your data. You determine which data to share with whom, and you're not pressured to accept compromises.</p>",
    "persons": [
      "Martin Schanzenbach"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 30,
    "room": "UA2.220 (Guillissen)",
    "title": "Knocking Down the Nest",
    "subtitle": "secushareBOX - p2p & encrypted IoT and beyond...",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>More and more people are inviting corporate-controlled networked devices into their homes. Can we make truly \"smart devices\" which we control, and communicate directly with, instead of through the cloud? We're building a privacy-preserving and peer-to-peer IoT platform: secushareBOX</p>",
    "description": "<p>Lightbulbs, thermostats, video cameras, maybe even toasters; People are putting all kinds of networked devices in their homes these days. The majority of these devices can only be controlled through proprietary and centralized cloud based services, with the data and metadata being ingested by surveillance machine.</p>\n\n<p>Let's build a better \"Internet of Things\".</p>\n\n<p>secushareBOX is a peer-to-peer and privacy-preserving project for remote system management, including embedded devices. Conceived as an alternative to the centralized pattern so common to IoT platforms; With secushareBOX you communicate directly with your devices, and manage access control with your peers and other systems as modeled in a social graph. Using GNUnet as our underlying p2p framework, we inherit an active network of nodes with which we are able to route our traffic in a manner which preserves our metadata, and utilizes end-to-end encryption for all connections.</p>\n\n<p>This talk will introduce the ideals, and concepts of the project. Followed with a demo of the current state, and discussion of our future plans.</p>",
    "persons": [
      "Devan Carpenter"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 30,
    "room": "UA2.220 (Guillissen)",
    "title": "Peer-to-peer collaboration, search & discovery",
    "subtitle": "Decentralized collaborative application development",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>A data-centric, offline-first approach to decentralized collaborative application development focusing on data ownership and privacy.</p>",
    "description": "<p>Exploring replicated mergeable data structure stores as building blocks of decentralized applications that enable asynchronous collaboration and offline search in combination with peer-to-peer gossip-based protocols that provide pub/sub, dissemination, and recommendation services both over the internet as well as on local and mobile proximity networks, thereby forming interest-based networks that facilitate discovery of personally relevant content and people.</p>",
    "persons": [
      "TG x"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 30,
    "room": "UA2.220 (Guillissen)",
    "title": "DAT protocol in the browser: Progress and Challenges",
    "subtitle": "",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>Dweb protocols, like DAT and IPFS, promise significant benefits over the standard client-server protocols for web content. Particularly for self-hosting and -publishing, these protocols could reduce barriers to entry by eliminating server costs as well as promoting data ownership. Despite this, there has been no adoption of these protocols in mainstream browsers yet. This talk gives an overview of work to add native-like support for the DAT protocol to Gecko-based browsers. We discuss the limitations of the current WebExtension APIs in Chrome and Firefox for this purpose, and how Firefox's libdweb project improves on this. We present the dat-webext browser extension which implements DAT support in Firefox on Desktop and for Geckoview on Android.</p>",
    "description": "<p>This talk will cover the content of the following two blog posts, as well as more recent developments:</p>\n\n<ul>\n<li>https://sammacbeth.eu/blog/2019/03/22/dat-for-firefox-1.html</li>\n<li>https://sammacbeth.eu/blog/2019/05/12/dat-for-firefox-2.html</li>\n</ul>",
    "persons": [
      "Sam Macbeth"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 30,
    "room": "UA2.220 (Guillissen)",
    "title": "An Introduction to the Tor Ecosystem for Developers",
    "subtitle": "",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>Tor is a free and open-source software anonymization system that allows people around the world to use the internet safely. The Tor network itself is operated by various volunteering individuals and organizations around the globe, and the network carries around 200 Gbit/s of traffic and helps somewhere between 2,000,000 and 8,000,000 users every day. The Tor ecosystem is much larger than the anonymity system that Tor provides itself: The Tor Project, the non-profit behind the anonymity system, also develops and maintains a web browser based on Mozilla Firefox. The organization also does monitoring of the network, work on emerging anti-censorship technology, work with translators, and downstream distributions that do packaging in free software operating system distributions.</p>\n\n<p>In this presentation, we will have a look at what it takes to develop and maintain an anonymity system like Tor and the various other components in the Tor ecosystem. We will look at what The Tor Project has been up to lately, primarily with a focus on core Tor itself. However, we will also have a look at some of our recent developments with anti-censorship technology. Finally, we will have a look at how the participant can contribute to the Tor project.</p>\n\n<p>No prior knowledge of Tor is necessary to participate.</p>",
    "description": "",
    "persons": [
      "Alexander Færøy"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 30,
    "room": "UA2.220 (Guillissen)",
    "title": "OpenPush",
    "subtitle": "Introducing a Free, Decentralized Push Messaging Framework for Android",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>Push messages are an essential part of connected mobile devices. They are also one of the critical missing pieces in the open source Android ecosystem.\nUntil now, free Android apps would either need to implement their own push notification system, do without any push messaging or use the proprietary Google Cloud Messaging service.\nIn this talk I will introduce OpenPush, a free and open source, self-hosted and decentralized replacement for Google Cloud Messaging.</p>",
    "description": "<p>We expect both a long battery life and instant notifications from our mobile devices.\nWhen implementing your own mobile push functionality you can usually optimize for either of these goals.\nThis is especially true if the user is running multiple applications which each come with their own persistent on-going connection for push notifications.\nWanting to combat the battery drain associated with maintaining multiple connections Google introduced the Google Cloud Messaging (GCM) framework which recently has become Firebase Cloud Messaging (FCM). Firebase Cloud Messaging relies on the availability of the proprietary Google Play Services Framework on an Android device. Using FCM also requires the inclusion of the proprietary FCM client library into open source Android apps like Signal, Wire or even Firefox, which makes them effectively non-free software which cannot be distributed via the fully free F-Droid software repository.\nAdditionally all push notifications delivered via FCM need to pass through Google's servers leaving a metadata trace, even if it's an empty wakeup event or if the content of the message is encrypted.</p>\n\n<p>Decentralized, self-hosted systems like Matrix, Nextcloud or RocketChat currently still have a dependency on Google's infrastructure and Terms of Service for delivering push Notifications.</p>\n\n<p>In this talk I'll present a self-hosted, free alternative push messaging implementation which can either run alongside or as a replacement to FCM.\nThe talk will give a general architecture overview as well as walk through the design and implementation challenges of a push messaging service.</p>\n\n<p>Further I'll present how OpenPush can be used by different projects and discuss some additional ideas on how the wider ecosystem could look like in the future.</p>",
    "persons": [
      "Marcus Hoffmann"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 30,
    "room": "UA2.220 (Guillissen)",
    "title": "The Path to Peer-to-Peer Matrix",
    "subtitle": "In which we throw away DNS and run Matrix clientside over libp2p and friends",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>Matrix is an open source project run by the non-profit Matrix.org Foundation dedicated to building an open protocol and communication network for decentralised, encrypted communication - providing a viable open alternative to WhatsApp, Slack, Discord an other proprietary communication silos.  In this talk we will show of the work we've been doing over the last year to shift Matrix from being a decentralised-server architecture to a fully decentralised-client p2p architecture, through running clientside homeservers and experimenting with libp2p and friends as a p2p transport.  We'll also show the route we'll be following over the year to go from proof-of-concept to the live Matrix network.</p>",
    "description": "<p>Traditionally Matrix decentralises communication by replicating conversation history over a mesh of servers, so that no single server has ownership of a given conversation.  Meanwhile, users connect to their given homeserver from clients via plain HTTPS + DNS.  This has the significant disadvantage that for a user to have full control and ownership over their communication, they need to run their own server - which comes with a cost, and requires you to be a proficient sysadmin.  In order to fully democratise communication and eliminate a compulsory dependency on a homeserver, we've started seriously working on making Matrix run as a P2P protocol by compiling homeservers to run clientside and using P2P transports such as libp2p - while seamlessly supporting all existing Matrix clients (e.g. Riot.im), bots and bridges with negligible changes.  This work includes:</p>\n\n<ul>\n<li>Compiling Matrix homeservers (e.g. Dendrite) to efficiently run clientside</li>\n<li>Layering HTTPS over P2P transports such as libp2p (e.g. https://github.com/matrix-org/libp2p-proxy)</li>\n<li>Switching Matrix identifiers from @user:domain tuples to be Curve25519 public keys (<a href=\"https://github.com/matrix-org/matrix-doc/blob/rav/proposal/remove_mxids_from_events/proposals/1228-removing-mxids-from-events.md\">MSC1228</a>)</li>\n<li>Decentralising accounts so they can be hosted concurrently on multiple nodes (e.g. a mix of server-side and client-side homeservers)</li>\n<li>Experimenting with node discovery from DNS to DHTs and other mechanisms (e.g. gossip mechanisms)</li>\n<li>Experimenting with smarter bandwidth-efficient routing algorithms than full-mesh (e.g. combinations of spanning trees, overlapping spanning trees, gossip mechanisms)</li>\n<li>Making Matrix's low-bandwidth CoAP transport production grade</li>\n<li>Experimenting with metadata-protecting relay mechanisms rather than using full homeservers for server-side relaying.</li>\n</ul>\n\n\n<p>In this talk we'll show off our progress so far, and lay out the path forwards over the coming year as we go from proof-of-concept to the live Matrix network.</p>",
    "persons": [
      "Matthew Hodgson"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 30,
    "room": "UA2.220 (Guillissen)",
    "title": "Building a Web App that Doesn’t Trust the Server",
    "subtitle": "Securing ProtonMail",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>How do you know WhatsApp Web isn’t spying on your messages, despite the end-to-end encryption? Why did Signal decide to build a desktop application instead of a web app?</p>\n\n<p>Open Source clients are a necessary, but unfortunately not sufficient, requirement for guaranteeing privacy. This talk explores two other issues: how to securely deliver that source code, and how to securely deliver the encryption keys that users use to communicate. It also presents our proposed solutions to these problems.</p>",
    "description": "<p>At ProtonMail, we’re aiming to build a web application that gives users the guarantee that we are physically unable to read their email, even if we wanted to. This comes with a set of challenges: how can the user trust the source code that comes from the server (without reading it each time), and how can the user trust the public keys that they receive (without hosting key signing parties, however fun they may be :)).</p>\n\n<p>We currently support self-hosting, and manual key verification and pinning as solutions to these issues, respectively. However, these are highly manual solutions. This talk will present two projects we’ve been working on to provide privacy guarantees without requiring any action: Source Code Transparency and Key Transparency.</p>\n\n<p>Finally, we’ll also briefly discuss what kind of APIs we could add to browsers to make it easier to develop web apps that don’t trust the server.</p>",
    "persons": [
      "Daniel Huigens"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 30,
    "room": "UA2.220 (Guillissen)",
    "title": "MaadiX, your cloud in your hands",
    "subtitle": "Tool Kit and Graphical interface for VPS management",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>Avoiding repressive surveillance, circumventing censorship and protecting privacy can become a complicated and costly challenge. Many of the available alternatives do not completely solve the problem of trust, centralization of information and dependency on whoever is offering the services to us. Initiatives that offer alternative tools often become targets of censorship and repressive surveillance. Others do not include all the services the community needs, or require a minimum of technical knowledge, forcing organizations to continue using applications offered by third parties or renounce them.</p>\n\n<p>MaadiX is a solution that reverses this imbalance in favor of users, giving them back control over their communications and data, as well as over all the applications they need in order to process them, facilitating the technical adoption and maintenance of server-side, privacy-oriented, secure, and censorship circumvention technologies.</p>",
    "description": "<p>MaadiX provides one click installation of advanced free and open source applications such as email server, mailman 3, openVPN, RocketChat. Owncloud, Nextcloud, OnlyOffice. Libre Office Online among many others, on remote or local private servers, without the need to have access to their system and data, and  providing updates and technical support.</p>\n\n<p>MaadiX  acts as a repository of 'recipes' that provides all the instructions and commands needed to automatically install and configure applications from a  graphical interface.</p>\n\n<p>The catalogs are served through Puppet modules but we've  changed the way these technologies works, improving them in order to avoid creating yet another centralized google-like model and avoiding having access to users' systems and data.</p>\n\n<p>MaadiX has been reviewed by external security auditors. We would like to dicuss with the audience how to better deal with the balance between security and usability and share how MaadiX works as well as which community is actually around the project or is using it.</p>",
    "persons": [
      "Maddish Falzoni (MaadiX)"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 30,
    "room": "UA2.220 (Guillissen)",
    "title": "Decentralizing OAuth2.0 in a post-GDPR world for full privacy and portability",
    "subtitle": "Automating, API-fying and Tokenizing GDPR for privacy and portability with open source software",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>Users want their data back and the ability to transfer them the way they want to the platform they want. This si user's freedom in a digital world. Today, because of current authorization protocols/framework design like OAuth2.0, power is concentrated to the identity providers who decide what applications they allow to access their API and the user cannot say anything about it.  New regulations like GDPR have appeared to enforce this freedom for users by law but there is not yet tooling for developers to make  GDPR data ownership and  GDPR data portability happen, useful for users to avoid this\nTo really decentralize data permissions from platforms control, make users in control of their privacy and make companies GDPR compliant, you need now to update OAuth2.0 dance into a stateless flow and tokenize the GDPR authorization and agreements to make it programmable for developers.\nIn this talk, Mehdi will explain how you can use open source technologies to automate GDPR requests for your users to, build APIs on top of GDPR takeouts, export GDPR user 3rd-party data in your system and tokenize your GDPR agreements to make them programmable for compliance using opens source technologies.</p>",
    "description": "<p>Making GDPR programmable and adding decentralization of data portability to OAuth2.0</p>\n\n<p>In the classic OAuth 2.0 flows, the authorization server and the resource server are behind the same firewall, giving full power and control about sharing capabilities to the Identity Provider (i.e. Facebook, Amazon, Google etc...). The Identity Provider decides what can be shared to whom via its API, and the user is limited into making data exportable to what the Identity provider allows.\nBecause of new regulations about data portability (GDPR in Europe and CCPA in California), now every user is able to ask a full export of its data to be stored anywhere, breaking Identity Provider monopoly and control. In that context, users can now own fully a copy of their data and share it to who they want. They can now become theoretically independent from previous Identity providers, by becoming their own Identity Provider if they are able to install a server to do so themselves, or theoretically choose the Identity provider that is the best delivering value for them about managing their personal data and permissions.\nAs we seen in Bitcoin, a large majority of users will still want to delegate authorizations to a trusted 3rd-party to manage permissions, as they do until today with banks for their money, or to wallet managers for their Bitcoins/Crytocurrencies. In the Alias protocol ecosystem,users decide where their data is stored (on the server of their choice) and decide the Alias authorization server that will manage its permissions.</p>\n\n<p>Introducing Alias protocol</p>\n\n<p>Alias is a protocol enabling decentralized data export authorizations. When implemented, Alias enables for users to decide to share the data they want, to whom they want, without limitations from any centralized Identity Provider, and in fine grained control.\nTechnically, Alias is a decentralized protocol based on OAuth 2.0, where each user, identified by an cryptographic alias, can let third-parties (\"clients\") access to their data stored in servers (\"resource servers\"). Access to the data is controlled by an Authorization server (\"authorization servers\") that manages permissions and scopes. The main innovation of Alias is that the resource server and the authorization server do not need to be behind the same firewall, enabling users to decide freely and in full control who store their data and who manage permissions in a decentralized way.</p>",
    "persons": [
      "Mehdi Medjaoui"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 30,
    "room": "UA2.220 (Guillissen)",
    "title": "Who will Decentralise the Fediverse?",
    "subtitle": "Self hosting on the Fediverse 3 years on.",
    "track": "Decentralized Internet and Privacy",
    "abstract": "<p>The promise of the internet has not been kind. In mainstream tech and open source alike, social media tech has failed a lot of people. People often face surveillance and abuse over valuable human interaction, or technology for technology's sake.</p>\n\n<p>Software like Mastodon has signaled a significant step forward towards a vision for how we can take existing social media and distribute power so that people can benefit.\nIn many respects, the experience is still not ideal, this talk highlight some of the key point that can make or break the fediverse</p>",
    "description": "<p>The talk will be split in 3 parts.\nMostly looking at the past and the Fediverse history.\nThe present and the set of challenges\nThe future and some proposal on how to overcome those challenges.\nQuesions</p>\n\n<h1>Past:</h1>\n\n<p>The success of Mastodon and other AP compatible software brought a lot of people from different sphere together, and with that diversity the network took off.</p>\n\n<p>Free and Open Source enthusiasts, activists, hacktivists, sex workers, G+ / Tumblr/Twitter refugee, communities of interest (tabletop, craft, parenthood, etc.) and people that simply wanted a more personal place to socialise, all mingled together and really pushed the envelop of what a social network can be.</p>\n\n<p>Within this mix of people and interest, the most marginalized people challenged the status quo and got us better tools than we ever had like post visibility, content warning, image description, etc.  as a standard way of communication.</p>\n\n<h1>Forward to today:</h1>\n\n<p>There are a few major obstacle for the fediverse to operate:\n* A naive vision of moderation and hostile actors constantly puts community moderators on the back foot. Gab and kiwifarm showed us that admins need to be constantly on the lookout if they want to protect their community</p>\n\n<ul>\n<li><p>Activity Pub software, by and large, are not made for efficiency. It's hard to host a Mastodon instance unless you have can afford expensive hosting and storage fees.</p></li>\n<li><p>Designs within AP compatible software is often confusing and not very friendly for newcomers. We have seen how waves of people come to the fediverse and how most of them leave again, unable to comprehend the experience possible.</p></li>\n<li><p>Marginalised communities are less and less heard, diminishing the trust in the current tooling. The fediverse pushing for a mainstream audience means that marginalised communities are now mostly operating in forks and the fringe rather than able to contribute to upstream.</p></li>\n<li><p>Tools and software have an increasing emphasis on emulating proprietary software like Twitter/Youtube/etc. The current direction is replicating mechanism to drive engagement rather than fostering meaningful interactions between people.</p></li>\n<li><p>ActivityPub - the protocol that Mastodon really help get traction, has significant privacy and security flaws. This is one of the fundamental challenge we currenltly have. It is being worked on.</p></li>\n</ul>\n\n\n<h1>Future</h1>\n\n<ul>\n<li>Proposal on some of the key feature and changes that would solve some of the problems mentionned above.</li>\n</ul>\n\n\n<h1>Question</h1>\n\n<p>Question from the audience.</p>",
    "persons": [
      "kyzh"
    ]
  },
  {
    "start": 1577871000000,
    "duration": 25,
    "room": "UB2.147",
    "title": "Tesselle image viewer",
    "subtitle": "Ease viewing and sharing High Resolution images on the web",
    "track": "Open Media",
    "abstract": "<p>Tesselle is an open source image viewer allowing anyone to open, annotate and share big images on the web. It is part of the \"Quinoa\" project family, a suite of digital storytelling tools tailored for the FORCCAST teaching program and the scientific activities of Sciences Po's médialab. (list tools with links ?)</p>",
    "description": "<p>Tesselle is a tool for annotating and publishing large, very large, huge images!\nTo scale with High Res image, it embeds a tiling feature to seamlessly display and navigate them on the web.\nIt allows to comment on specific portions of photographs, maps or visualizations. It gives you the possibility to explore and analyse visual items in detail and precisely.\nFurthermore, Tesselle allows to export your work as a simple folder to publish anywhere on the web.\nThose features have been built to allow scholars to create stories be crafting annotations on cartographic map, artwork analysis, network vizualisations... Using image opens many usecases.</p>\n\n<p>It is built using Typescript, React and Leaflet.\nAs a standalone serverless webapp, Tesselle has met some pitfalls:\n - What is the fastest way to tile an image on the front-end?\n - Are there enough tools in a browser or shall we bring WebAssembly in?\n - How to handle memory management when dealing with hundreds of tiles?\n - Can we beat a \"native\" image viewer?\n - How should we handle sharing and embedding?\n - Is IIIF an appropriate standard?\n - What are our limitations?\nThose are some of the questions we had / have to answer while building Tesselle.</p>\n\n<p>https://github.com/medialab/tesselle/</p>",
    "persons": [
      "Arnaud Pichon"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 25,
    "room": "UB2.147",
    "title": "The Rise and Fall and Rise of JPEG2000",
    "subtitle": "Currently a niche codec, recent enhancements to the JPEG 2000 standard speed it up by 10x and will propel it into the mainstream.",
    "track": "Open Media",
    "abstract": "<p>JPEG 2000 was developed to replace the very successful JPEG standard, but it has instead\nremained a niche code. With recent updates to the standard speeding up decode by 10X, is\nworld domination around the corner ? This talk will describe many of the sophisticated features\nthat JPEG 2000 offers, and discuss why a 20 year old standard may be the codec of the\nfuture.</p>",
    "description": "<p>Released in 2000 as a potential replacement for the wildly successful JPEG standard, JPEG 2000 is a versatile codec with many sophisticated features including:</p>\n\n<pre><code>Superior compression at low bit rates\nStorage of multiple resolutions in a single bitstream\nPrecise rate control without re-compression\nLossy and losssless compression\nProgression by resolution, component, spatial region or quality\n</code></pre>\n\n<p>It is an essential codec in medical imaging, digital cinema and remote sensing. However, due to its high complexity,\nit has remained a niche codec that never gained the popularity of its predecessor.</p>\n\n<p>All of this is about to change with the recently released High Throughput JPEG 2000 standard that speeds up the codec by up to 10x,\nwhile leaving almost all of its features intact. This will propel it into the mainstream, particularly in broadcast and digital cinema.</p>\n\n<p>I will talk about the history of JPEG 2000, give an overview of its features and discuss the upcoming changes.\nI will also talk about current and planned GStreamer support for JPEG 2000.</p>",
    "persons": [
      "Aaron Boxer"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 25,
    "room": "UB2.147",
    "title": "Rendering QML to make videos in Kdenlive",
    "subtitle": "How QML, a language prominently used for designing UI, is being used in editing videos.",
    "track": "Open Media",
    "abstract": "<p>How QML, a language prominently used for designing UI, could be used to create title clips containing text and/or images and rendered using Qt OpenGL libraries, which can then be composited over videos during the video editing process. Kdenlive's Google Summer of Code 2019 project tried to achieve this and is still under active development.</p>",
    "description": "<p>QML is used primarily for UI development in Qt Applications, which means designing and creating is quite easy and comfortable in QML. Kdenlive is a popular non-linear open-source video editor and it currently makes use of XML to describe a title clip (which are clips which contain text or images used to composite over videos) and XML requires more processing on the backend as one needs to explicitly write code for, say an animation of the text. Using QML eases this restriction, making the backend more robust and maintainable. This year (2019), Kdenlive's Google Summer of Code Student tried to achieve this by creating a new rendering backend library and a new MLT QML producer and is still in active development. Also, rendering QML makes use of the Qt Scene Graph, which could possibly have a greater performance since the rendering backend makes use of the Qt Scene Graph.</p>",
    "persons": [
      "Akhil Gangadharan Kurungadathil"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 25,
    "room": "UB2.147",
    "title": "GStreamer on the Magic Leap One",
    "subtitle": "",
    "track": "Open Media",
    "abstract": "<p>Magic Leap One is an augmented reality glasses. Let's run an Open Source Browser (Mozilla Servo) using GStreamer multimedia framework on it.</p>",
    "description": "<p>The Magic Lean One device runs a custom OS called LuminOS, derived from Android with JAVA stripped off. Servo is Mozilla's browser written in Rust that uses GStreamer to render multimedia content.</p>\n\n<p>Presentation of the challenges and solutions found to make it happen:\n- GStreamer's Meson build system.\n- Stagefright-ish API in the SDK.\n- Completely new audio API, with 3D space localization.\n- OpenGL rendering, including stereoscopic SBS.\n- It's now all upstream!</p>",
    "persons": [
      "Xavier Claessens"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 25,
    "room": "UB2.147",
    "title": "GPAC 1.0 Overview",
    "subtitle": "GPAC's past, present and future",
    "track": "Open Media",
    "abstract": "<p>In this talk, we present the next release of GPAC, the complete rearchitecture of its streaming core, the many new features and possibilities of the multimedia framework. Get ready for a lot of OTT/IP streaming and broadcast, encryption, packaging and media composition!</p>",
    "description": "",
    "persons": [
      "Jean Le Feuvre"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 25,
    "room": "UB2.147",
    "title": "IMSC Open Source Projects",
    "subtitle": "How to combine different Open Souce Caption Tools",
    "track": "Open Media",
    "abstract": "<p>IMSC is the Internet Media Subtitle and Caption Profile of the W3C Timed Text Markup Languages. The presentation will show how to combine different open source tools to create, render and validate IMSC subtitles. The focus will be on an open-source editor for IMSC.</p>",
    "description": "",
    "persons": [
      "Andreas Tai"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 25,
    "room": "UB2.147",
    "title": "Which video network streaming protocol should I use?",
    "subtitle": "",
    "track": "Open Media",
    "abstract": "<p>Open source stacks such as GStreamer, ffmpeg and UPipe now implement a large number of different ways to stream audio &amp; video over a network. Just to name a few, there are RTSP, SRT, RIST, WebRTC, HLS, DASH, AES67, SmoothStreaming, RTMP! Some are for local networks and some target the Internet, depending on the use-case, these protocols have different upsides and downsides. To create a successful project, one needs to select the best suited technology. I'll go over the various protocols and explain how they relate to each other and their individual advantages and inconveniences.</p>",
    "description": "",
    "persons": [
      "Olivier Crête"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 25,
    "room": "UB2.147",
    "title": "FOSS in Animation",
    "subtitle": "The state of Free and Open Source software in the Animation and VFX Industry",
    "track": "Open Media",
    "abstract": "<p>The Animation industry has always been ruled by proprietary software, mainly from Autodesk, Adobe and The Foundry. But recently we noticed a rise of interest in software like Blender or Krita. Alongside them, initiatives like the Academy Software Foundation are popping. Last but not least, more and more studios publish the sources of their in-house software. During this conference, we'll explain how a typical production pipeline works. Then, we'll discuss how open source impacts animation productions and what we can expect for the future. As a conclusion, I'll explain how studios collaborate more together through free and open-source software.</p>",
    "description": "",
    "persons": [
      "Frank Rousseau"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 25,
    "room": "UB2.147",
    "title": "dav1d: 1 year later",
    "subtitle": "dav1d is a fast AV1 decoder",
    "track": "Open Media",
    "abstract": "<p>dav1d is an open source decoder for the AV1 format, focused on being fast and lean.</p>",
    "description": "<p>It was started a bit more than one year ago. This is a talk to see where this project is now, how fast we achieved for decoding AV1 samples and what is left to do on this project.</p>",
    "persons": [
      "Jean-Baptiste Kempf"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 25,
    "room": "UB2.147",
    "title": "rav1e - 0.3.0 and after",
    "subtitle": "What we did so far and what will do in the future",
    "track": "Open Media",
    "abstract": "<p>rav1e is an opensource Av1 encoder.</p>\n\n<p>We'll see what makes it fairly unique beside the choice of using Rust as main development language.</p>",
    "description": "<p>We'll see what we did in the past releases, what design choices we took and what we plan to do in the next two releases.</p>\n\n<p>By February we will have the release 0.2.0 and the release 0.3.0 out. I'll present what's coming in the release 0.4.0 and 0.5.0.</p>\n\n<p>This will include some performance evaluation and describing some of the features are currently unique in rav1e.</p>",
    "persons": [
      "Luca Barbato"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 25,
    "room": "UB2.147",
    "title": "Spleeter by Deezer",
    "subtitle": "Open-Sourcing a Machine-Learning Music Source Separation Software",
    "track": "Open Media",
    "abstract": "<p>Source separation, stem separation, de-mixing are all different ways of referring to the same problem of recovering the mono-instruments tracks that were mixed together to produce a music file. Recently, the research team at Deezer released a free and open source software as well as trained models to perform multi-source separation of music, with state-of-the-art accuracy.\nIn this presentation we come back on our journey to open sourcing the Spleeter library, from doing the ground research, training the models, to releasing them. We put emphasis on the technological challenges that had to be solved as well as the practical and legal considerations that came into play.</p>",
    "description": "<p>Released on october 29th 2019, the Spleeter (https://github.com/deezer/spleeter) github repository received more than 5000 stars on its first week online and numerous positive feedbacks as well as press coverage. This talk will explain how we went from research code to this fairly easy to use open Python library, that integrates pre-trained models for inference and re-training.</p>\n\n<p>While not a broadly known topic, the problem of source separation has interested a large community of music signal researchers for a couple of decades now. It starts from a simple observation: music recordings are usually a mix of several individual instrument tracks (lead vocal, drums, bass, piano etc..). The task of music source separation is: given a mix can we recover these separate tracks (sometimes called stems)? This has many potential applications: think remixes, upmixing, active listening, educational purposes, but also pre-processing for other tasks such as transcription.</p>\n\n<p>The current state-of-the-art systems start to give convincing results on very wide catalogs of tracks, but the possibility of training such models remains largely bound by training data availability. In the case of copyrighted material like music, getting access to enough data is a pain point, and a source of inequality between research teams. Beside, an essential feature of good scientific research is that it must be reproducible by others. For these reasons and to even the playing field, we decided to not only release the code, but also our models pretrained on a carefully crafter in-house dataset.</p>\n\n<p>Specific topics on which our presentation will dwell on are:\n- technical aspects of the models architecture and training\n- software design, and how to leverage tensorflow’s API in a user facing python library\n- how to package and version a code that leverages pre-trained models and that can be run on different architectures: CPU and GPU.\n- licensing and legal concerns\n- what we learned along the way\n- legacy</p>",
    "persons": [
      "Anis Khlif",
      "Félix Voituret"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 25,
    "room": "UB2.147",
    "title": "Functional audio and video stream generation with Liquidsoap",
    "subtitle": "",
    "track": "Open Media",
    "abstract": "<p>The talk will give a general overview of the Liquidsoap language, and put focus on recent new features: support for HLS, efficient video, etc.</p>",
    "description": "<p>The task of generating multimedia streams such as for webradios or live youtube channels is a complicated task. One needs to face low-level issues (properly encoding and distributing the streams), mid-level issues (performing normalization, signal processing, color grading, etc.) and high-level issues such as generating the stream from a wide variety of sources (local files, other streams, live interventions, user requests, etc.) and properly combining them (performing transitions, adding commercials, vary the contents during the day, etc.). In this talk, we present Liquidsoap, a dedicated high-level functional language, which allows performing all these tasks in a modular way, with strong guarantees that the stream will not fail on the long run.</p>",
    "persons": [
      "Romain Beauxis"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 25,
    "room": "UB2.147",
    "title": "Building an Open-Source based audio streaming platform",
    "subtitle": "",
    "track": "Open Media",
    "abstract": "<p>Learn how Radiofrance leverage open-source software to transport, encode, deliver and monitor audio stream in the cloud. You will get a global infrastructure overview on a platform that serve audio stream at scale.</p>",
    "description": "<p>What we will talk about:\nHow we chose our audio streaming protocol (HLS and Icecast).\nHow to transport audio from corporate SI to the cloud with the protocol SRT.\nHow we use Liquidsoap as streaming server to implement high availability logic, encode and mux our stream.\nHow we monitor our system with Prometheus and Grafana.\nHow to scale an audio streaming platform for 80 radios and 200K+ concurrent listeners.</p>",
    "persons": [
      "Maxime Bugeia"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 15,
    "room": "UB2.147",
    "title": "The moldability of mpv",
    "subtitle": "Deploying diverse media setups in our museum",
    "track": "Open Media",
    "abstract": "<p>Many museums around the world use commercial closed source solutions to present media and interact with their visitors. Biodiversity museum Naturalis has decided to present most of their interactive content using open source software. In this presentation we talk about our experiences with mpv as the go to tool for a diverse selection of media setups in our brand new museum and how we made it work with show controllers, Arduino devices and open source content and config management tools.</p>",
    "description": "",
    "persons": [
      "David Heijkamp"
    ]
  },
  {
    "start": 1577895600000,
    "duration": 40,
    "room": "UB2.147",
    "title": "Getting Your Virtual Hands On RIST",
    "subtitle": "",
    "track": "Open Media",
    "abstract": "<p>There are a number of error correction protocols that provide backwards error correction. These are commonly used to transport media streams from remotes to the content provider, or the content provider to distribution. They allow, for example, streams from a pro basketball game to be transported over public Internet from stadium to network NOC without error; or as another example, packages of ethnic TV channels, to be moved from continent to continent. Players include DVEO, which uses the proprietary Dozer protocol for which the speaker holds the patent; WOWZA uses a customized SRT which is based on open source, and a few more. They all work on the principle of shooting off a bunch of udp packets from one IP to another, setting up a buffer, and then using an automatic re-request mechanism to request re-sends of lost or corrupted udp packets. RIST was designed with the participation of several vendors to bring some of the features normally found in proprietary error correction protocols into the free and open source world. It may even become a \"lingua franca\" between vendors. VLC, upipe and gstreamer can already reassemble and play back RIST transported streams. We will talk about a new open source project that provides an easy to use lib for rist and we'll discuss two pre-packaged images we've made available for AWS, Azure, VMWare and KVM. With these images, you can send a RIST encoded stream from cloud to end user viewer, or from cloud to cloud.</p>",
    "description": "",
    "persons": [
      "Sergio Ammirata"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 30,
    "room": "UB2.252A (Lameere)",
    "title": "The State of Go",
    "subtitle": "What's new since Go 1.12",
    "track": "Go",
    "abstract": "<p>Go 1.14 is planned to be released in February 2019 and this talk covers what's coming up with it.</p>\n\n<p>We'll talk about new features and fixes in Go, new proposals for Go 2. All of the new things you might have missed.</p>",
    "description": "<p>This has been a staple talk of the Go devroom, opening the stage every year, and has always been a successful one.</p>",
    "persons": [
      "Francesc Campoy",
      "Maartje Eyskens"
    ]
  },
  {
    "start": 1577871000000,
    "duration": 30,
    "room": "UB2.252A (Lameere)",
    "title": "Functional Programming with Go",
    "subtitle": "",
    "track": "Go",
    "abstract": "<p>Are you tired of seeing Object Oriented code everywhere, with mutations, side-effects and headaches? Luckily, writing Go does not have to be that way! Functional programming is perfectly possible within Go, hence we can leverage FP techniques to make our code more robust, testable and fun.</p>",
    "description": "<p>Functional Programming in Go\nGo is a multi-paradigm language, yet most code you encounter ‘in the wild’ is written in a mostly Object-Oriented way. Yet, Go allows us to write code in a functional way as well, which offers certain advantages over a more traditional “OO” approach. What follows in this description is also the general flow of how it will be presented.</p>\n\n<p>What is FP?\nFirst we’ll start by defining what we mean by functional programming. Superficially Go might not look like what you expect a functional language to look like. But we’re really just missing the syntactic sugar here, as a lot of the underlying concepts that are central to Functional Programming are reflected in Go. Hence it is important to take a look at what many programmers consider requirements for being “functional” such as: Higher-order functions, recursion (with tail-call optimization), purity, idempotence, .. and how these requirements are met (or aren’t met) by Go.</p>\n\n<p>How to leverage them in Go\nOnce we have convinced ourselves that Go gives us the building blocks for writing FP code, we’ll dive into some concrete examples of what we can do with Go. We’ll look at function currying, continuation-passing style programming, pure functions, recursion (without TCO), monads and more. It’s important to highlight here why we want to use these constructions and when. At best case, you’ll learn how to leverage them in your own codebase. At worst, you’ll have seen some cool things with Go. Don’t be put of if you don’t know these terms yet, we’ll start with the easy concepts you’ve probable already used such as recursion, and work our way up to the complexer ones.</p>\n\n<p>Using libraries\nGo actually has libraries that provide an API for programming in a more functional style. We’ll give them an honorable mention but they won’t be the focus of the talk, as you can get started easily without them. But, they do offer certain things we “miss” in Go by default (like Generics).</p>\n\n<p>Benefits\nWriting Go code in this style has numerous benefits over our traditional approach. My goal of this talk is not just to show you cool things you can do with Go, but also why you want to them. You’ll also see introducing them to an existing codebase is easy, and that FP is really not as scary as it might sound!</p>\n\n<p>Downsides\nUsing this style of programming is not entirely a walk in the park. There’s a price to be paid for writing functional code in Go, the main one will be that you’ll take a performance hit. But the performance hit might not be where you expect it! Functional programming is one tool in your toolbox, it’ll greatly empower you to solve certain problems, whilst it’ll help you shoot yourself in the foot for other problems.</p>\n\n<p>Bonus benefits!\nYes, you’ll even take away something from this talk you might not have expected! A lot of people think of Haskell when they hear functional programming. Which might have scared them away from functional programming. In this talk you’ll get a look at functional programming with a familiar syntax and a language you already love. This will help you understand the underlying concepts and see how they relate to Haskell and other functional languages, where the syntax might be a bit different but the idea remains the same.</p>\n\n<p>Do I need prior knowledge of FP?\nNo, absolutely not! You don’t need to have done functional programming before to benefit from this talk. There are concepts for all levels of understanding of functional programming. If you don’t know anything about functional programming yet, you’ll discover it in this talk. And if you’re already an FP-wizard who dreams in Haskell, you’ll learn how to transfer that understanding to Go.</p>",
    "persons": [
      "Dylan Meeus"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 30,
    "room": "UB2.252A (Lameere)",
    "title": "Porting Go to NetBSD/arm64",
    "subtitle": "",
    "track": "Go",
    "abstract": "<p>An introduction to calling conventions, thread-local storage, signal handling and how they relate to Go, in the context of my new port of Go to NetBSD/arm64.</p>",
    "description": "<p>Running a weird operating system comes at the cost of having to adjust software to run on it.\nGo is probably one of the hardest projects to adjust.\nDoing so required learning the guts of:\n- ARM64 calling conventions\n- signal handling\n- thread-local storage\n- a lot of Go-specifics</p>\n\n<p>Which will be discussed in this lecture.</p>",
    "persons": [
      "Maya Rashish"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 30,
    "room": "UB2.252A (Lameere)",
    "title": "Build real-world gaming hardware with TinyGo",
    "subtitle": "Make toys and other contraptions that run on Go",
    "track": "Go",
    "abstract": "<p>Learn the multiples and fun possibilities of using Go on microcontrollers like Arduino to make gaming related hardware.</p>",
    "description": "<p>After a brief overview of the new features of TinyGo (http://tinygo.org), we'll move onto some cool and easy to make smart-toys that run Go. From classic PONG and a \"Simon says\" device to a pocket gaming console, and some other surprises. We'll end with an Open LED Race competition and the possibility to win hardware and make your our TinyGo device.</p>\n\n<p>This talk will feature bright lights and sounds, maybe lasers too.</p>",
    "persons": [
      "Daniel Esteban"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 30,
    "room": "UB2.252A (Lameere)",
    "title": "Diversity, Finally",
    "subtitle": "",
    "track": "Go",
    "abstract": "<p>What if we decided to solve, once and for all, the problem of underrepresentation in the Go community of women, gender minorities, people of color, or any other group the same way we handle our problems in production, by identifying \"bugs\" and then fixing them?  Can it even be done? What if we took the engineering approach? Ronna is planning to convince you it's not a matter of if, but a matter of how, and we are going to analyze some of the statistics, find where the problems actually lay, and build a Trello card full of achievable tasks to address them.</p>",
    "description": "<p>What if we decided to solve, once and for all, the problem of underrepresentation in the Go community of women, gender minorities, people of color, or any other group the same way we handle our problems in production, by identifying \"bugs\" and then fixing them?  Can it even be done? What if we took the engineering approach? Ronna is planning to convince you it's not a matter of if, but a matter of how, and we are going to analyze some of the statistics, find where the problems actually lay, and build a Trello card full of achievable tasks to address them.</p>",
    "persons": [
      "Ronna Steinberg"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 30,
    "room": "UB2.252A (Lameere)",
    "title": "From Go to Kubernetes CRDs and Back",
    "subtitle": "Workflow for building strongly typed APIs",
    "track": "Go",
    "abstract": "<p>Kubernetes is built using Golang. CustomResourceDefinitions are the primary extension points for bringing custom data into a Kubernetes cluster. This hands-on talk is about the workflow of API definitions in Golang, generation of OpenAPI schemas as part of the CRD, client and informer generation and how to use these to process data in real-time using logic implemented in Golang controllers.</p>",
    "description": "<p>This hands-on talk is an introduction to the creation of CustomResource based API extensions for Kubernetes clusters. Following an example project we will</p>\n\n<ul>\n<li>define API objects live on stage</li>\n<li>turn that into CRD definitions with OpenAPI schema</li>\n<li>install the CRD into a Kubernetes cluster</li>\n<li>generate typed Golang clients and informers</li>\n<li>build Golang based realtime logic reacting to changes to the CustomResource objects.</li>\n</ul>\n\n\n<p>The talk does not require any knowledge about Kubernetes, just some Golang experience for unterstanding API type definitions.</p>",
    "persons": [
      "Stefan Schimanski"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 30,
    "room": "UB2.252A (Lameere)",
    "title": "Deterministic debugging with Delve",
    "subtitle": "And the state of Delve",
    "track": "Go",
    "abstract": "<p>In this talk I will begin by delivering the \"State of Delve\" update. In similar fashion to the popular \"State of Go\" talk I will begin by discussing all of the exciting new features and changes that have happened over the past year, since last FOSDEM. Following that I will go into a live demo showcasing how Delve can leverage Mozilla RR in order to perform deterministic debugging. This talk will begin by introducing users to the concept of deterministic debugging and the power that comes with it. Following that I will launch into a demo showcasing how to leverage this concept to debug an otherwise unpredictable highly concurrent program. Users will walk away with immediate practical knowledge they can begin applying in their day to day debugging.</p>",
    "description": "<p>It's been a year since last FOSDEM and a lot has changed with Delve! I will discuss all the new features and changes that have been implemented in the last year.</p>\n\n<p>Following the \"State of Delve\" introduction, I will dig into how Delve can be utilized to perform deterministic debugging. This style of debugging enables users to record the execution of their process and \"play it back\" in a deterministic fashion in order to more quickly and efficiently perform root cause analysis on a bug that may otherwise be difficult to reproduce or track down. This section of the talk will begin by introducing the concept of deterministic debugging and why it is so useful and powerful. Once everyone is familiar with the concept I will launch into a live demo showcasing how to leverage this debugging approach to track down and fix a bug which is hard to reproduce and happens only intermittently.</p>\n\n<p>Attendees will walk away with practical knowledge that they can begin applying to their debugging problems immediately.</p>",
    "persons": [
      "Derek Parker"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 30,
    "room": "UB2.252A (Lameere)",
    "title": "Classify things in Go: the easy way.",
    "subtitle": "Building classifiers quickly with the community contributions.",
    "track": "Go",
    "abstract": "<p>Go and public training models can provide great potential: A fast way to build \"eyes around the world\", also known as classifiers. And with great powers, come great opportunities, such as building fantastic applications to turn our world a better place to live through the technology with few steps.</p>",
    "description": "<p>Go and public training models can provide great potential: A fast way to build \"eyes around the world\", also known as classifiers. And with great powers, come great opportunities, such as building fantastic applications to turn our world a better place to live through the technology.\nThe GO language, have  GoCV package, and it provides the most modern and advanced Computational Vision libraries that exist like OpenCV.\nIn this talk, I'll demonstrate how to use public models from TensorFlow Hub and OpenCV library to easily build classifiers for APIs, taking a super leap from draft to a working classifier, in a few steps!\nThe idea is to demystify the concepts behind classifiers and show how to build one in a few steps and make rankers accessible to the business, showing how GO does this in a unique, scalable and self-performing way, and of course, encouraging the community to contribute and sharing more training models to they can turn more and more accurate!</p>",
    "persons": [
      "Sheimy Rahman"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 30,
    "room": "UB2.252A (Lameere)",
    "title": "Dragons of CGO",
    "subtitle": "Hard-learned Lessons from Writing Go Wrappers",
    "track": "Go",
    "abstract": "<p>YottaDB is a mature, hierarchical key-value, free / open source NoSQL database whose is used in enterprise-scale mission-critical applications in banking and healthcare, and also scales down to fit on IoT devices like the Raspberry Pi Zero, as well as applications in-between (like the University of Antwerp library catalog system). When a customer funded us to develop a Go API to YottaDB, we thought it would be a straightforward project. But it was a very painful exercise for us. The presentation discusses the problems we faced, and how we overcame them.</p>",
    "description": "<p>Go is a popular language for writing highly concurrent software, and works well when used in isolation. Using Go alongside software written in other languages (such as C) can be done, but there are some hidden dragons to keep an eye out for. In addition to obvious problems, such as calling C variadic functions, other more subtle problems are hidden deep in the depths of Go documentation.</p>\n\n<p>Consider callback functions; how does one pass a function pointer from Go to C to provide a callback? There are strict limitations enforced by Go on what pointers may be passed to C routines, intended to prevent faults resulting from Go structures being garbage collected without knowledge of the C code. Without being able to pass pointers, even function pointers, how do we “pass” a callback function to the C code to callback into? Furthermore, how do we pass data to the callback function, since we can’t pass Go structures?</p>\n\n<p>Given that we can’t pass Go structures to C code, at some point we will need to allocate C structures to store data for the C code to operate on. Go promises one thing about memory allocated in C land; it will not keep track of it for you. The garbage collector will gladly clean up any Go structures no longer needed, but will not clean up the associated C memory. How can one write code which isn’t likely to result in memory leaks, using this model?</p>\n\n<p>Perhaps the most difficult challenge to overcome is that Go makes no promises about what thread is running code. Go does its best to hide the identity of Go routines from the user, so they won’t rely on this metadata for handling code execution. This presents a problem for many C applications, which often use POSIX mutexes to control access, and the owner of a mutex needs to belong to a specific thread. How can one write Go applications that allow the concurrency Go users expect, without trashing the libraries they are calling?</p>\n\n<p>Of course, none of the solutions we talk about here are any good unless you can compile your program. The hidden dragons of Go also lurk behind the “go build“ command; fitting in the required C flags requires knowledge of, among other things, pkg-config, a systems tool used behind the scenes by programs like CMake and autotools.</p>\n\n<p>We had to tackle all these problems, and many more, during the development of the YottaDB Go wrapper (https://gitlab.com/YottaDB/Lang/YDBGo and https://yottadb.com). This presentation hopes to pass some of our hard-learned lessons to other programmers who will use Go to interface with non-Go libraries and utilities.</p>",
    "persons": [
      "K.S. Bhaskar"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 30,
    "room": "UB2.252A (Lameere)",
    "title": "Advanced debugging techniques of Go code",
    "subtitle": "",
    "track": "Go",
    "abstract": "<p>In an ideal world, you would write Go code, compile it, and then it would work perfectly the first time. But unfortunately it doesn't work in this manner. There are many different books and articles about how to write good code in go, but not so many how to debug code efficiently. In my talk I'll try to cover such important topic.\nGo is a new programming language with best tools for development. In my talk I'll cover how to efficiently using these tools to debug your code. I’ll start from history of debuggers, later I'll show you how to debug go itself, if you need to find bug in language. Than I can demonstrate  how to effectively debug microservices using docker and k8s, what’s remote debugging and how to apply it to application which already has been deployed. Debugging unit tests and not only code. Some tricks of debugging command line applications.</p>",
    "description": "<p>My talk is about:\n- compare go debuggers (delve, gdb) in real world applications;\n- how to effectively debug inside containers (using remote-debuggers)\n- how to use Mozilla rr to record and play you golang app (https://rr-project.org/)\n- how to use dig into slices using gdb\nI'm using the term docker and k8s to show how to debug applications in different environments without lot's of details of k8s, rather showing tips/tricks to speed up you microservices.</p>",
    "persons": [
      "Andrii Soldatenko"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 30,
    "room": "UB2.252A (Lameere)",
    "title": "Debug code generation in Go",
    "subtitle": "",
    "track": "Go",
    "abstract": "<p>If you are interested to learn about what and how Go generates machine code, this talk is for you. By learning more about the compilation, you can either avoid unnecessary hand-crafted optimizations or learn more about the compiler to become a contributor to the Go compiler.</p>",
    "description": "<p>Have you ever optimized some Go code to later realize Go compiler is already doing the same optimization automatically? Have you ever tried to understand what makes of of the Go compiler? Or, have you ever wondered how can you inspect machine code generated from Go source code? If you are interested to learn about what and how Go generates machine code, this talk is for you. By learning more about the compilation, you can either avoid unnecessary hand-crafted optimizations or learn more about the compiler to become a contributor to the Go compiler.</p>",
    "persons": [
      "Jaana Dogan"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 30,
    "room": "UB2.252A (Lameere)",
    "title": "Uplift your Linux systems programming skills with systemd and D-Bus",
    "subtitle": "Practical examples and best practices on how to leverage systemd and D-Bus in Go",
    "track": "Go",
    "abstract": "<p>Systemd is a de-facto standard process manager in all mainstream Linux distributions for almost a decade.\nD-Bus is most widely used inter-process communication on a local host. It's used in many core apps on Linux Desktop.</p>\n\n<p>Yet both systemd and D-Bus are undervalued.\nVery often, programs that are only intended to run on Linux attempt to re-implement (with bugs) what systemd and D-Bus already provide\n(for example: watchdog function, reliable process termination, notifying another program about some event, coordination between multiple processes).</p>\n\n<p>The goal of this talk is to shift perspective on systemd and D-Bus (using concrete practical examples in Go),\nand show how basic building block these systems provide can be re-used in software you write for modern Linux system.</p>",
    "description": "<p>This is an exploratory talk. Then intent is to look at systemd and DBUS from a different angle.</p>\n\n<p>Most of current tutorials about systemd focused on operating a service like apache, nginx or redis.\nD-Bus tutorials are very abstract, basic and lack any concrete useful use-cases.</p>\n\n<p>I plan to present few recent additions to systemd, such as portable services and resource control.\nAs well as re-introduce few existing concepts, like sd-notify, watchdogs and transient units.</p>\n\n<p>On D-Bus I plan to show how to use bus abstraction and few neat features,\nlike passing file descriptors and receiving notifications.</p>\n\n<p>The focus is on how to not re-invent things that systemd and D-Bus do much better.</p>\n\n<p>Examples are given as a few simple Golang programs, with full source available on https://github.com/lvsl/tutorial-go-systemd-dbus.</p>\n\n<p>The indented audience is anyone who write and operate Go code on Linux.\nPreferred experience of the audience: basic knowledge of Linux and Golang, familiarity with systemd and D-Bus concepts would be useful as well.</p>",
    "persons": [
      "Leonid Vasilyev"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 30,
    "room": "UB2.252A (Lameere)",
    "title": "Deep Learning For Gophers",
    "subtitle": "",
    "track": "Go",
    "abstract": "<p>The software has not eaten the world yet, but infact has changed the way it was before. That software has given us, the human a new superpower which is the power of artificial neural networks. The goal of those networks is to help us answer the question : “Given X, predict Y with Z% accuracy”. This is where Deep Learning comes into picture. Let’s build a basic building block of deep learning :  neural network.</p>",
    "description": "",
    "persons": [
      "Rashmi Nagpal"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 30,
    "room": "UB2.252A (Lameere)",
    "title": "Speed up the monolith",
    "subtitle": "building a smart reverse proxy in Go",
    "track": "Go",
    "abstract": "<p>GitLab is a ruby on rails application, but this didn’t prevent us from having fun with Go.\nLearn how we decomposed our monolith by writing a smart reverse proxy in Go that handles I/O intensive operations.\nA technique that every web app can use, regardless of the company stack.</p>",
    "description": "<p>We set a deadline for releasing a cloud-native version of GitLab and put a team of engineers to work planning the helm charts, splitting several components into independently scalable PODs. The team faced a few challenges.</p>\n\n<p>GitLab’s main codebase is written in Ruby, which has a global interpreter lock. We relied on NFS to asynchronously upload files from our workers fleet. Removing shared file system by uploading directly from the controller was not an option. We wanted to move to an object storage based solution, but that was a paid feature and we had to port it to the open-source codebase. Oh, we also needed to make sure the rest of our engineers could keep shipping new features at our regular monthly cadence.</p>\n\n<p>At the same time, we were planning our infrastructure migration from Azure to Google Cloud. Removing this intermediate state, where a file is on GitLab server NFS but not yet uploaded to the object storage, would have made the migration a lot easier.</p>\n\n<p>We had to remove the NFS dependency to make GitLab easily deployable on Kubernetes and we needed a performant multi-cloud object storage uploader viable also for on-prem installations, a solution that would work for a single server setup up to Ggitlab.com scale.</p>\n\n<p>Luckily we already had written workhorse, a smart reverse proxy written in Go for handling git operations. It was time to extend workhorse capabilities leveraging the full power of goroutines.</p>\n\n<p>We had a plan, but the devil is in the detail. Allow me to guide you through this journey. During the talk I’ll tell you how a ruby-on-rails company began to write Go code, how we implemented an object storage uploader inside our proxy, the problems we faced, and tradeoffs we took to deliver this in time.</p>",
    "persons": [
      "Alessio Caiazza"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 60,
    "room": "UB2.252A (Lameere)",
    "title": "Go Lightning Talks",
    "subtitle": "Come speak!",
    "track": "Go",
    "abstract": "<p>At the end of the day we will have lightning talks of 8 minutes in the Go Devroom!\nEach talk will be 8 minutes long, the CfP for these is open till a few hours before the talks start to give everyone the chance to submit a proposal.</p>",
    "description": "",
    "persons": []
  },
  {
    "start": 1577871000000,
    "duration": 120,
    "room": "UB4.132",
    "title": "LPI Exam Session 3",
    "subtitle": "",
    "track": "Certification",
    "abstract": "<h3>LPI offers discounted certification exams at FOSDEM</h3>",
    "description": "<p>As in previous years, the Linux Professional Institute (LPI) will offer discounted certification exams to FOSDEM attendees.\nLPI offers level 1, level 2 and level 3 certification exams at FOSDEM with an almost <strong>50% discount</strong>.</p>\n\n<p>For further information and instructions see <a href=\"https://fosdem.org/certification\">https://fosdem.org/certification</a>.</p>",
    "persons": [
      "LPI Team"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 60,
    "room": "UB4.132",
    "title": "LibreOffice Exam Session 1",
    "subtitle": "",
    "track": "Certification",
    "abstract": "<p>LibreOffice Certifications are designed to recognize professionals in the areas of development, migrations and trainings who have the technical capabilities and the real-world experience to provide value added services to enterprises and organizations deploying LibreOffice on a large number of PCs.</p>",
    "description": "<p>In the future, LibreOffice Certifications will be extended to Level 1 and Level 2 Support professionals.</p>\n\n<p>The LibreOffice Certification is not targeted to end users, although Certified Training Professionals will be able to provide such a service upon request (although not as a LibreOffice Certification). In general, end user certification is managed by organizations with a wider reach such as the Linux Professional Institute.</p>",
    "persons": [
      "LibreOffice Team"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 60,
    "room": "UB4.132",
    "title": "LibreOffice Exam Session 2",
    "subtitle": "",
    "track": "Certification",
    "abstract": "<p>LibreOffice Certifications are designed to recognize professionals in the areas of development, migrations and trainings who have the technical capabilities and the real-world experience to provide value added services to enterprises and organizations deploying LibreOffice on a large number of PCs.</p>",
    "description": "<p>In the future, LibreOffice Certifications will be extended to Level 1 and Level 2 Support professionals.</p>\n\n<p>The LibreOffice Certification is not targeted to end users, although Certified Training Professionals will be able to provide such a service upon request (although not as a LibreOffice Certification). In general, end user certification is managed by organizations with a wider reach such as the Linux Professional Institute.</p>",
    "persons": [
      "LibreOffice Team"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 60,
    "room": "UB4.132",
    "title": "LibreOffice Exam Session 3",
    "subtitle": "",
    "track": "Certification",
    "abstract": "<p>LibreOffice Certifications are designed to recognize professionals in the areas of development, migrations and trainings who have the technical capabilities and the real-world experience to provide value added services to enterprises and organizations deploying LibreOffice on a large number of PCs.</p>",
    "description": "<p>In the future, LibreOffice Certifications will be extended to Level 1 and Level 2 Support professionals.</p>\n\n<p>The LibreOffice Certification is not targeted to end users, although Certified Training Professionals will be able to provide such a service upon request (although not as a LibreOffice Certification). In general, end user certification is managed by organizations with a wider reach such as the Linux Professional Institute.</p>",
    "persons": [
      "LibreOffice Team"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 40,
    "room": "UB4.136",
    "title": "Continuous Delivery starts with Continuous Infrastructure",
    "subtitle": "",
    "track": "Continuous Integration and Continuous Deployment",
    "abstract": "<p>Most organisations start their journey towards Continuous Delivery with their development teams, or often their web or mobile teams. I’ve seen many of these journeys fail because “ops” was not included in the picture.  The organisation assumed DevOps didn’t need ops. So the team didn’t adapt, didn’t provide the right stacks, couldn’t support the tools. I’ve started a number of successful journeys with the ops teams doing Continuous Delivery of their infrastructure as code. They changed their mindset, allowing them to understand, support and onboard the development teams.  This talk will document that approach with some supporting cases and examples.</p>\n\n<p>Taking one step further we'll showcase a on how to do Continuous Delivery of your Infrastructure as Code,    obviously with Open Source tools</p>",
    "description": "",
    "persons": [
      "Kris Buytaert"
    ]
  },
  {
    "start": 1577871900000,
    "duration": 40,
    "room": "UB4.136",
    "title": "An event based approach for CI/CD pipelines",
    "subtitle": "What challenges are there in the communication between different tools in CI/CD ecosystems and how can they be mitigated?",
    "track": "Continuous Integration and Continuous Deployment",
    "abstract": "<p>How can we listen to when new upstream software has been tested to the extent that we feel comfortable integrating it into our software? How can we communicate about new artifacts available for others to integrate? How can we see what has been integrated where? How can we achieve traceability across pipelines run on different tooling infrastructure? How can we visualize our pipelines to follow changes from source code to customer deployment?</p>\n\n<p>We will describe these challenges and show how we tackled them using self-documenting integration pipelines providing traceability and visualization to benefit multiple needs in the organization. We will present based on our experience from large-scale software development.</p>",
    "description": "",
    "persons": [
      "Emelie Pettersson",
      "Fredrik Fristedt"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 40,
    "room": "UB4.136",
    "title": "Mario’s adventures in Tekton land",
    "subtitle": "Testing, releasing and deploying Tekton with Tekton",
    "track": "Continuous Integration and Continuous Deployment",
    "abstract": "<p>In this talk, the speakers will present their experiences about using Tekton - a cloud-native pipeline system - to test, release and continuously deploy itself.</p>",
    "description": "<p>Tekton is a Kubernetes-native, lightweight, easy to manage CI/CD pipelines engine. Pipeline building blocks can be reused, version controlled and curated in a catalogue that embeds best practices. Tekton, hosted by the CD Foundation, aspires to be the common denominator in CI/CD, modelling what Kubernetes has become in cloud-native application development. The Tekton team wanted to make sure that the project is going in the right direction by \"dogfooding\" i.e. by using Tekton to run its own automation \"plumbing\". The initial continuous integration setup embedded most of the testing pipelines in bash scripts. The speakers replaced this with Tekton, hence improving the readability of the pipelines and the reproducibility of CI runs. Eventually, they moved onto continuously delivering Tekton and its pipelines via Tekton. In this talk, the speakers will tell their experiences about using a cloud-native pipeline system to test, release and continuously deploy itself.</p>",
    "persons": [
      "Andrea Frittoli",
      "Vincent Demeester"
    ]
  },
  {
    "start": 1577877300000,
    "duration": 40,
    "room": "UB4.136",
    "title": "Test Software On Emulated Hardware In Containers... In The Cloud",
    "subtitle": "",
    "track": "Continuous Integration and Continuous Deployment",
    "abstract": "<p>Modernizing the traveler information systems of an international railway and transportation company, including the modernization and renewal of traveler facing devices at the train stations. For a variety of devices ranging from 20-year-old x86 PC104 based embedded systems up to modern 64bit multi-core systems, a Buildroot based Linux system, and a custom application stack is being developed.</p>",
    "description": "<p>In this talk, we will show how we use a fully automated CI pipeline to build our custom application components resulting in deployable Linux disk images. These images are then containerized and deployed on our Kubernetes cluster. Using Qemu in our containers allows us to simulate external hardware normally connected through serial interfaces and is the basis for automated tests.</p>\n\n<ul>\n<li>presentation of our technology stack.</li>\n<li>multi-stage Linux disk image creation in the CI.</li>\n<li>running embedded device images in a Kubernetes cluster.</li>\n<li>automated testing of simulated embedded systems.</li>\n</ul>",
    "persons": [
      "Sean A. Parker",
      "Paul Schroeder"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 15,
    "room": "UB4.136",
    "title": "Infrastructure CICD with KubeVirt and Tekton",
    "subtitle": "",
    "track": "Continuous Integration and Continuous Deployment",
    "abstract": "<p>Over the past few years, we’ve all realized the benefits of CICD. The more that we test our code and the faster that we can have these changes integrated, the more value we derive from these systems (and our code). However, in general this only ever seems to be applied to application code. We sometimes forget that no matter how fast deploy our applications -- if the underlying infrastructure components are not benefiting from this same process, we’ll only ever be as good as these weaker links. But why are things like this? Because testing these components can be hard and often access to these systems are highly restricted (for good reason!). But with tools like KubeVirt and Kubernetes, we don’t need this access  just to test out our changes. We can safely test infrastructure related  deployments and changes (DNS, networking, ansible roles) safely within our own environment before pushing things straight to production! What we’ll dig in on during this talk is how to safely deploy infrastructure related components on Kubernetes and orchestrate the testing and deployment of these changes with Tekton and KubeVirt. We’ll also discuss some of the advantages and disadvantages that may come with an approach like this and how we’re looking to use this on a handful of internal projects.</p>",
    "description": "",
    "persons": [
      "Tyler Auerbeck"
    ]
  },
  {
    "start": 1577881200000,
    "duration": 40,
    "room": "UB4.136",
    "title": "Choosing The Right Deployment Strategy",
    "subtitle": "",
    "track": "Continuous Integration and Continuous Deployment",
    "abstract": "<p>Deployment strategies affect everyone, no matter whether we are focused only on a single aspect of the application lifecycle or we are in full control. The way we deploy affects the architecture, testing, monitoring, and many other aspects. And not only that, but we can say that architecture, testing, and monitoring affect the way we deploy. All those things are closely related and affect each other.</p>\n\n<p>We'll discuss different deployment strategies and answer a couple of questions. Is your application stateful or stateless? Does its architecture permit scaling? How do you roll back? How do you scale up and down? Do you need your application to run always? Should you use Kubernetes Deployments instead of, let's say, StatefulSets? Answers to those questions will not serve much unless we are familiar with some of the most commonly used deployment strategies. Not only that knowledge will help us choose which one to pick, but they might even influence the architecture of our applications.</p>",
    "description": "<p>For many people, deploying applications is transparent or even irrelevant. If you are a developer, you might be focused on writing code and allowing magic to happen. By magic, I mean letting other people and departments figure out how to deploy your code. Similarly, you might be oblivious to deployments. You might be a tester, or you might have some other role not directly related to system administration, operations, or infrastructure. Now, I doubt that you are one of the oblivious. The chances are that you would not be even reading this if that's the case. If, against all bets, you do belong to the deployment-is-not-my-thing group, the only thing I can say is that you are wrong.</p>\n\n<p>Deployment strategies affect everyone, no matter whether we are focused only on a single aspect of the application lifecycle or we are in full control. The way we deploy affects the architecture, testing, monitoring, and many other aspects. And not only that, but we can say that architecture, testing, and monitoring affect the way we deploy. All those things are closely related and affect each other in ways that might not be obvious on the first look.</p>\n\n<p>We'll discuss different deployment strategies and answer a couple of questions. Is your application stateful or stateless? Does its architecture permit scaling? How do you roll back? How do you scale up and down? Do you need your application to run always? Should you use Kubernetes Deployments instead of, let's say, StatefulSets? Those are only a few of the questions you need to answer to choose the right deployment mechanism. But, answers to those questions will not serve much unless we are familiar with some of the most commonly used deployment strategies. Not only that knowledge will help us choose which one to pick, but they might even influence the architecture of our applications.</p>\n\n<p>We'll explore <strong>serverless, recreate, rolling update, and canary deployment strategies</strong> and we'll automate them all using <strong>Jenkins X</strong>.</p>",
    "persons": [
      "Viktor Farcic"
    ]
  },
  {
    "start": 1577883900000,
    "duration": 40,
    "room": "UB4.136",
    "title": "Progressive Delivery",
    "subtitle": "Continuous Delivery the Right Way",
    "track": "Continuous Integration and Continuous Deployment",
    "abstract": "<p>Progressive Delivery makes it easier to adopt Continuous Delivery, by deploying new versions to a subset of users and evaluating their correctness and performance before rolling them to the totality of the users, and rolled back if not matching some key metrics. Canary deployments is one of the techniques in Progressive Delivery, used in companies like Facebook to roll out new versions gradually. But good news! you don't need to be Facebook to take advantage of it.</p>\n\n<p>We will demo how to create a fully automated Progressive Delivery pipeline with Canary deployments and rollbacks in Kubernetes using Jenkins X, an open source platform for cloud native CI/CD in Kubernetes, and Flagger, a project that uses Istio and Prometheus to automate Canary rollouts and rollbacks.</p>",
    "description": "",
    "persons": [
      "Carlos Sanchez"
    ]
  },
  {
    "start": 1577886600000,
    "duration": 15,
    "room": "UB4.136",
    "title": "A Practical CI/CD Framework for Machine Learning at Massive Scale",
    "subtitle": "",
    "track": "Continuous Integration and Continuous Deployment",
    "abstract": "<p>Managing production machine learning systems at scale has uncovered new challenges that have required fundamentally different approaches to that of traditional software engineering and data science. In this talk, we'll provide key insights on MLOps, which often encompasses the concepts around monitoring, deployment, orchestration and continuous delivery for machine learning. We will be covering a hands on an example where we will be training, deploying and monitoring ML at scale.  We'll be using Jenkins X (+ Prow &amp; Tekton) to deploy/promote these models across multiple environments. We will use KIND (Kubernetes in Docker) to run integration tests in our development environment. Finally, we'll be using Seldon to orchestrate &amp; monitor these models leveraging advanced ML techniques.</p>",
    "description": "<p>Managing production machine learning systems at scale has uncovered new challenges which have required fundamentally different approaches to that of traditional software engineering and data science. In this talk, we'll provide key insights on MLOps, which often encompasses the concepts around monitoring, deployment, orchestration and continuous delivery for machine learning. We will be covering a hands on an example where we will be training, deploying and monitoring ML at scale.  We'll be using Jenkins X (+ Prow &amp; Tekton) to deploy/promote these models across multiple environments. We will use KIND (Kubernetes in Docker) to run integration tests in our development environment. Finally, we'll be using Seldon to orchestrate &amp; monitor these models leveraging advanced ML techniques.</p>",
    "persons": [
      "Alejandro Saucedo"
    ]
  },
  {
    "start": 1577887800000,
    "duration": 40,
    "room": "UB4.136",
    "title": "Our road to a k8s/GKE based Closed Build Environment",
    "subtitle": "A small journey to an autoscaling build env based on Jenkins.",
    "track": "Continuous Integration and Continuous Deployment",
    "abstract": "<p>My team build a new Closed Build Environment for building Release Packages on Google Cloud Platform(gcp) with Google Kubernetes Engine (GKE).</p>\n\n<p>I like to take you on a small journey through a variety of topics we came across (open for change):</p>\n\n<ul>\n<li><p>How we bootstrap and how we use ArgoCD</p></li>\n<li><p>Autoscaling to 100 Build nodes for building</p></li>\n<li><p>Why we are using Prometheus-Operator</p></li>\n<li><p>SRE or how we maintain our stack</p></li>\n<li><p>Product aspect</p></li>\n<li><p>Base Image building &amp; scanning</p></li>\n<li><p>Network setup with Shared VPC</p></li>\n<li><p>Google Cloud Platform IAM Permissions vs. RBAC</p></li>\n<li><p>Specific GKE Features like Workload Identity</p></li>\n</ul>\n\n\n<p>And others</p>\n\n<p>Simple real live example how my team is doing it. Looking forward to inspire and to get feedback from others!</p>",
    "description": "",
    "persons": [
      "Siegfried Kiermayer"
    ]
  },
  {
    "start": 1577890500000,
    "duration": 40,
    "room": "UB4.136",
    "title": "From a Pipeline to a Government Cloud",
    "subtitle": "How the UK government deploy a Platform-as-a-Service using Concourse, an open-source continuous thing-doer",
    "track": "Continuous Integration and Continuous Deployment",
    "abstract": "<p>Since 2016, the UK Government has been running an open-source, cross-government Platform-as-a-Service (PaaS) to make it easier and cheaper to build government services. The GOV.UK PaaS is built on BOSH and Cloud Foundry, and is deployed using Concourse.</p>\n\n<p>Concourse is \"an open-source continuous thing-doer\", with abstractions that help build pipelines quickly, and for extending the behaviour of the system.</p>\n\n<p>This presentation will provide an introduction to Concourse, and then describe how the GOV.UK PaaS team use Concourse to continuously deploy a whole PaaS whilst ensuring high-availability and minimal impact to services and users.</p>",
    "description": "<p>Toby Lorne is a site reliability engineer working at the UK Government Digital Service on the GOV.UK Platform-as-a-Service.</p>\n\n<p>This presentation is split into four parts:</p>\n\n<ol>\n<li>An introduction to Concourse:</li>\n<li>How it works</li>\n<li>The abstractions</li>\n<li><p>The design decisions, patterns, and anti-patterns</p></li>\n<li><p>An overview of the architecture and implementation of GOV.UK PaaS:</p></li>\n<li>Terraform - a tool for managing infrastructure as code</li>\n<li>BOSH - a tool for release engineering and software lifecycle management</li>\n<li>Cloud Foundry - a set of components for Platform-as-a-Service</li>\n<li><p>Prometheus &amp; Grafana - monitoring and visualisation tools</p></li>\n<li><p>A walkthrough of the pipelines used in deployment and development</p></li>\n<li><p>An examination of patterns used in the GOV.UK PaaS deployment pipeline, and how you could use these patterns in your pipelines.</p></li>\n</ol>",
    "persons": [
      "Toby Lorne (tlwr)"
    ]
  },
  {
    "start": 1577893200000,
    "duration": 40,
    "room": "UB4.136",
    "title": "Deployment to hardware",
    "subtitle": "A multi pipeline challenge",
    "track": "Continuous Integration and Continuous Deployment",
    "abstract": "<p>Our project takes a fun, road-following app which leverages a basic neural network and deploys it to real hardware with an OStree update system. This has meant managing a variety of different CI-runners; GPU, aarch64 and x86_64. These have variety of different dependencies, drivers and have interfaces with a number of services and caches.</p>\n\n<p>I will focus on how we constructed and developed our CI pipelines to build, test and integrate a number of disparate components to produce images and push updates into an OStree server to be deployed over the air onto our hardware.</p>",
    "description": "",
    "persons": [
      "William Salmon"
    ]
  },
  {
    "start": 1577895900000,
    "duration": 35,
    "room": "UB4.136",
    "title": "Safe, gated and integrated GitOps for Kubernetes",
    "subtitle": "",
    "track": "Continuous Integration and Continuous Deployment",
    "abstract": "",
    "description": "<p>There is no single tool that can do everything in your CI/CD pipeline. You have the freedom to pick these tools and integrate them to complete the lifecycle of a release. In this talk, we’re discussing the integration of three different DevOps tool not only completes a CI/CD pipeline but also leads to effective automation by adding extra layers of intelligence.</p>\n\n<p>We begin with using Git as a version control system that houses the entire state for a Kubernetes deployment. Adding Zuul to the mix, as your open-source tool that assumes the role of project gating, ensuring no broken code is merged into the main branch of your clusters, and Argo CD as the continuous delivery platform in place that pulls that code and applies to all application code in question.</p>\n\n<p>It is the integration of all three tools that are essential for an efficient trail of your entire deployments. The talk will further elaborate on the role of each of these mechanisms when managing a Kubernetes cluster and their integration with one another.</p>",
    "persons": [
      "Mohammed Naser"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 420,
    "room": "UB4.228",
    "title": "Open Source Hardware and Soldering Workshop",
    "subtitle": "",
    "track": "Workshops",
    "abstract": "<p>Open Source Hardware room with two day soldering workshops.</p>\n\n<p>Day 2 soldering workshop will be dedicated to Surface Mount Technology and is good for beginners which have no experience with SMT technology.</p>\n\n<p>Beside the soldering workshop we will show our latest OSHW boards we work on, you are welcome to join and show your own OSHW projects too.</p>",
    "description": "<p>We will solder Binary Watch PCB made exclusively with SMT components.</p>\n\n<p>During the soldering workshop we will introduce the electronic components used in the PCB and how to identify them and how components with polarity is to be recognized.</p>\n\n<p>We will teach you the basics of SMT soldering, how to print solder paste with stencil, how to reflow using hot air, how good and bad solder joints look like and what is cold solder joint.\nAt the end of the workshop you will build your own binary watch.</p>",
    "persons": [
      "Tsvetan Usunov"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 25,
    "room": "UB5.132",
    "title": "Introducing HPC with a Raspberry Pi cluster",
    "subtitle": "A practical use of and good excuse to build Raspberry Pi Clusters",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>This talk will discuss the development of a RaspberryPi cluster for teaching an introduction to HPC.</p>\n\n<p>The motivation for this was to overcome four key problems faced by new HPC users:</p>\n\n<ol>\n<li>The availability of a real HPC system and the effect running training courses can have on the real system, conversely the availability of spare resources on the real system can cause problems for the training course.</li>\n<li>A fear of using a large and expensive HPC system for the first time and worries that doing something wrong might damage the system.</li>\n<li>That HPC systems are very abstract systems sitting in data centres that users never see, it is difficult for them to understand exactly what it is they are using.</li>\n<li>That new users fail to understand resource limitations, in part because of the vast resources in modern HPC systems a lot of mistakes can be made before running out of resources. A more resource constrained system makes it easier to understand this.</li>\n</ol>\n\n\n<p>The talk will also discuss some of the technical challenges in deploying an HPC environment to a Raspberry Pi and attempts to keep that environment as close to a \"real\" HPC as possible. The issue to trying to automate the installation process will also be covered.</p>",
    "description": "",
    "persons": [
      "Colin Sauze"
    ]
  },
  {
    "start": 1577871000000,
    "duration": 25,
    "room": "UB5.132",
    "title": "Building an open source data lake at scale in the cloud",
    "subtitle": "",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>This presentation will give an overview of the various tools, software, patterns and approaches that Expedia Group uses to operate a number of large scale data lakes in the cloud and on premise. The data journey undertaken by the\nExpedia Group is probably similar to many others who have been operating in this space over the past two decades - scaling out from relational databases to on premise Hadoop clusters to a much wider ecosystem in the cloud. This talk\nwill give an overview of that journey and then describe the various open source components that Expedia Group have used and built to create multi-petabyte data lakes. These include existing open source projects like Hive, Hadoop, Terraform,\nDocker, Kubernetes as well as open source tools that we built to overcome some of the unexpected challenges we faced. The first of these is Circus Train — a dataset replication tool that copies Hive tables between clusters and clouds. We will also discuss various other options for dataset replication and what unique features Circus Train has. The second tool is Waggle Dance — a federated Hive metadata service that enables querying of data stored across multiple Hive metastores. We will then look at Apiary - a means to simplify the deployment of the various components of an open source data lake at scale including the Hive metastore, Waggle Dance, S3 bucket access, metadata change notifications and much more. We focus on actual problems and solutions that have arisen in a huge, organically grown corporation, rather than idealised architectures.</p>",
    "description": "",
    "persons": [
      "Adrian Woodhead"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 25,
    "room": "UB5.132",
    "title": "Magic Castle: Terraforming the Cloud for HPC",
    "subtitle": "",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>Compute Canada provides HPC infrastructures and support to every academic research institution in Canada. In recent years, Compute Canada has started distributing research software to its HPC clusters using with CERN software distribution service, CVMFS. This opened the possibility for accessing the software from almost any location and therefore allow the replication of the Compute Canada experience outside of its physical infrastructure.</p>\n\n<p>From these new possibilities emerged an open-source software project named Magic Castle, which aims to recreate the Compute Canada user experience in public clouds. Magic Castle uses the open-source software Terraform and HashiCorp Language (HCL) to define the virtual machines, volumes, and networks that are required to replicate a virtual HPC infrastructure. The infrastructure definition is packaged as a Terraform module that users can customize as they require. Once their cluster is deployed, the user is provided with a complete HPC cluster software environment including a Slurm scheduler, a Globus Endpoint, JupyterHub, LDAP, DNS, and over 3000 research software compiled by experts with EasyBuild. Magic Castle is compatible with AWS, Microsoft Azure, Google Cloud, OpenStack, and OVH.</p>\n\n<p>Compute Canada staff has been using this software to deploy ephemeral clusters for training purposes every other week for the past two years. Magic Castle is also gaining in popularity with HPC cluster users for development, testing, and continuous integration.</p>\n\n<p>In this talk, we will give a live demonstration of the creation of a cluster. We will present the architecture of Magic Castle, explain infrastructure and provisioning design, and present use cases. We will conclude by describing some of the challenges experienced while developing this novel usage of Terraform.</p>",
    "description": "",
    "persons": [
      "Félix-Antoine Fortin"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 25,
    "room": "UB5.132",
    "title": "Maggy: Asynchronous distributed hyperparameter optimization based on Apache Spark",
    "subtitle": "Asynchronous algorithms on a bulk-synchronous system",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>Maggy is an open-source framework built on Apache Spark, for asynchronous parallel execution of trials for machine learning experiments. In this talk, we will present our work to tackle search as a general purpose method efficiently with Maggy, focusing on hyperparameter optimization. We show that an asynchronous system enables state-of-the-art optimization algorithms and allows extensive early stopping in order to increase the number of trials that can be performed in a given period of time on a fixed amount of resources.</p>",
    "description": "<p>In \"The Bitter Lesson of AI\", Rich Sutton (father of reinforcement learning) claimed that general purpose methods (like search and learning) that scale with increased computation are the future of AI. Apache Spark is a general purpose framework for scaling out data processing with available compute, but there are challenges in making Sparks' bulk-synchronous execution mechanism work efficiently with search and (deep) learning.\nIn this talk, we will present our work on Maggy, an open-source framework to tackle search as a general purpose method efficiently on Spark. Spark can be used to deploy basic optimizers (grid search, random search, differential evolution) proposing combinations of hyperparameters (trials) that are run synchronously in parallel on executors. However, many such trials perform poorly, and a lot of CPU and hardware accelerator cycles are wasted on trials that could be stopped early, freeing up resources for other trials. What is needed is support for asynchronous mechanisms.\nMaggy is an asynchronous hyperparameter optimization framework built on Spark that is able to transparently schedule and manage hyperparameter trials, by allowing limited communication, thereby increasing resource utilization, and massively increasing the number of trials that can be performed in a given period of time on a fixed amount of resources. Maggy is also built to support parallel ablation studies and applies to black box optimization/search problems in general. We will report on the gains we have seen in reduced time to find good hyperparameters and improved utilization of GPU hardware. Finally, we will perform a live demo on a Jupyter notebook, showing how to integrate Maggy in existing PySpark applications.</p>",
    "persons": [
      "Moritz Meister"
    ]
  },
  {
    "start": 1577876400000,
    "duration": 25,
    "room": "UB5.132",
    "title": "Snorkel Beambell - Real-time Weak Supervision on Apache Flink",
    "subtitle": "",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>The advent of Deep Learning models has led to a massive growth of real-world machine learning. Deep Learning allows Machine Learning Practitioners to get the state-of-the-art score on benchmarks without any hand-engineered features. These Deep Learning models rely on massive hand-labeled training datasets which is a bottleneck in developing and modifying machine learning models.</p>\n\n<p>Most large scale Machine Learning systems today like Google’s DryBell use some form of Weak Supervision to construct lower quality, large scale training datasets that can be used to continuously retrain and deploy models in a real-world scenario.</p>\n\n<p>The challenge with continuous retraining is that one needs to maintain prior state (e.g., the learning functions in case of Weak Supervision or a pre-trained model like BERT or Word2Vec for Transfer Learning) that is shared across multiple streams, while continuously updating the model. Apache Beam’s Stateful Stream processing capabilities are a perfect match here including support for scalable Weak Supervision.</p>\n\n<p>Prior work on using Beam’s State coupled with Flink’s dynamic processing capabilities to store and update word embeddings for real-time Online Topic Modeling of text has been presented at Flink Forward Berlin 2018.  Similar streaming pipelines would also work for real-time model updates using Weak Supervision and Transfer Learning. In this talk, we’ll be looking at a framework - Snorkel BeamBell - a framework leveraging Stanford’s Snorkel library for Weak Supervision and Apache Beam for large scale Weak Supervision Learning for online labeling of large amounts of data that can continuously learn new classification models based on Stateful Learning Functions and user feedback.</p>",
    "description": "<p>The advent of Deep Learning models has led to a massive growth of real-world machine learning. Deep Learning allows Machine Learning Practitioners to get the state-of-the-art score on benchmarks without any hand-engineered features. These Deep Learning models rely on massive hand-labeled training datasets which is a bottleneck in developing and modifying machine learning models.</p>\n\n<p>Most large scale Machine Learning systems today like Google’s DryBell use some form of Weak Supervision to construct lower quality, large scale training datasets that can be used to continuously retrain and deploy models in a real-world scenario.</p>\n\n<p>The challenge with continuous retraining is that one needs to maintain prior state (e.g., the learning functions in case of Weak Supervision or a pre-trained model like BERT or Word2Vec for Transfer Learning) that is shared across multiple streams, while continuously updating the model. Apache Beam’s Stateful Stream processing capabilities are a perfect match here including support for scalable Weak Supervision.</p>\n\n<p>Prior work on using Beam’s State coupled with Flink’s dynamic processing capabilities to store and update word embeddings for real-time Online Topic Modeling of text has been presented at Flink Forward Berlin 2018.  Similar streaming pipelines would also work for real-time model updates using Weak Supervision and Transfer Learning. In this talk, we’ll be looking at a framework - Snorkel BeamBell - a framework leveraging Stanford’s Snorkel library for Weak Supervision and Apache Beam for large scale Weak Supervision Learning for online labeling of large amounts of data that can continuously learn new classification models based on Stateful Learning Functions and user feedback.</p>\n\n<p>The audience would come away with a better understanding of how Weak Supervision with Apache Beam’s stateful stream processing can be used to accelerate the labeling of training data, and real-time training and update of machine learning models.</p>",
    "persons": [
      "Suneel Marthi"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 25,
    "room": "UB5.132",
    "title": "Efficient Model Selection for Deep Neural Networks on Massively Parallel Processing Databases",
    "subtitle": "",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>In this session we will present an efficient way to train many deep learning model configurations at the same time with Greenplum, a free and open source massively parallel database based on PostgreSQL.  The implementation involves distributing data to the workers that have GPUs available and hopping model state between those workers, without sacrificing reproducibility or accuracy.   Then we apply optimization algorithms to generate and prune the set of model configurations to try.</p>",
    "description": "<p>Deep neural networks are revolutionizing many machine learning applications, but hundreds of trials may be needed to generate a good model architecture and associated hyperparameters.  This is the challenge of model selection.  It is time consuming and expensive, especially if you are only training one model at a time.</p>\n\n<p>Massively parallel processing databases can have hundreds of workers, so can you use this parallel compute architecture to address the challenge of model selection for deep nets, in order to make it faster and cheaper?</p>\n\n<p>It’s possible!</p>\n\n<p>We will demonstrate results from this project using a version of Hyperband, which is a well known hyperparameter optimization algorithm, and the deep learning frameworks Keras and TensorFlow, all running on Greenplum database using Apache MADlib.  Other topics will include architecture, scalability results and bright opportunities for the future.</p>\n\n<p>We look forward to presenting this topic at FOSDEM’20!</p>",
    "persons": [
      "Frank McQuillan"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 25,
    "room": "UB5.132",
    "title": "Predictive Maintenance",
    "subtitle": "from milliseconds to months",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>Predictive maintenance and condition monitoring for remote heavy machinery are compelling endeavors to reduce maintenance cost and increase availability. Beneficial factors for such endeavors include the degree of interconnectedness, availability of low cost sensors, and advances in predictive analytics. This work presents a condition monitoring platform built entirely from open-source software. A real world industry example for an escalator use case from Deutsche Bahn underlines the advantages of this approach.</p>",
    "description": "<p>Predictive maintenance and condition monitoring for remote heavy machinery are compelling endeavors to reduce maintenance cost and increase availability. Beneficial factors for such endeavors include the degree of interconnectedness, availability of low cost sensors, and advances in predictive analytics. This work presents a condition monitoring platform built entirely from open-source software. A real world industry example for an escalator use case from Deutsche Bahn underlines the advantages of this approach.</p>\n\n<p>Audio analysis is performed on miliseconds of audio data to get accurate predictions of an asset's condition. Even with this high resolution knowledge about our equipment under supervision, sensitive alarming of our customers requires a system of systems approach taking into account up to several months of data.</p>\n\n<p>This talk highlights the challenges and learnings involved in building the platform and high-level aggregation for our alarming system.</p>",
    "persons": [
      "Corvin Jaedicke"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 10,
    "room": "UB5.132",
    "title": "Towards reproducible Jupyter notebooks",
    "subtitle": "",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>Jupyter has become a tool of choice for researchers willing to share a narrative and supporting code that their peers can re-run.  This talk is about Jupyter’s Achille’s heel: software deployment.  I will present Guix-Jupyter, which aims to make notebook self-contained and to support reproducible deployment.</p>",
    "description": "<p>Jupyter has become a tool of choice for researchers in data science and others fields.  Jupyter Notebooks allow them to share a narrative and supporting code that their peers can re-run, which is why it is often considered a good tool for reproducible science.</p>\n\n<p>However, Jupyter Notebooks do not describe their software dependencies, which significantly hinder reproducibility: What if your peer runs different Python version?  What if your notebook depends on a library that your peer hasn’t installed?  What will happen if you try to run your notebook in a few years?</p>\n\n<p>All these issues are being addressed by tools such as Binder and its friend repo2docker.  These solutions, though, do not address what we think is the core issue: that notebooks lack information about their software dependency.</p>\n\n<p>In this talk I will present our take on this problem, Guix-Jupyter. Guix-Jupyter allows users to annotate their notebook with information about their run-time environment.  Those annotations are interpreted and Guix takes care of deploying the dependencies described.  Furthermore, Guix-Jupyter ensures that code runs in an isolated environment (a container) as a way to maximize reproducibility.</p>\n\n<p>Guix-Jupyter is work-in-progress and we are eager to share our approach and get your feedback!</p>",
    "persons": [
      "Ludovic Courtès"
    ]
  },
  {
    "start": 1577882700000,
    "duration": 10,
    "room": "UB5.132",
    "title": "Buildtest: HPC Software Stack Testing Framework",
    "subtitle": "",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>HPC support teams are often tasked with installing\nscientific software for their user community and the complexity of\nmanaging a large software stack gets very challenging. Software\ninstallation brings forth many challenges that requires a team of\ndomain expertise and countless hours troubleshooting to build an\noptimal software state that is tuned to the architecture. In the past\ndecade, two software build tools (Easybuild, Spack) have emerged\nthat are widely accepted in HPC community to accelerate building\na complete software stack for HPC systems. The support team are\nconstantly involved in fulfilling software request for end-users\nwhich leads to an ever-growing software ecosystem. Once a\nsoftware is installed, the support team hands it off to the user\nwithout any testing because scientific software requires domain\nexpertise in order to test software. Some software packages are\nshipped with a test suite that can be run at post build while many\nsoftware have no mechanism for testing. This poses a knowledge\ngap between HPC support team and end-users on the type of\ntesting to do. Some HPC centers may have developed in-house test\nscripts that are suitable for testing their software, but these tests\nare not portable due to hardcoded paths and are often site\ndependent. In addition, there is no collaboration between HPC\nsites in building a test repository that will benefit the community.\nIn this talk I will presents buildtest, a framework to automate software\ntesting for a software stack along with several module operations\nthat would be of interest to the HPC support team.</p>",
    "description": "<p>HPC computing environment is a tightly coupled system that\nincludes a cluster of nodes and accelerators interconnected with\na high-speed interconnect, a parallel filesystem,multiple storage\ntiers, a batch scheduler for users to submit jobs to the cluster and\na software stack for users to run their workflows. A software\nstack is a collection of compilers, MPI, libraries, system utilities\nand scientific packages typically installed in a parallel filesystem.\nA module tool like environment-modules or Lmod is generally used for loading the software environment into\nthe users’ shell environment.</p>\n\n<p>Software are packaged in various forms that determine how\nthey are installed. A few package formats are: binary, Makefile,\nCMake, Autoconf, github, PyPi, Conda, RPM,tarball, rubygem,\nMakeCp, jar, and many more. With many packaging formats,\nthis creates a burden for HPC support team to learn how to build\nsoftware since each one has a unique build process. Software\nbuild tools like Easybuild and Spack can build up to\n1000+ software packages by supporting many packaging\nformats to address all sorts of software builds. Easybuild and\nSpack provide end-end software build automation that helps\nHPC site to build a very large software stack with many\ncombinatorial software configurations. During the installation,\nsome packages will provide a test harness that can be executed\nvia Easybuild or Spack which typically invokes a make test or\nctest for packages that follow ConfigureMake, Autoconf, or\nCMake install process.</p>\n\n<p>Many HPC sites rely on their users for testing the software\nstack, and some sites may develop in-house test scripts to run\nsanity check for popular scientific tools. Despite these efforts,\nthere is little or no collaboration between HPC sites on sharing\ntests because they are site-specific and often provide no\ndocumentation. For many sites, the HPC support team don’t\nhave the time for conducting software stack testing because: (1)\nlack of domain expertise and understaffed, (2) no standard testsuite and framework to automate test build and execution.\nFrankly, HPC support teams are so busy with important day-day\noperation and engineering projects that software testing is either\nneglected or left to end-users. This demands for a concerted\neffort by HPC community to build a strong open-source\ncommunity around software stack testing.</p>\n\n<p>There are two points that need to be addressed. First, we need\na framework to do automatic testing of installed software stack.\nSecond, is to build a test repository for scientific software that is\ncommunity driven and reusable amongst the HPC community.\nAn automated test framework is a harness for automating the\ntest creation process, but it requires a community contribution to\naccumulate this repository on per-package basis. Before we\ndive in, this talk will focus on conducting sanity check of the\nsoftware stack so tests will need to be generic with simple\nexamples that can be compiled easily. In future, buildtest will\nfocus on domain-specific tests once there is a strong community\nbehind this project.</p>",
    "persons": [
      "Shahzeb Siddiqui"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 10,
    "room": "UB5.132",
    "title": "Facilitating HPC job debugging through job scripts archival",
    "subtitle": "",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>SArchive is a lightweight tool to facilitate debugging HPC job issues by providing support teams with the exact version of the job script that is run in the HPC job in an archive either on the filesystem, in Elasticsearch, or by producing it to a Kafka topic.</p>",
    "description": "<p>HPC schedulers usually keep a version of the user’s job script in their spool directory for the lifetime of the job, i.e., from job submission until the job has run to completion — either succesfully or failed. However, once the job has completed, the job script and associated files are removed to avoid stacking up a large number of files. HPC systems typically run several millions of jobs, if not many more, over their lifetime -- it is not feasible to keep them all in the spool directory. In case the job failed, user support teams are often asked to help figure out the cause of the failure. For these occasions, it often is helpful if the exact job script is available. Since a typical scheduler setup will make changes to every submitted script through, e.g., a submission filter, simply obtaining what the user submitted requires an extra hoop to run the given script through the filter(s). Furthermore, users may have tweaked, changed, or removed the job script, which may add to the difficulty of debugging the issue at hand.</p>\n\n<p>SArchive aims to address this problem by providing user support teams with an exact copy of the script that was run, along with the exact additional files that are used by the scheduler, e.g., to set up the environment in which the jobs runs. It can be argued that making a backup copy is actually the job of the scheduler itself, but we decided to use a tool outside the scheduler. This has the advantages that (i) one need not have access to the scheduler’s source code (not all schedulers are open source) and (ii) sites running multiple schedulers need not make any changes to each of them, but only to SArchive — which should be a fairly limited effort, if any at all. SArchive is currenly tailored towards the Slurm scheduler (hence the name), but it also supports the Torque resource manager. Adding support for other schedulers should be fairly straightforward — pull requests are welcome :)</p>\n\n<p>Currently, SArchive provides three archival options: storing archived files inside a file hierarchy, ship them to Elasticsearch, or produce them to a Kafka topic. File archival is pretty feature complete, the code for shipping to Elasticsearch and Kafka is still under development and only has what is needed in our (HPCUGent) specific setup — which may evolve.</p>",
    "persons": [
      "Andy Georges"
    ]
  },
  {
    "start": 1577884500000,
    "duration": 10,
    "room": "UB5.132",
    "title": "Sharing Reproducible Results in a Container",
    "subtitle": "A container you can build anywhere",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>Containers do a great job separating out different parts of a system, making sure that they don't interact unless we want them to. What happens when a colleague hands us a project they've written and we're supposed to host it for them? They're not programmers, they're scientists. Who knows what they have in their program? How can we keep it up to date and deployed with a minimum of fuss?\nCome and see how we've solved this problem with Guix, from rebuilding or replacing the dependencies with modern versions like a pro, having only the bare minimum required software in the container, deploying in an artisanally crafted container like a hero, and upgrading and rolling back when ready.</p>",
    "description": "",
    "persons": [
      "Efraim Flashner"
    ]
  },
  {
    "start": 1577885400000,
    "duration": 25,
    "room": "UB5.132",
    "title": "Putting Artificial Intelligence back into people's hands",
    "subtitle": "Toward an accessible, transparent and fair AI",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>Artificial intelligence is now widespread for critical tasks such as crime recidivism risk assessment, credit risk scoring, job application review or disease detection. Because it has more and more impact on our lives, it becomes essential to make auditable AI software so that everyone can benefit from it and participate in its development.</p>\n\n<p>This talk will present the methods that can be used to build fairness into artificial intelligence and explain how to control its progress thanks to the four freedoms of Free Software.</p>",
    "description": "<p>The talk is divided in three parts:</p>\n\n<ul>\n<li>How to make accessible Artificial Intelligences?</li>\n<li>Can AI be transparent and accurate?</li>\n<li>How to build fairness into AI?</li>\n</ul>\n\n\n<p>For each part the context will be presented as well as possible solutions.</p>",
    "persons": [
      "Vincent Lequertier"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 25,
    "room": "UB5.132",
    "title": "GraphBLAS: A linear algebraic approach for high-performance graph algorithms",
    "subtitle": "",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>There is increasing interest to apply graph analytical techniques to a wide array of problems, many operating on large-scale graphs with billions of edges. While graph algorithms and their complexity is textbook material, efficient implementation of such algorithms is still a major challenge due to a number of reasons. First, the irregular and unstructured nature of graphs leads to a massive amount of random data access, which makes it difficult to use typical caching and parallelization techniques. Second, to optimize their code, developers need to be aware of the nuances of the underlying hardware which, at the very least consists of multiple CPU cores but often also incorporates heterogeneous components such as GPUs or even FPGAs. During the last decade, a number of graph programming models (such as Google's Pregel) have been proposed but most of these focused defining high-level abstractions for distributed execution environments and introduced a significant runtime overhead.</p>\n\n<p>A potential approach for defining efficient graph processing algorithms is to exploit the well-known duality of graphs and sparse adjacency matrices, using matrix operations to capture algorithms. Surprisingly, only a few recent research prototypes have used this model with little consensus on the set of necessary building blocks. The GraphBLAS initiative (launched in 2013) aims to define a standard to capture graph algorithms in the language of linear algebra - following the footsteps of the BLAS standard which, starting four decades ago, revolutionized scientific computing by defining constructs on dense matrices.</p>\n\n<p>In this talk, I give an overview of the GraphBLAS standard and its key components. First, I illustrate how matrix operations on various semirings correspond to the steps in graph algorithms. I then use these operations to present fundamental graph algorithms such as breadth-first search, shortest paths, and the clustering coefficient. Finally, I demonstrate the scalability of the GraphBLAS-based algorithms with the LDBC Graphalytics benchmark. The presented implementations are available open-source as part of LAGraph, a library built on top of GraphBLAS to demonstrate how to design efficient algorithms in linear algebra.</p>",
    "description": "<p><strong>Intended audience:</strong> Developers interested in implementing high-performance graph algorithms.</p>\n\n<p><strong>Expected prior knowledge:</strong> Familiarity with linear algebra helps plus but we only use basic concepts such as matrix-matrix multiplication</p>",
    "persons": [
      "Gabor Szarnyas"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 25,
    "room": "UB5.132",
    "title": "Selecting a Finite Element Analysis Backend for Exascale Fusion Reactor Simulations",
    "subtitle": "",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>Accelerating the development of fusion energy requires large scale simulations on cutting edge supercomputing resources.\nGreat hardware is only half the challenge and the software must be scalable to match.\nThis talk presents an objective approach to selecting a suitable back end to fusion simulations.</p>",
    "description": "<p>The UKAEA's mission is to develop commercially viable fusion energy.\nCurrent fusion technology is yet to break even\non power out compared to power in,\nthus designs for future reactors,\nwhich necessarily must exceed break even,\ncarry a great amount of uncertainty.\nWith cost estimates of a first of a kind fusion reactor\nin the order of billions of euros,\nany design flaw making it through to the construction stage\nwill be an expensive mistake.</p>\n\n<p>Thankfully, software can help.\nBy simulating a fusion reactor prior to construction,\nthe design can be tested and refined for a considerably lower cost.\nHowever, covering all the necessary scales and physics\nfor a digital twin of a fusion reactor\nrequires computational resources at the exascale.</p>\n\n<p>In this work, a number of potential finite element backends\nfor a multiphysics reactor simulation are evaluated.\nThe sheer scale makes open source a practical necessity\nand scalability is the primary performance metric.\nFrom the plethora of open source finite element libraries,\nthe most promising are selected\nand compared against a number of objective, unbiased criteria.</p>\n\n<p>None of the tested back ends scored perfectly in all criteria,\nso a method and rationale for weighting the results\nto select the best one for the purpose is presented.\nThe aspects of open source projects\nthat are important to high performance computing are highlighted.</p>",
    "persons": [
      "Aleksander J. Dubas"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 25,
    "room": "UB5.132",
    "title": "Build for your microarchitecture: experiences with Spack and archspec",
    "subtitle": "",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>In HPC, software is typically distributed as source code, so that users can build optimized software that takes advantage of specific microarchitectures and other hardware.  While this approach provides a lot of flexibility, building software from source remains a huge barrier for users accustomed to simple, fast binary package mangers.  Most package managers and container registries label binaries with a high-level architecture family name, e.g., x86_64 or ppc64le, but there is no standard way to label binaries for specific microarchitectures (haswell, skylake, power9, zen2, etc.).</p>\n\n<p>We’ll present a new project called “archspec” that aims to bridge this gap.  Archspec provides a standard set of human-understandable labels for many popular microarchitectures.  It models compatibility relationships between microarchitectures, and it aggregates information on ISA extensions, compiler support, and compiler flags needed to optimize these machines.  Finally, it provides a standard set of names for both microarchitectures and ISA features.  These features allow container tools and package managers to detect, build, and use optimized binaries.</p>\n\n<p>Archspec grew out of the Spack package manager, but it is intended for widespread use by other build, packaging, and containerization tools.  We will describe how it has been used in practice so far, how it has simplified writing generic packages, and our plans to get contributions from vendors and the broader community.</p>",
    "description": "<p>Expected prior knowledge / intended audience:\nAudience should have basic knowledge of build systems, as well as some knowledge about processor architectures.  There will be some brief background on this in the talk.  This will be interesting to HPC users, developers, packagers, and admins, as well as to anyone writing tools that deal with microarchitecture metadata (like container systems).</p>\n\n<p>Speaker bio:\nTodd Gamblin is a Senior Principal Member of Technical Staff in the Advanced Technology Office in Livermore Computing at Lawrence Livermore National Laboratory. His research focuses on scalable tools for measuring, analyzing, and visualizing parallel performance data. In addition to his research, Todd leads LLNL's DevRAMP (Reproducibility, Analysis, Monitoring, and Performance) team and the Software Packaging Technologies project in the U.S. Exascale Computing Project.  He created Spack, a popular open source HPC package management tool with a community of over 450 contributors. Todd has been at LLNL since 2008.</p>\n\n<p>Links to code / slides / material for the talk (optional):\nTo be provided closer to FOSDEM.</p>\n\n<p>Links to previous talks by the speaker:\nhttps://www.youtube.com/watch?v=DRuyPDdNr0M\nhttps://www.youtube.com/watch?v=edpgwyOD79E&amp;t=2891s\nhttps://www.youtube.com/watch?v=BxNOxHu6FAI\nhttps://insidehpc.com/2019/03/spack-a-package-manager-for-hpc/\nhttps://www.youtube.com/watch?v=iTLBkpHskzA</p>\n\n<p>See https://tgamblin.github.io/cv/todd-cv.pdf for more (including tutorials and other presentations at major conferences)</p>",
    "persons": [
      "Todd Gamblin"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 25,
    "room": "UB5.132",
    "title": "HPC on OpenStack",
    "subtitle": "the good, the bad and the ugly",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>HPC systems have been traditionally operated as monolithic installations on bare-metal hardware primarily used by users with computational background to submit classic batch jobs. However the commoditization of compute resources and the introduction of new scientific fields such as life sciences to high performance computing has caused a shift in this paradigm. Today, an increasing number of biological software is made accessible through web portals. This improved ease of use has led towards a democratization of access to computational resources\nUsers of those fields don’t have the same computational knowledge as traditional HPC users from physics or chemistry and additionally require different kinds of workloads and applications that don’t fit traditional non-interactive batch scheduling resource management systems. Additionally, cloud computing is becoming more and more relevant and various efforts to lift HPC into the Cloud were started.</p>\n\n<p>We manage the HPC infrastructure for 3 life science and 2 particle physics institutions at the Vienna Bio Center (VBC). For the new HPC system that was procured at the end of 2018, we decided to go with an on-prem cloud framework based on OpenStack to accommodate the various emerging workflows and programs. OpenStack is not a finished product and requires considerable amount of engineering. It took us around 2 years of testing and engineering to feel confident in deploying the new HPC infrastructure on top of OpenStack. Since summer 2019 we have our 200 node production SLURM cluster running on top of VMs in OpenStack.</p>\n\n<p>In this talk we want to share our experiences from our endeavor into HPC on OpenStack. We want to briefly discuss the reasoning behind HPC in the cloud and specifically OpenStack.\nOften times these kind of projects either completely fade away in case of failure or get published in a  high-level white paper that is only useful as marketing material.\nWe want to share our honest experience from both implementer and operator perspective. We discuss how we use 3 environments to test updates and configuration changes. We will also explain our approach to automation and infrastructure as code all the way from the underlying infrastructure to the SLURM payload and how we keep our sanity using development procedures around pull requests and code reviews. We will also share some stories from the trenches, such as why you still learn new things about OpenStack after 1000 deploys or discover that a simple config change can destroy performance.\nThis talk will contain information that you won’t find in success stories or white papers but is hopefully very helpful or anyone who considers deploying HPC on OpenStack.</p>",
    "description": "",
    "persons": [
      "Ümit Seren"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 25,
    "room": "UB5.132",
    "title": "Interactive applications on HPC systems",
    "subtitle": "Jupyterhub, Galaxy, RStudio, XPRA",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>Exploratory data analysis has increased the demand for interactive tools. In the same way, workshops and other teaching events often benefit from immediate and on-demand access to preconfigured, interactive environments.</p>\n\n<p>For low resource requirements these interactive environments can be run on workstations. However, as user count and resource demand increase, these setups become more complex. While these frameworks typically provide good support for cloud based deployments in container orchestrations, it is often preferable to deploy them on existing compute infrastructure that provides access to both software packages and the data to be analysed. The deployment on HPC batch systems specifically brings challenges on how to handle authentication, user identities, and job submissions.</p>\n\n<p>The architecture of these applications can be considered as following the master -- minion paradigm in most cases. One central component manages user access and acts as a gateway. It launches one or multiple per-user instances of a compute component, that provides the actual user environment.</p>\n\n<p>We want to demonstrate how we provide applications like Galaxy, Jupyterhub, and RStudio to scientists of the Vienna Biocenter. The presentation will focus on the similarities and pitfalls of these deployments. We run the web application gateway based on our standardized container environment. The compute components run as SLURM jobs on the CLIP batch environment (CBE). Specific focus will be placed on the integration of web-based Single-Sign-On, and how we address the management of user identities for starting jobs on the batch system. Sources and configuration examples on the specific setup will be provided.</p>\n\n<p>After the operator’s perspective, we will pan to the end-users view. Beginners and workshop situations typically prefer a static, pre-configured setup of the user session. Contrary to that, advanced users will want to customize their execution environment as much as possible. We will explore how scientists can tailor the setup to their individual needs.</p>\n\n<p>Finally, we will summarize the setups of the applications in a high-level comparison from both the operators and the end-users perspective.</p>",
    "description": "",
    "persons": [
      "Erich Birngruber"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 25,
    "room": "UB5.132",
    "title": "Building cloud-based data services to enable earth-science workflows across HPC centres",
    "subtitle": "",
    "track": "HPC, Big Data, and Data Science",
    "abstract": "<p>Weather forecasts produced by ECMWF and environment services by the Copernicus programme act as a vital input for many downstream simulations and applications. A variety of products, such as ECMWF reanalyses and archived forecasts, are additionally available to users via the MARS archive and the Copernicus data portal. Transferring, storing and locally modifying large volumes of such data prior to integration currently presents a significant challenge to users. The key aim for ECMWF within the H2020 HiDALGO project is to migrate some of these tasks to the cloud, thereby facilitating fast and seamless application integration by enabling precise and efficient data delivery to the end-user. The required cloud infrastructure development will also feed into ECMWF's contribution to the European Weather Cloud pilot which is a collaborative cloud development project between ECMWF and EUMETSAT.</p>\n\n<p>ECMWF and its HiDALGO partners aim to implement a set of services that enable the simulation of complex global challenges which require massive high performance computing resources alongside state-of-the-art data analytics and visualization.</p>\n\n<p>ECMWF's role in the project will be to enable seamless integration of two pilot applications with its meteorological data and services delivered via ECMWF's Cloud and orchestrated by bespoke HiDALGO workflows. The demonstrated workflows show the increased value of weather forecasts, but also derived forecasts for air quality as provided by the Copernicus Atmospheric Monitoring Service (CAMS).</p>\n\n<p>The HiDALGO use-case workflows are comprised of four main components: pre-processing, numerical simulation, post-processing and visualization. The core simulations are ideally suited to running in a dedicated HPC environment, due to their large computational demands, coupled with the heavy communication overhead between parallel processes. However, the pre-/post-processing and visualisation tasks generally do not demand more than a few cores to compute and do not require message passing between instances, hence they are good candidates to run in a cloud environment. Enabling, managing and orchestrating the integration of both HPC and cloud environments to improve overall performance is the key goal of HiDALGO.</p>\n\n<p>This talk will give a general overview of HiDALGO project and its main aims and objectives. It will present the two test pilot applications which will be used for integration, and an overview of the general workflows and services within HiDALGO. In particular, it will focus on how ECMWF's cloud data and services will couple with the test pilot applications to improve overall workflow performance and enable access to new data for the pilot users.</p>\n\n<p>This work is supported by the HiDALGO project and has been partly funded by the European Commission's ICT activity of the H2020 Programme under grant agreement number: 824115.</p>",
    "description": "",
    "persons": [
      "John Hanley"
    ]
  },
  {
    "start": 1577869500000,
    "duration": 25,
    "room": "UB5.230",
    "title": "Applying Open Culture Practices across Distributed Teams",
    "subtitle": "",
    "track": "Community devroom",
    "abstract": "<p>Distributed teams are where people you work with aren’t physically co-located, ie. they’re at another office building, home or an outsourced company abroad. They’re becoming increasingly popular, for DevOps and other teams, due to recruitment, diversity, flexibility and cost savings. Challenges arise due to timezones, language barriers, cultures and ways of working. People actively participating in Open Source communities tend to be effective in distributed teams. This session looks at how to apply core Open Source principles to distributed teams in Enterprise organisations, and the importance of shared purposes/goals, (mis)communication, leading vs managing teams, sharing and learning. We'll also look at practical aspects of what's worked well for others, such as alternatives to daily standups, promoting video conferencing, time management and virtual coffee breaks. This session is relevant for those leading or working in distributed teams, wanting to know how to cultivate an inclusive culture of increased trust and collaboration that leads to increased productivity and performance. All are welcome to attend.</p>",
    "description": "",
    "persons": [
      "Katrina Novakovic"
    ]
  },
  {
    "start": 1577871300000,
    "duration": 25,
    "room": "UB5.230",
    "title": "Organizing Open Source for Cities",
    "subtitle": "Adapting the Open Source Program Office",
    "track": "Community devroom",
    "abstract": "<p>Open Source is vital in the expansion wave of smart cities. Yet, where is the sustainable municipal open innovation economic engine/s, and how do we start them spinning at scale? — Only through structured collaboration and community.  We present the community collaboration efforts, accomplishments, and vision of the partners behind the launch of the Johns Hopkins Open Source Program Office for Open Cities, the community creation efforts of the City of Paris's open source city services platform Lutece, and the interactions with and between Baltimore communities, Paris communities, and open source communities and institutions.</p>",
    "description": "<p>Open Source is vital in the expansion wave of smart cities. Yet, where is the sustainable municipal Open innovation economic engine/s, and how do we start them spinning at scale? — Only through structured collaboration and community. The open source communities and institutions are highly successful at this in other industries. In cities, open source is not enough, we also need open data, open standards, etc. As we scale, openness and transparency, interoperability, feedback mechanisms, security, non-bias, privacy, become dominating design requirements. Accelerating the scale of good solutions needs help and structure. There are 18,000 municipalities in the US alone, currently siloed, and meaningful technical and community cooperation is minimal. We need a new flexible institutional framework to advance cooperation and scaling within our interdisciplinary design requirements.</p>\n\n<p>The open source program office is a successful industry construct in the open source world, and we aim to investigate adapting this construct to accelerate and scale open cities; from open source software &amp; services, open data, and standards, to non-bias, security, privacy, access, diversity, and above all TRUST!</p>\n\n<p>Johns Hopkins University has launched what is the believed to be the first OSPO for higher education and launching it in part to support Open Cities including Baltimore.  Jacob Green from Mosslabs.io discussed the launch of the JHU OSPO, its initial initiatives, and collaborations with City of Paris.</p>",
    "persons": [
      "Jacob Green"
    ]
  },
  {
    "start": 1577873100000,
    "duration": 25,
    "room": "UB5.230",
    "title": "The next generation of contributors is not on IRC",
    "subtitle": "Discussing communication channels for inclusive open source communities",
    "track": "Community devroom",
    "abstract": "<p>There is some combination of a turf war and a diaspora happening in the open source communities I participate in. There are synchronous and async channels galore. Every one of them has fans and haters with firmly held opinions on how it's the best or worst thing in the world.</p>\n\n<p>Let's take a step back and take a look at the landscape together. What are our communities searching for when they hop into communication channels? How do we meet new members where they are comfortable in order to be more welcoming?</p>\n\n<p>As a self-defined GitHub generation of open source enthusiast, I'd like to start a conversation from my personal experience and then jump into research on the options available to us today. I hope we can leave with a view of the world spanning across channels with a focus on our contributors.</p>",
    "description": "<p>Topics to be discussed:\n-Defining new contributors - where are they coming from and why\n-Empathize with an inexperienced contributor trying to jump right in\n-Introduce the concept of \"a third place\"\n-The challenges of synchronous communication (IRC, Slack, Gitter, others)\n-Options to focus on asynchronous channels (Email, Discourse, others)\n-Understanding when you need which\n-A pitch for why IRC could be great with Matrix.org (Riot.im)\n-Bringing it back to the new contributor with specific examples\n-An optimistic conclusion of our collective efforts to improve</p>",
    "persons": [
      "Matthew Broberg"
    ]
  },
  {
    "start": 1577874900000,
    "duration": 25,
    "room": "UB5.230",
    "title": "The Ethics of Open Source",
    "subtitle": "A Critical Reflection",
    "track": "Community devroom",
    "abstract": "<p>Open Source was supposed to level the playing field for creating and consuming software by reducing the monopolistic power of companies building proprietary software. But we didn't get the kind of democratized gift economy we were expecting. Instead, we are seeing open source creating opportunities and incentives for the already privileged to create new, and exacerbate existing, injustices. To the extent that we want to use software to create a just world, we should reject the Open Source ideology, and start thinking seriously about what comes next.</p>",
    "description": "<p>The Open Source Movement has always been focused on code. The result is a system that sadly neglects people, and now many maintainers are in a bad place, as they struggle to figure out how to make ends meet even as their labor creates immense value for others, and how to avoid making the lives of others worse through weaponized code. We find ourselves in this position because the key Open Source values exacerbate an existing injustice; by valuing the consumers of code over the producers of code, Open Source helps concentrate power in the hands of already powerful economic actors at the expense of maintainers. I feel that not only could we do better, we have a moral imperative to find better development models.</p>\n\n<p>Building on Scanlon's contractualist theory of morality, we will apply it to the world of open source—and the results will shock you. Open Source as an ideology is focused first and foremost on code, rather than people. As I have argued in the past, and will continue to argue, morality is about people first and foremost. This by itself doesn’t damn the Open Source movement, but it doesn’t take a whole lot of digging into the heart of Open Source to see that it has created a context in which maintainers are dehumanized, atrocities are visited upon innocent third parties, and large wealthy corporations are lionized. If we think that people matter, then we must reject the Open Source ideology. I’ve not the foggiest idea what comes next, but it’s time to start having a serious conversation about what a collaborative development model that values people first and foremost looks like.</p>",
    "persons": [
      "Don Goodman-Wilson"
    ]
  },
  {
    "start": 1577876700000,
    "duration": 25,
    "room": "UB5.230",
    "title": "Engineers, Call Your Policy People!",
    "subtitle": "Lessons from the Campaign against the Copyright Directive",
    "track": "Community devroom",
    "abstract": "<p>To make a long story short, the game-changer in the SaveCodeshare campaign against the Copyright Directive was when the open letter reached the software team at one of Germany’s biggest industrial companies. Instead of just signing the open letter, they picked up the phone and called their Public Policy team in Brussels. Two weeks later, we saw new text in the Parliament’s version of the legal proposal.</p>\n\n<p>OFE therefore calls on everyone who cares about Free and Open Source Software, who works (or knows someone who works) at companies, across industrial verticals, to build relationships with your policy or government relations teams.</p>\n\n<p>This is more important than ever, as there are new challenges and opportunities for FOSS on the horizon. Apart from promoting FOSS, we have to make sure that it never becomes an unintended victim again, as was the case with the first version of the EU’s Copyright Directive. As we are entering the Age of Tech Regulation, we need to mature our advocacy to increase our effectiveness.</p>",
    "description": "<p>I hope to give you some ideas and tools for how to increase our your political impact as engineers and developers, and how to engage with your companies’ policy or government relations teams.</p>\n\n<p>To make a long story short, the game-changer in the SaveCodeshare campaign against the Copyright Directive was when the open letter reached the software team at one of Germany’s biggest industrial companies. Instead of just signing the open letter, they picked up the phone and called their Public Policy team in Brussels. Two weeks later, we saw new text in the Parliament’s version of the legal proposal.</p>\n\n<p>OFE therefore calls on everyone who cares about Free and Open Source Software, who works (or knows someone who works) at companies, across industrial verticals, to build relationships with your policy or government relations teams.</p>\n\n<p>This is more important than ever, as there are new challenges and opportunities for FOSS on the horizon. Apart from promoting FOSS, we have to make sure that it never becomes an unintended victim again, as was the case with the first version of the EU’s Copyright Directive. As we are entering the Age of Tech Regulation, we need to mature our advocacy to increase our effectiveness.</p>",
    "persons": [
      "Astor Nummelin Carlberg & Paula Grzegorzewska"
    ]
  },
  {
    "start": 1577878500000,
    "duration": 25,
    "room": "UB5.230",
    "title": "Building Ethical Software Under Capitalism",
    "subtitle": "",
    "track": "Community devroom",
    "abstract": "<p>The software that is the easiest to build -- the software that is the easiest to fund the development of -- tends to serve those who are already extremely well-served. So, how do we bridge the gap between what society needs and what many people with money want to fund? Free and open source software platforms can get us part of the way there, but without some big changes, it won't be enough. Let's talk structure!</p>",
    "description": "<p>We want to provide useful, intuitive, non-invasive software that all people can use, whether they personally have money for fancy customizations or not. But the software that is the easiest to build -- the software that is the easiest to fund the development of -- tends to serve those who are already extremely well-served. A technology community that primarily serves privileged people, while leaving all other users behind is not one we should expect people to spend their unpaid or volunteer time on. And for certain reprehensible functions, no one should be building the software at all, under any license. So, how do we bridge the gap between what society needs and what many people with money want to fund?</p>\n\n<p>This talk will cover:</p>\n\n<pre><code>* Non-profits, fundraising and community-building\n* Small businesses, co-ops and other niches\n* Possible changes to the broader landscape\n</code></pre>\n\n<p>If we want to build a better world, we will have to move beyond quick fixes and silver bullets. Free and open source software platforms can get us part of the way there, but without some big changes, it won't be enough. We need to build ethical structures for the creation of ethical software.</p>",
    "persons": [
      "Deb Nicholson"
    ]
  },
  {
    "start": 1577880300000,
    "duration": 25,
    "room": "UB5.230",
    "title": "Cognitive biases, blindspots and inclusion",
    "subtitle": "",
    "track": "Community devroom",
    "abstract": "<p>Open source thrives on diversity. The last couple of years has seen huge strides in that aspect with codes of conduct and initiatives like the Contributor Covenant. While these advancements are crucial, they are not enough. In order to truly be inclusive, it’s not enough for the community members to be welcoming and unbiased, the communities’ processes and procedures really support inclusiveness by not only making marginalized members welcome, but allowing them to fully participate.</p>",
    "description": "",
    "persons": [
      "Allon Mureinik"
    ]
  },
  {
    "start": 1577882100000,
    "duration": 25,
    "room": "UB5.230",
    "title": "Growing Sustainable Contributions Through Ambassador Networks",
    "subtitle": "",
    "track": "Community devroom",
    "abstract": "<p>Open Source Program Offices are utilizing ambassador programs more and more. We'll talk about why we decided to implement ambassador programs, how we implemented them, got buy-in (from a time and budget standpoint), and more.</p>\n\n<p>We'll both talk about how we use this program in our respective companies to scale and reach thousands of developers internally. We'll also throw in a few case studies and lessons learned throughout our (ongoing) journeys.</p>",
    "description": "<p>Comcast and Indeed are committed to fostering open source contributions to the external projects that we depend on. One type of program that both companies use is an Open Source Ambassador Program to help new and experienced individuals contribute to open source projects that they use.</p>\n\n<p>During this talk we’ll go over what an ambassador program is, how we decided to use them in our organizations, the path to buy-in and budget approval, how they were implemented, results we saw, and lessons learned. We’ll present specific case studies of how our Ambassador Programs helped with specific campaigns and how that fosters open source sustainability.</p>\n\n<p>At Indeed we’ll compare results from Hacktoberfest in 2018 and 2019, and the results that we saw before and after implementing the Open Source Ambassador Program.</p>\n\n<p>At Comcast, the Open Source Ambassador working group was formed in 2018 where ambassadors focus on Open Source practices, raising awareness, compliance and strategy. The aim of the OSAP is to foster discussion across the various Comcast tech offices and understand Open Source needs and issues across the organization.</p>",
    "persons": [
      "Alison Yu and Shilla Saebi"
    ]
  },
  {
    "start": 1577883900000,
    "duration": 25,
    "room": "UB5.230",
    "title": "Bringing back ethics to open source",
    "subtitle": "",
    "track": "Community devroom",
    "abstract": "<p>Most discussions around ethical licenses today consider the Open Source Definition (OSD) with the same reverence as Moses did the tablets delivered to him on Mount Sinai.</p>\n\n<p>The OSD is in fact much more mundane than that. And it tells us more about its authors than about the open source movement in general; had open source been born in less privileged circles, ethical considerations would have been baked in from the start.</p>\n\n<p>With that in mind, let's revisit what we're actually trying to collectively achieve through the open source movement and reconsider the notion that its mission requires we allow the software we build be used in violation of Human Rights.</p>\n\n<p>There are minimally-disruptive changes that can be made to the OSD and to existing licenses which would puth ethical concerns centerstage, where they belong, and help us foster responsibility and accountability within our community and within software vendors.</p>\n\n<p>We'll look at the past attempts at creating ethical licenses and why they have failed. We'll ask all of the hard questions, even those we don't have good answers to yet. And we'll propose a new, multi-pronged approach to this issue. One that we believe, while more demanding to implement, has a much better chance of success than previous attempts have had.</p>",
    "description": "",
    "persons": [
      "Tobie Langel"
    ]
  },
  {
    "start": 1577885700000,
    "duration": 25,
    "room": "UB5.230",
    "title": "Be The Leader You Need in Open Source",
    "subtitle": "Learn key skills to guide yourself and your project towards a healthy future",
    "track": "Community devroom",
    "abstract": "<p>Stronger open source leadership can address a myriad of sustainability challenges and there is a call for more leaders in every project. Good news! Every contributor is a leader either through self leadership, leading others, or leading the community, yet most people have never been trained on how to lead.</p>\n\n<p>This talk provides the leadership the training you need and covers:\n- Why strengthen community leadership\n- Key leadership and emotional intelligence principles\n- Practical ways to lead as a contributor</p>",
    "description": "<p>Open source crossed the chasm into mainstream with users in all industries. Maintaining the users’ trust and sustaining innovation is key to open source’s success.</p>\n\n<p>However, in a world where communities are passionate, multicultural, and primarily use online communication, it is challenging to move communities towards a shared vision in a frictionless, sustainable way. Community challenges can impact innovation, putting user adoption at risk and even more importantly, hurting community members.</p>\n\n<p>Stronger open source leadership can address these challenges and there is a call for more leaders in every project. Good news! Every contributor is a leader either through self leadership, leading others, or leading the community, yet most people have never been trained on how to lead.</p>\n\n<p>This talk provides the leadership the training you need and covers:\n- Why strengthen community leadership\n- Key leadership and emotional intelligence principles\n- Practical ways to lead as a contributor</p>",
    "persons": [
      "Megan Sanicki"
    ]
  },
  {
    "start": 1577887500000,
    "duration": 25,
    "room": "UB5.230",
    "title": "Building Community for your Company’s OSS Projects",
    "subtitle": "",
    "track": "Community devroom",
    "abstract": "<p>Your company has just started an open source project, but where is the community? This talk provides practical tips and suggestions along with what not to do when building a community around your company’s open source project.</p>",
    "description": "<p>Building a community around your company’s open source project is no easy task, and there is no magic bullet or one size fits all solution. However, there are some things that you can do (or not do) to increase the chances of successfully building a community for your project.</p>\n\n<p>A few of the dos and don’ts covered in this talk include:</p>\n\n<ul>\n<li>Planning and product management: Do use a transparent process in the open with tools that allow anyone to participate. Don’t use your internal tools and private meetings to make all of the decisions.</li>\n<li>Encourage participation: Do be proactive about helping community members contribute in meaningful ways. Don’t inadvertently set the expectation that employees will be the ones always answering questions and making decisions.</li>\n<li>Be honest: Do be honest with yourselves about where and how you prefer to have community members contribute. Don’t encourage people to contribute in areas where you are less likely to accept outside contributions.</li>\n<li>Managing contributions: Do have enough people trained in how to provide constructive feedback to manage the flow of incoming community contributions. Don’t assume that your existing developers have the time and skills to magically perform this difficult function.</li>\n</ul>\n\n\n<p>The audience will walk away with practical advice about building communities for corporate open source projects.</p>",
    "persons": [
      "Dawn Foster"
    ]
  },
  {
    "start": 1577889300000,
    "duration": 25,
    "room": "UB5.230",
    "title": "Lessons Learned from Cultivating Open Source Projects and Communities",
    "subtitle": "",
    "track": "Community devroom",
    "abstract": "<p>Over the last decade, I’ve had the privilege professionally of building and cultivating some Open Source projects and communities. I’ve grown other projects along the way some successful, and some not. I’ve learned a ton on this journey; honestly still am, and I want to tell this story.</p>",
    "description": "<p>Over the last decade, I’ve had the privilege professionally of building and cultivating some Open Source projects and communities. To start off this isn’t a tools talk, this is a talk about the soft skills you have to have to be able to succeed as a leader in an Open Source project. My journey started tending the frequently asked questions for a small Linux Distribution called CRUX, and then years later professionally moved to the OpenStack-Chef project to build OpenStack clouds. I’ve grown other projects along the way helped build tooling and communities some successful and still running today, others were just flashes in the pan.  I’ve learned a ton on this journey; honestly still am, but I have some lessons that are hard learned and hopefully I warn pitfalls that can cause wasted cycles and pain.\nI’ll be going over:</p>\n\n<pre><code>This isn’t a tools talk\nScoping your project\n    Personally-backed\n    Corporate-backed\nEmpathy and audience is important\n    Celebrations\n    Defeats\nSuccessful traits of Open Source projects\n    Trust\n    Clear Vision\n    Have a plan to move on if needed\nHonestly, is it even worth this hassle?\n</code></pre>",
    "persons": [
      "JJ Asghar"
    ]
  },
  {
    "start": 1577891100000,
    "duration": 25,
    "room": "UB5.230",
    "title": "Free software in education",
    "subtitle": "News on tools and developments for free software and data liberation in schools",
    "track": "Community devroom",
    "abstract": "<p>Schools is where IT and software users of tomorrow are made, and next to teaching digital skills, educating on privacy and consequences of the use of different types of software and servcies plays an important role. We would like to report on various projects from the field.</p>",
    "description": "<p>Helping schools and teachers with using free software is far more involved than just selling a product. Where big companies have huge budgets for advertising and marketing, free software projects have to attract educators with the power of the community.</p>\n\n<p>The most exciting upside of this is that our community does not only sell a product, but wants to get people involved. One of the benefits of free software for educators is that all our community goals play into their hands - free software is the basis of extending independence, democracy and all values modern schools are supposed to convey into the digital lives of students.</p>\n\n<p>As a person or project getting involved with free software in education, there are many challenges and opportunities. Teckids and the projects around it have collected experience from the work with schools, teachers, political decision makers and free software developers that we would like to share with the community.</p>",
    "persons": [
      "Dominik George"
    ]
  },
  {
    "start": 1577892900000,
    "duration": 25,
    "room": "UB5.230",
    "title": "Engaging Enterprise consumers of OSS",
    "subtitle": "Enterprise contribution, participation, and support of OSS",
    "track": "Community devroom",
    "abstract": "<p>It is no secret that open source software is a foundational element to many enterprise IT and software development strategies and it's also not a secret that the rate of participation, contribution, or support amongst many enterprise companies lags significantly behind the adoption rate. Higher rates of participation are seen in software-based or forwarded companies founded in the past decade, but older companies have been slow to adapt. The solution to participation is often seen as a cultural shift, but this only accounts for a portion of the lack of participation. Motiviation and incentive structures, legal structures, and project governance and management structual alignments can have a bigger impact on enterprise participation in open source projects. In this talk I'd like to discuss a mixture of academic research and my personal real-world experience in bridging the gap between enterprise development and open source projects.</p>",
    "description": "<p>Over the past decade Open Source has grown to become the de facto standard and preferred software for Enterprise businesses and government agencies and continues to be the choice amongst small businesses and non-profits. We are now entering a new era where not only are these organizations adopting the use of Open Source, but they are actively participating in and contributing back to the projects. This is a large shift that requires change both within the Enterprise organizations and in the open source projects to welcome these new contributors and methods of working. However, not all organizations are ready for this shift and not all open source projects are ready to handle enterprise collaboration.</p>\n\n<p>As an example the motivation and incentive structure are often misaligned with an open source project incentivizing long-term participation and enterprise software development encouraging short and fast development and deployment. The problem is compounded when discussing the role of system integrators, outsourced development agencies, and consulting agencies, which are frequently used to accelerate the development of enterprise software. This mismatched timelines, management practices, and incentive structured can lead to reduced participation in open source software. However, there are changes that can be made on both sides to counteract this tension, which can lead to greater participation of enterprise software developers in open source software.</p>\n\n<p>In this talk I’d like to call upon academic research and lessons learned in other industries. In particular I pull lessons from the following areas:</p>\n\n<p>“Community Development as a Process: 1970 - Lee J. Cary - A collection of academic researching in community development organizations with reviews of the psychological (Warren C. Haggstrom) and sociological (Willis A. Sutton, Jr) implications of development organization (including discussions of Burnout and the impact thereof to the individual and community), and the role of the agent in the community development process (Robert Morris)</p>\n\n<p>“Governing The Commons” - 1990 - Elinor Ostrom - An academic review of institutions for collective actions including longitudinal studies of fisheries and forest management.</p>\n\n<p>“Roles of Boundary Organizations” - 2008 - Siobhan O’Mahony and Beth A. Bechky (University of California, Davis) - Research drawing on social movement and organizational theory that reviews the roles of a boundard organization (Association, Foundations, etc.) in managing four critical domains - governance, membership, ownership, and control over production - to provide\nanalytic levers for determining when boundary organizations work.</p>\n\n<p>“How Firms Leverage Crowds and Communities for Open Innovation” - 2016 - Joel West (Keck Graduate Institute) and Jonathan Sims (Babson College) - Research on crowds and communities, identifying a third form — a crowd-community hybrid — that combines attributes of both</p>\n\n<p>\"100 Years of Sustainability\" - 2019 - Me :) Jacob Redding - A review of the American Society of Composers, Artists, and Publishers (ASCAP) and the lessons the open source world could learn and adapt to build the next century of sustainability in open source development and innovation.</p>\n\n<p>In addition I want to bring in my own personal experience spending 12+ years growing the Drupal Open Source project spending time as a developer, founding board member, and founding Executive Director watching the project grow from a handful of developers to over 30,000 active contributors. I also pull from the other side of the table in my current role as an Open Source Strategy/Governance lead within Accenture - a systems integrator with nearly 500,000 employees worldwide. In my role at Accenture I craft our internal open source strategies including the use of Inner source and legal and cultural shifts to encourage more open source participation.</p>",
    "persons": [
      "Jacob Redding"
    ]
  },
  {
    "start": 1577894700000,
    "duration": 25,
    "room": "UB5.230",
    "title": "Recognising Burnout",
    "subtitle": "",
    "track": "Community devroom",
    "abstract": "<p>Mental health is becoming an increasingly important topic. For this talk Andrew will focus on one particular aspect of mental health, burnout. Including his own personal experiences of when it can get really bad and steps that could be taken to help catch it early.</p>",
    "description": "<p>Working in technology can be extremely demanding and stressful. People put a lot of passion and themselves into what they do, removing the separation of their work from themselves. This can lead to burnout in many cases which is similar to depression in many ways.</p>\n\n<p>In this talk Andrew will talk through his personal experience of his worst case of burnout including the mental and physical toll it took, as well as giving advice on how to spot it early and ways to help mitigate against it.</p>",
    "persons": [
      "Andrew Hutchings"
    ]
  },
  {
    "start": 1577896500000,
    "duration": 25,
    "room": "UB5.230",
    "title": "How Does Innersource Impact on the Future of Upstream Contributions?",
    "subtitle": "",
    "track": "Community devroom",
    "abstract": "<p>Innersource is a growing phenomenon that is widely viewed as improvement over existing regimes of proprietary silos within for-profit corporate walls. The bargain it strikes is compelling but curious: developers yield benefits that please them regarding software sharing &amp; improvement, while companies succeed in keeping their software crown jewels locked up &amp; locked down.\n How will that impact software freedom? Will it increase or decrease upstream contribution?  Will developers use Innersource as a jumping ground to FLOSS contribution, or will silos stay siloed?   What can Open Source Program Offices do to mitigate downsides to Innersource in an effort to increase FOSS-curious employee retention and interest? This talk explores these issues.</p>",
    "description": "<p>Innersource is a growing phenomenon that is widely viewed as improvement over existing regimes of proprietary silos within for-profit corporate walls. The bargain it strikes is compelling but curious: developers yield benefits that please them regarding software sharing &amp; improvement, while companies succeed in keeping their software crown jewels locked up &amp; locked down.\n How will that impact software freedom? Will it increase or decrease upstream contribution?  Will developers use Innersource as a jumping ground to FLOSS contribution, or will silos stay siloed?   What can Open Source Program Offices do to mitigate downsides to Innersource in an effort to increase FOSS-curious employee retention and interest? This talk explores these issues.</p>",
    "persons": [
      "Bradley M. Kuhn"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 5,
    "room": "UD2.119",
    "title": "Welcome to the Free Tools & Editors Room!",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>A quick introduction to the room, the sessions, and the team that put everything together. :-)</p>",
    "description": "",
    "persons": [
      "Geertjan Wielenga",
      "Lars Vogel",
      "Trisha Gee"
    ]
  },
  {
    "start": 1577869500000,
    "duration": 20,
    "room": "UD2.119",
    "title": "Insights into the Eclipse Open Source Project - News from the Eclipse Platform and IDE Project",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>Join this talk to learn about the current status of the Eclipse IDE Open Source projects. We'll talk about new developments, our improvements in the development process, performance improvements, and new features in the Eclipse IDE.</p>",
    "description": "<p>In this talk, we'll demo the new features of the Eclipse IDE and show the improvements of the latest, best, and fastest Eclipse IDE ever.</p>",
    "persons": [
      "Lars Vogel"
    ]
  },
  {
    "start": 1577871000000,
    "duration": 20,
    "room": "UD2.119",
    "title": "Surfing the Tsunami - News from the IntelliJ IDEA Community",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>With releases of Java coming thick and fast every 6 months, it's a full time job staying on top of the features. If your IDE can help you here, it's one less thing to worry about.  IntelliJ IDEA Community had three releases this year, each on improved the support for modern versions of Java, but that's not the only thing on offer.</p>\n\n<p>Join this session to see what's new in IntelliJ IDEA Community (the free one!). This is not limited to just supporting new language features, which some of us might not get to use for ages, but better support for things developers do every day, and improved performance and stability, because an all-singing, all-dancing IDE is all well and good, but if it's not usable those features mean nothing.</p>",
    "description": "",
    "persons": [
      "Trisha Gee"
    ]
  },
  {
    "start": 1577872500000,
    "duration": 20,
    "room": "UD2.119",
    "title": "From Oracle to Apache - News from the NetBeans Community",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>NetBeans is now a top level Apache project! How did it get to Apache and what's the state of the donation process? What are the new features and how can you get involved? Join this session to find out!</p>",
    "description": "",
    "persons": [
      "Geertjan Wielenga"
    ]
  },
  {
    "start": 1577874000000,
    "duration": 20,
    "room": "UD2.119",
    "title": "New Java Features & Apache NetBeans",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>The Java platform experiences an outburst of cool new features – recently, local variable type inference, switch enhancements and multi-line string literals have been added to the Java language. Many other features are being in the pipeline and are actively worked on, like simple data carriers and pattern matching for the Java language, or  value classes for the Java virtual machine. These features are delivered quickly, thanks to the recently adopted, predictable, six-months schedule of major Java SE releases. This new release cadence means new Java platform features are delivered twice every year!</p>\n\n<p>In this talk, we will show a live demo of many of the recently added and newly developed features and improvements for the Java platform, including those mentioned above. The Apache NetBeans IDE will be used to demonstrate the features.</p>",
    "description": "",
    "persons": [
      "Jan Lahoda"
    ]
  },
  {
    "start": 1577875500000,
    "duration": 20,
    "room": "UD2.119",
    "title": "OpenBeans IDE - Creating an Apache NetBeans Distribution",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>A talk about how OpenBeans, an Apache NetBeans IDE distribution was possible due to pkgsrc, the NetBSD package management framework.</p>",
    "description": "<p>OpenBeans IDE started in Nov 2018 under the name CoolBeans. This was an Apache NetBeans IDE 'distribution' which repackaged the Apache NetBeans modules, plus some other modules that were still not available yet (such as the C/C++ support modules, JavaEE modules, native notifications on macOS/Windows, etc).</p>\n\n<p>CoolBeans was open sourced under the plain ISC license under the OpenBeans name.</p>\n\n<p>OpenBeans produces Windows installers as well as macOS disk images for end users. It does this using pkgsrc from NetBSD, which seems to be a very good match for distribution-like projects.</p>",
    "persons": [
      "Emilian Bold"
    ]
  },
  {
    "start": 1577877000000,
    "duration": 20,
    "room": "UD2.119",
    "title": "LSP for Java and GraalVM Development",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>In this session, we will explore how the Language Server Protocol, LSP, is used to aid developers. We will cover not only the Java language, but also other languages, especially those supported by the GraalVM, like JavaScript, R, Python, Ruby. This includes development in various IDEs and editors, for example the NetBeans IDE and VisualStudio Code.</p>",
    "description": "",
    "persons": [
      "Martin Entlicher"
    ]
  },
  {
    "start": 1577878500000,
    "duration": 20,
    "room": "UD2.119",
    "title": "Eclipse Loves LSP - Achieving More with Less",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>Eclipse and Language Server Protocol - what, why, how and most important visible results for users thanks to it.</p>",
    "description": "<p>Brief description of how it all started. Followed by examples of the success stories benefiting almost every Eclipse user by getting better tools while minimizing the burden on Eclipse developers.</p>",
    "persons": [
      "Alexander Kurtakov"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 20,
    "room": "UD2.119",
    "title": "Language Server Protocol & Debug Adapter Protocol to the Rescue of Web Development in Eclipse IDE",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>The state of Web (HTML, CSS, JS...) development in Eclipse IDE used to be bad. Indeed, some internal parsers had to be maintained to follow the frequent and major changes in those standards; the developer community behind them couldn't catch up, leading tools to a pretty bad and outdated state. Fortunately, the rise of technologies like TextMate grammars, Language Servers and Debug Adapters as reusable components have allowed Eclipse ecosystem to catch up with the best tools for web development. By integrating pieces of VSCode and others and with a relatively small investment, Eclipse Wild Web Developer provides a quite comfortable and efficient tool set for a wide variety of web-based projects. In this presentation, we'll demo Wild Web Developer in practicie, explaining as we demo how the LSP/DAP world is leveraged to enable those productive workflows.</p>",
    "description": "",
    "persons": [
      "Mickael Istria"
    ]
  },
  {
    "start": 1577881500000,
    "duration": 20,
    "room": "UD2.119",
    "title": "Flutter Development in Eclipse",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>The Eclipse IDE was once heavily used for mobile app development on Android. The Android Development Tools (ADT) for Eclipse have since been deprecated and replaced by a dedicated Android Studio. To make mobile app development possible again, we leveraged the language server protocol to provide stable support for the Dart language and the Flutter SDK in the Eclipse IDE.</p>\n\n<p>This talk will provide an overview of the Dartboard project including its Dart language support as well as the Flutter development experience.</p>",
    "description": "",
    "persons": [
      "Jonas Hungershausen"
    ]
  },
  {
    "start": 1577883000000,
    "duration": 20,
    "room": "UD2.119",
    "title": "Emacs Should Be Emacs Lisp - Thoughts on the Future of Emacs",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>Emacs Lisp is good, actually, and Emacs should primarily be written in Emacs Lisp.  This talk will describe a way forward.</p>",
    "description": "<p>The future of the Emacs core is a frequent topic in the Emacs community.  Should the core be in C?  Or Rust?  Should Elisp continue to be the scripting language?  Or Guile?  Or Python, Perl (the \"Perfect Emacs Rewriting Language\") or JS?</p>\n\n<p>This talk advocates the rarely discussed view that Emacs Lisp is good, actually, and that Emacs should be written in Emacs Lisp.  Threading, compilation, and addressing Emacs' other low-level deficiencies will be covered, with an eye toward a practical way to roll out the changes.</p>",
    "persons": [
      "Tom Tromey"
    ]
  },
  {
    "start": 1577884500000,
    "duration": 20,
    "room": "UD2.119",
    "title": "TerosHDL - Open Source IDE for FPGA Developers.",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>TerosHDL is an open source project focused in the development and integration of EDA tools (ghdl, VUnit...) in an IDE. It is currently based on Atom, but in the future it will be extended to other code editors such as Visual Studio Code.</p>",
    "description": "<p>The goal of TerosHDL is bringing all facilities of software code tools to the HDL (VHDL and Verilog)development: linter, code completion, simulators management, automate documentation, snippets...</p>\n\n<p>https://youtu.be/tgr1KGIitIQ.</p>\n\n<p>We will introduce TerosHDL 2.0 with multiple features. In the new release the architecture has been completely rebuild in order to support more tools, reduce some dependencies and clarify the code. Some of the new features include Verilog support, additional simulators, a linter and more beautiful documentation.</p>",
    "persons": [
      "Carlos Alberto"
    ]
  },
  {
    "start": 1577886000000,
    "duration": 20,
    "room": "UD2.119",
    "title": "Projectional Editing and Its Implications",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>Let’s shake some of the dogmas that constrain our programming worldview. In this session, I would like to take you to an alternative world - a world where programming languages are not parsed, a world where languages can be downloaded from the Internet and plugged easily into your IDE. A world where you have the power to customize the languages that you use. You’ll see that projectional editing in JetBrains MPS gives you incredible freedom in how you express your thoughts. It allows you to choose notations that best fit the task at hand. Your code can be edited as text, tables, diagrams, a form, or a combination of those. This is especially useful for Domain-specific languages and we’ll see real-life examples from domains such as the insurance industry, embedded software development, bioinformatics, enterprise systems and legislation. We’ll also discuss the downsides and integration challenges that projectional editors face. My goal is that you’ll leave this session inspired, enriched and motivated to try something new.</p>",
    "description": "",
    "persons": [
      "Václav Pech"
    ]
  },
  {
    "start": 1577887500000,
    "duration": 20,
    "room": "UD2.119",
    "title": "IntelliJ Elixir - Elixir Plugin for JetBrains IDEs",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>Using Java, Kotlin, and GrammarKit to reimplement to Erlang, Yecc grammars, and Elixir for static analysis for Elixir source and BEAM bytecode.  How decompiling and disassembly tools can quickly answer optimization arguments.</p>",
    "description": "<p>IntelliJ Elixir is the Elixir plugin for JetBrains IDEs like IntelliJ and Rubymine.  It uses JetBrains OpenAP, JFlexI and GrammarKit to reimplement the Elixir grammar, which is natively implemented as bespoke Erlang lexer and YECC LALR parser.  This meant translating a recursive Erlang lexer into a strict regular expression state machine used by JFlex with some interesting needed extension.  Porting the grammar from LALR Yecc to the LL Pratt Parser generated by Grammar Kit involved understanding the non-universality of BNF.  Reimplementing and extensive testing of the plugin led to finding bugs in native Elixir, showing that alternative implementations of languages in editors and tools can find bugs in the original implementations.  The BEAM bytecode decompiler and disassembler has led to better understanding of how the VM optimizes different Elixir code.</p>",
    "persons": [
      "Luke Imhoff"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 20,
    "room": "UD2.119",
    "title": "VSCode Extension for OpenShift Developers",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>IDE based extension to run the instance of OpenShift on the local machine.</p>\n\n<p>Easy to use all OpenShift related command in VSCode to create, building, and deploying an application on OpenShift.</p>\n\n<p>Key points:</p>\n\n<ol>\n<li>OpenShift VSCode IDE base extension and it's Dependency.</li>\n<li>Ease installation of extension from VSCode Market Place.</li>\n<li>How to create Project, application, component, Services, Storage and more in OpenShift VSCode extension (Demo)</li>\n</ol>\n\n\n<p>Link: https://github.com/redhat-developer/vscode-openshift-tools</p>",
    "description": "",
    "persons": [
      "Sudhir Verma"
    ]
  },
  {
    "start": 1577890500000,
    "duration": 20,
    "room": "UD2.119",
    "title": "Developer Workspace As Code - Is Developer Heaven in the Cloud?",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>Developer workspaces are assembled using outdated wiki pages and usually require weeks to \"get right\". What if the IDE and workspace setup (in the broadest sense) lived directly with the code?</p>",
    "description": "<p>In the age of devops many things live with the source code. We compose our applications from Kubernetes/OpenShift objects, CI/CD pipelines and IDEs have their configuration in dot files, all defined along with the source code of our precious applications. But how do you define the common development environment/workspace for your developers? Let's take a look at the devfile - a declarative format for specifying the developer workspace with all the tools the developers need to code, build, test and debug the applications and how it enables Eclipse Che to be the next-gen Kubernetes-native IDE for developer teams.</p>",
    "persons": [
      "Lukas Krejci"
    ]
  },
  {
    "start": 1577892000000,
    "duration": 20,
    "room": "UD2.119",
    "title": "FaaS You Like It: Create Serverless Functions & Run Anywhere",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>\"Serverless\" is a hot topic right now, and something that a lot of developers are keen to try.</p>\n\n<p>A lot of focus has been on implementations that are proprietary to and only run on a single provider's cloud.</p>\n\n<p>In this talk, I'll show how you can develop \"serverless\" functions on your laptop, with an open source platform and run them where you like.</p>",
    "description": "<p>However, if you attend a Serverless event, you may come away with the impression that it's a world of proprietary walled gardens from the major cloud providers.</p>\n\n<p>In this talk I will cover:</p>\n\n<ul>\n<li> The case for open source serverless frameworks in general</li>\n<li> The fnproject (fnproject.io) in particular</li>\n<li> Implementing serverless Shakespeare on a laptop</li>\n</ul>",
    "persons": [
      "Ewan Slater"
    ]
  },
  {
    "start": 1577893500000,
    "duration": 20,
    "room": "UD2.119",
    "title": "Bach.java",
    "subtitle": "Lightweight Java Build Tool for modular projects",
    "track": "Free Tools and Editors",
    "abstract": "<p>I present \"Bach.java\" - a lightweight build tool for Java. Bach.java uses jshell/java to build\nmodular Java projects. It supports a \"zero installation\" run mode, convention over configuration\npragmatism, simple properties file to tweak defaults, and an API to build custom projects.</p>\n\n<p>In the spirit of Rémix Forax, who wrote: \"No need to be a maven to be able to use a build tool\",\nBach.java is targeted to coders of small to mid-size Java projects, who want to focus on their\nideas and modules instead of learning and taming a build tool.</p>",
    "description": "",
    "persons": [
      "Christian Stein"
    ]
  },
  {
    "start": 1577895300000,
    "duration": 20,
    "room": "UD2.119",
    "title": "Unit Testing with JUnit Jupiter - How to Use the new JUnit Jupiter Platform",
    "subtitle": "",
    "track": "Free Tools and Editors",
    "abstract": "<p>Starting with the new <a href=\"https://junit.org/\">JUnit Jupiter Platform</a> which is available for longer time there are much more options than with JUnit 4 or TestNG. This lecture will show the differences of JUnit Jupiter platform and how it can be used to write better unit- and or integration tests.</p>",
    "description": "<p>It will be shown how to migrate to JUnit Jupiter platform and which things should be considered/changed based on the differences between JUnit Jupiter and JUnit 4/TestNG and which things have been changed in comparison to TestNG/JUnit 4. Furthermore things like dynamic tests will be taken a short overview and things like extensions will also taken into account.</p>",
    "persons": [
      "Karl Heinz Marbaise"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 5,
    "room": "UD2.120 (Chavanne)",
    "title": "Intro",
    "subtitle": "Monitoring and Observability Devroom",
    "track": "Monitoring and Observability",
    "abstract": "<p>Introduction and welcome to the monitoring and observability devroom</p>",
    "description": "",
    "persons": [
      "Richard Hartmann"
    ]
  },
  {
    "start": 1577869800000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Distributed Tracing for beginners",
    "subtitle": "",
    "track": "Monitoring and Observability",
    "abstract": "<p>Distributed tracing is a tool that belongs to every developer's tool belt, but what it actually can do remains a mystery to most developers.</p>\n\n<p>In this slideless talk, we will introduce you to the world of distributed tracing by developing a cloud native application from scratch and applying all important distributed tracing concepts in practice, at first by hand and then by using existing libraries to automate our work.</p>\n\n<p>We will deploy this application in a Kubernetes cluster and see how we can take advantage of infrastructure components like service meshes to understand the routing decisions taken for specific requests.</p>\n\n<p>You will learn not only what distributed tracing is, but how it works, what it can do and what it can’t. By the end of this talk, you will have working knowledge to start using distributed tracing tools with your new projects, as well as with your legacy ones.</p>",
    "description": "",
    "persons": [
      "Juraci Paixão Kröhling"
    ]
  },
  {
    "start": 1577872200000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Grafana: Successfully correlate metrics, logs, and traces",
    "subtitle": "",
    "track": "Monitoring and Observability",
    "abstract": "<p>This talk presents current capabilities of Grafana to integrate metrics, logs and traces and shows how to setup both Grafana and application code to be able to correlate all 3 in Grafana. It assumes some familiarity with Grafana to follow the How To steps but should be suitable for beginner users. Afterwards it shows future direction of Grafana in context of \"Experiences\", for even more seamless experience when correlating data from multiple data sources.</p>",
    "description": "<ul>\n<li>Quick intro to observability, alerting > metrics > logs > traces.</li>\n<li>How to instrument sample 3 tiered app (metrics, logging, traces).</li>\n<li>How to setup Grafana to observe the sample app.\n\n<ul>\n<li>Setup metrics, logging and tracing data source</li>\n<li>Setup integrations between data sources.</li>\n</ul>\n</li>\n<li>Live demo of Grafana UI, going from metric to logs to trace when debugging.</li>\n<li>Future of Grafana and look at \"Experiences\".</li>\n</ul>",
    "persons": [
      "Andrej Ocenas"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Jaegertracing in Ceph",
    "subtitle": "An interesting case of distributed tracing",
    "track": "Monitoring and Observability",
    "abstract": "<p>Jaeger and Opentracing provide ready to use tracing services for distributed systems and are becoming widely used de-facto standard because of their ease of use. Making use of these libraries, Ceph, can reach to a much-improved monitoring state, supporting visibility to its background distributed processes. This would, in turn, add up to the way Ceph is being debugged, “making Ceph more transparent” in identifying abnormalities.\nIn this session, the audience will get to learn about using distributed tracing in large scale distributed systems like Ceph, an overview of Jaegertracing in Ceph and how someone can use it for debugging Ceph.</p>",
    "description": "",
    "persons": [
      "Deepika Upadhyay"
    ]
  },
  {
    "start": 1577877000000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Stories around ModBus",
    "subtitle": "Why ModBus is worse than SNMP",
    "track": "Monitoring and Observability",
    "abstract": "<p>Society would end if all ModBus stopped working overnight. Good thing it has zero security built in. Still, it's useful to get data out of industrial systems, be they a datacenter or a power plant.</p>",
    "description": "",
    "persons": [
      "Richard Hartmann"
    ]
  },
  {
    "start": 1577879400000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Monitoring strawberries",
    "subtitle": "Building observability for indoor farming",
    "track": "Monitoring and Observability",
    "abstract": "<p>According to the United Nations, 2.5 billion more people will be living in cities by 2050. This trend has caused indoor farming to draw a lot of attention and effort in recent years, in an attempt to scale the production of highly nutritious, healthy food inside cities.</p>\n\n<p>Over the past 3 years, Agricool has recycled 20 industrial containers into farms that grow strawberries, herbs and salads, in the very heart of cities, and without any pesticide. These urban farms are currently operated in Paris and Dubaï.</p>\n\n<p>Operating a fleet of indoor farms presents a diverse set of observability challenges. At the most traditional end of the observability spectrum, engineers rely on devops tools to operate computers, microservices, and an IoT infrastructure embedded inside the farms. On the other hand, living organisms like strawberry plants draw their own observability requirements, such as disease detection, physiological measurements, nutrient absorption, water analysis, or exposition rate to pollinating bumblebees.</p>\n\n<p>The purpose of this talk is to highlight observability challenges and best practices that are specific to indoor farming, and to illustrate them through the learnings that were made at Agricool when building observability pipelines.</p>",
    "description": "<ul>\n<li><p>Deployment of microservices to automate indoor farming environments.</p></li>\n<li><p>How to build an operational model for indoor farming based on telemetry, alerting and event stores, by using widely adopted devops/observability tools like Docker, Prometheus, InfluxDB, Grafana and Redash.</p></li>\n<li><p>Discover agronomic SLAs to drive design decisions for observability. eg. outages in irrigation systems that last too long can cause irreparable damage to strawberry plants, or a too great exposure to pollinating bumblebees can damage flowers and reduce yield.</p></li>\n<li><p>Understand that indoor farming requires to apply observability to a wide-ranging set of technical aspects such as microservices, hardware, sensors, actuators, hydraulic circuits, lighting systems, electrical cabinets, climate regulations, plant health or water quality.</p></li>\n<li><p>Build observability within an R&amp;D context in which scientists are continuously figuring out how to use time series to make breakthroughs about plant physiology.</p></li>\n<li><p>Align employees with different technical backgrounds (agronomists, engineers, technical operators) around a shared set of observability best practices.</p></li>\n<li><p>Enable domain experts to craft their own visualizations and alert rules.</p></li>\n</ul>",
    "persons": [
      "Jean-Marc Davril"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Querying millions to billions of metrics with M3DB's inverted index",
    "subtitle": "",
    "track": "Monitoring and Observability",
    "abstract": "<p>The cardinality of monitoring data we are collecting today continues to rise, in no small part due to the ephemeral nature of containers and compute platforms like Kubernetes. Querying a flat dataset comprised of an increasing number of metrics requires searching through millions and in some cases billions of metrics to select a subset to display or alert on. The ability to use wildcards or regex within the tag name and values of these metrics and traces are becoming less of a nice-to-have feature and more useful for the growing popularity of ad-hoc exploratory queries.</p>\n\n<p>In this talk we will look at how Prometheus introduced the concept of a reverse index existing side-by-side with a traditional column based TSDB in a single process.  We will then walk through the evolution of M3’s metric index, starting with ElasticSearch and evolving over the years to the current M3DB reverse index. We will give an in depth overview of the alternate designs and dive deep into the architecture of the current distributed index and the optimizations we’ve made in order to fulfill wildcards and regex queries across billions of metrics.</p>",
    "description": "",
    "persons": [
      "Rob Skillington"
    ]
  },
  {
    "start": 1577884200000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Secret History of Prometheus Histograms",
    "subtitle": "",
    "track": "Monitoring and Observability",
    "abstract": "<p>Representing distributions in a metrics-based monitoring system is both important and hard. Doing it right unlocks many powerful use cases that would otherwise require expensive event processing. Prometheus offers the somewhat weirdly named Histogram and Summary metric types for distributions. How have they become what they are today with all their weal and woe? To help understand the present, let's shed light on the past. Studying this piece of Prometheus's history will also allow a glimpse of the bigger picture, why certain things are the way they are in Prometheus, and which parts of the original vision are still awaiting fulfillment.</p>",
    "description": "",
    "persons": [
      "Björn Rabenstein (Beorn)"
    ]
  },
  {
    "start": 1577886600000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Are You Testing Your Observability? Patterns for Instrumenting Your Services",
    "subtitle": "",
    "track": "Monitoring and Observability",
    "abstract": "<p>Observability is the key to understand how your application runs and behaves in action. This is especially true for distributed environments like Kubernetes, where users run Cloud-Native microservices.</p>\n\n<p>Among many other observability signals like logs and traces, the metrics signal has a substantial role. Sampled measurements observed throughout the system are crucial for monitoring the health of the applications and, they enable real-time, actionable alerting. While there are many open-source robust libraries, in various languages, that allow us to easily instrument services for backends like Prometheus, there are still numerous possibilities to make a mistake or misuse those tools.</p>\n\n<p>During this talk, two engineers from Red Hat: Kemal and Bartek (Prometheus and Thanos project maintainer) will discuss valuable patterns and best practices for instrumenting your application. The speakers will go through common pitfalls and failure cases while sharing valuable insights and methods to avoid those mistakes. In addition, this talk will demonstrate, how to leverage unit testing to verify the correctness of your observability signals. How it helps and why it is important. Last but not least, the talk will cover a demo of the example instrumented application based on the experience and projects we maintain.</p>\n\n<p>The audience will leave knowing how to answer the following important questions:</p>\n\n<p>What are the essential metrics that services should have?\nShould you test your observability? What are the ways to test it on a unit-test level?\nWhat are the common mistakes while instrumenting services and how to avoid them?</p>\n\n<p>And more!</p>",
    "description": "<p>The end goal of this talk is to demonstrate to the audience, how to harvest the powers of metric-based instrumentation in their applications. We would like to share some pragmatic, best practices and common patterns that we learned while maintaining several open-source projects.</p>\n\n<p>During this talk:</p>\n\n<p>We will discuss valuable patterns and best practices for instrumenting libraries and applications.\nWe will go through a set of common pitfalls failure cases, and methods to avoid those mistakes. Some of the topics we plan to mention: common cardinality issues, summaries vs histograms, choosing histogram bucket, testing, instrumenting libraries vs applications, common middlewares etc\nWe will demonstrate, why, when and how to leverage unit testing to verify your observability signals.\nWe plan to present a demo of the example instrumented application. We plan to use Go as an example language of such application but the talk should be mostly language agnostic.</p>",
    "persons": [
      "Bartek Plotka",
      "Kemal Akkoyun"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "How to measure Linux Performance Wrong",
    "subtitle": "",
    "track": "Monitoring and Observability",
    "abstract": "<p>In this presentation, we will look at typical mistakes measuring or interpreting Linux Performance. Do you use LoadAvg to assess if your CPU is overloaded or Disk Utilization to see if your disks are overloaded?  We will look into these and a number of other metrics that are often misunderstood and/or misused as well as provide suggestions for better ways to measure Linux  Performance.</p>",
    "description": "",
    "persons": [
      "Peter Zaitsev"
    ]
  },
  {
    "start": 1577891400000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "From Zero to Useless to Hero: Make Runtime Data Useful in Teams",
    "subtitle": "",
    "track": "Monitoring and Observability",
    "abstract": "<p>Description:</p>\n\n<p>We introduced distributed tracing, central logging with trace correlation and monitoring with Prometheus and Grafana in a large internationally distributed software development project from the beginning. The result: Nobody used it.</p>\n\n<p>In this talk we show the good and not so good sides we have learned while introducing and operating the observability tools. We show which extensions and conventions were necessary in order to carry out a cultural change and to awaken enthusiasm for these tools. Today the tools are a first-class citizen and people are shouting when they are not available.</p>\n\n<p>Intended Audience:\n- Beginner (Should know the basic terms of monitoring, tracing etc..)\n- Anyone who wants to establish monitoring, distributed tracing and central logging in a team as profitably as possible</p>",
    "description": "<p>Rough Outline:\n- Brief description of the used toolchain to understand the runtime behavior of a cloud-native software system and our motivation to set it up very early (development phase)\n- Describe the initial standard toolchain setup that does not work\n- Show the reasons why other team members does not use the toolchain and its drawbacks\n- Describe the implemented solution and the necessary improvements to the toolchain that lowers the hurdle to use the toolchain\n- Show the outcomes in terms of cultural change: Willing to understand the software system at runtime, write tickets with important details and shorter times to fix bugs</p>\n\n<p>Goals:\n- Understand that a widely used tool chain in a team can improve the quality of the software system and the work culture (no finger pointing)</p>",
    "persons": [
      "Florian Lautenschlager"
    ]
  },
  {
    "start": 1577893800000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Grafana-As-Code: Fully reproducible Grafana dashboards with Grafonnet",
    "subtitle": "",
    "track": "Monitoring and Observability",
    "abstract": "<p>Grafana configuration can nowadays be fully done as code, which enables code review, code reuse, and in general better workflows when working with dashboards.</p>\n\n<p>This talk will present Grafonnet, a Jsonnet library to generate Grafana dashboards and some tips and tricks about how to use it efficiently and how to manage fully your grafana instances from code. We will also explore how Jsonnet and Grafonnet enable collaboration on dashboards, using Mixins and explain how to push dashboards to Grafana, either using Kubernetes, or direct to the Grafana API.</p>",
    "description": "<ul>\n<li>Presentation of Grafana provisioning as code features</li>\n<li>Presentation of Grafonnet</li>\n<li>How to make reusable components with jsonnet</li>\n<li>How to include raw json</li>\n<li>How to link both (jsonnet -m to generate a folder)</li>\n<li>How to make PR efficient (generate grafana snapshots on merge requests)</li>\n<li>Jsonnet Mixins</li>\n</ul>",
    "persons": [
      "Julien Pivotto",
      "Malcolm Holmes"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 25,
    "room": "UD2.120 (Chavanne)",
    "title": "Monitoring of a Large-Scale University Network: Lessons Learned and Future Directions",
    "subtitle": "",
    "track": "Monitoring and Observability",
    "abstract": "<p>The complexity of network monitoring strongly depends on the size of the network under observation. Challenges in monitoring large-scale networks arise not only from dealing with a large volume of traffic, but also from keeping track of all traffic sources, destinations, and who-talks-to-whom communications. Analyzing this information allows to uncover new behaviors that would have not been visible by merely observing common metrics such as bytes and packets. The drawback is that extra pressure is put on the monitoring system as well and on the downstream data- and timeseries-stores.</p>\n\n<p>This talk presents a case study based on the monitoring of a large-scale university network. Challenges faced, findings, and lessons learned will be examined. It will be shown how to make sense of the input data to properly manage and reduce its scale as early as possible in the monitoring system. The discussion will also highlight the advantages and limitations of the opensource software components of the monitoring system. In particular, the opensource network monitoring tool ntopng and the timeseries-store InfluxDB will be considered. It will be shown what happens when ntopng and InfluxDB are pushed to their limits and beyond, and what it can be done to ensure their smooth operation. Relevant findings, behaviors uncovered in the network traffic, and future directions will conclude the talk. Intended audience is technical and managerial individuals who are familiar with network monitoring.</p>",
    "description": "<ul>\n<li>Main challenges of monitoring large-scale networks</li>\n<li>Case study based on the monitoring of a university network</li>\n<li>Availability, scalability and suitability of opensource software for network monitoring</li>\n</ul>",
    "persons": [
      "Simone Mainardi",
      "Tobias Appel"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 15,
    "room": "UD2.208 (Decroly)",
    "title": "Jitsi: video conferencing for the privacy minded",
    "subtitle": "Journalists, tinkerers, privacy concerned netizens, Jitsi may help you!",
    "track": "Real Time Communications",
    "abstract": "<p>Jitsi is a set of Open Source projects which provide state-of-the-art videocconferencing capabilities. In this presentation we will explore the Jitsi ecosystem from a privacy minded point of view.</p>",
    "description": "<p>Communicating privately via a public network (specially if we are using video) can be challenging. Jitsi provides the necessary tools to do so and we'll explore the seccurity model employed by our tools and how to setup a Jitsi instance with ease, while respecting your privacy.</p>",
    "persons": [
      "Saúl Ibarra Corretgé"
    ]
  },
  {
    "start": 1577870400000,
    "duration": 15,
    "room": "UD2.208 (Decroly)",
    "title": "Janus as a WebRTC \"enabler\"",
    "subtitle": "Having fun with RTP and external applications",
    "track": "Real Time Communications",
    "abstract": "<p>This talk will cover several aspects related to Janus as a WebRTC \"enabler\" for non-WebRTC applications. In particular, it will focus on the RTP management in Janus, namely how to use it as input/output to interact with external applications for different use cases.</p>",
    "description": "<p>Janus is an open source and general purpose WebRTC server. Its modular nature makes it easy to implement heterogeneous multimedia applications based on WebRTC, whether it's for conferencing, talking to a SIP infrastructure, broadcast a stream or interacting with an IoT device. One of its strongest points is the ability to seamlessly involve plain RTP within the context of a WebRTC communication, whether it's for feeding media to a WebRTC endpoint, or use a WebRTC stream somewhere else: this makes Janus a good WebRTC \"enabler\" for platforms that may not be aware of, or be compliant with, the WebRTC specification.</p>\n\n<p>This talk will cover the different features Janus provides implementers with, when it comes to RTP. In particular, it will introduce the Streaming plugin (RTP- and RTSP-to-WebRTC broadcaster), the SIP/NoSIP plugins (for legacy VoIP integration) and the so-called RTP forwarders (to relay media coming from WebRTC sources as plain RTP to external endpoints), and explain how these different components can be used together in different scenarios, whether it's just to increase scalability or to implement a complex and rich multimedia application. Besides, it will spend a few words on how simulcast, SRTP and recordings can be part of the picture.</p>",
    "persons": [
      "Lorenzo Miniero"
    ]
  },
  {
    "start": 1577871600000,
    "duration": 15,
    "room": "UD2.208 (Decroly)",
    "title": "Build your own ENUM server using CGRateS",
    "subtitle": "",
    "track": "Real Time Communications",
    "abstract": "<p>In this talk, Teo will explain how you can use CGRateS as an ENUM server together with other subsystems to achieve advanced ENUM functionality as number portability or least cost routing.\nCGRateS is a battle-tested Enterprise Billing Suite with support for various prepaid and postpaid billing modes.</p>",
    "description": "<p>In this talk, Teo will explain how you can use CGRateS as an ENUM server together with other subsystems to achieve advanced ENUM functionality as number portability or least cost routing.\nAs part of the recipe for such implementation, there will be few major CGRateS components exemplified: AttributeService ( combined with FilterService) which will be used to replace data from events arrives in CGRateS based on filters and SupplierService which can be used to select your desired provider based on different strategies such as least cost, highest cost, weight, load distribution or a mixed ( in case a two providers have the same cost if supplier was configured it can fall through on weight strategy automatically)\nCGRateS is a battle-tested Enterprise Billing Suite with support for various prepaid and postpaid billing modes.</p>",
    "persons": [
      "Teofil Voivozeanu"
    ]
  },
  {
    "start": 1577872800000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Linphone Instant Messaging Encryption",
    "subtitle": "Protocols' extension to existing SIP standards, implementation challenges and future extensions",
    "track": "Real Time Communications",
    "abstract": "<p>For many years, Linphone has been one of the most active free communication software. Originally focused on voice, aditionnal functionalities were rapidly added like video, group chat and presence. All of these communication modes imply privacy.</p>\n\n<p>To achieve a good level of privacy, users must be able to ensure that their communications can only be displayed to the receiver of those communications, especially no-one from server infrastructure crossed by the messages shall be in the position of compromising secrecy of the communication. Basically, this is what end-to-end encryption is aiming to achieve.</p>\n\n<p>Linphone does implement end-to-end encryption for voice and video communications thanks to ZRTP (rfc 6189). However, for messaging, security was only performed using point-to-point cyphering, based on SIP TLS. To bring users of instant messaging features the same level of security, we decided to implement end-to-end encryption mechanisms for messaging too, including group chat.  Linphone Instant Messaging Encryption follows state-of-the-art methods for forward secrecy and MitM detection.</p>\n\n<p>This discussion will focus on protocols' extension to existing SIP standards, implementation challenges and future extensions.</p>",
    "description": "",
    "persons": [
      "Elisa Nectoux"
    ]
  },
  {
    "start": 1577874300000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Collaboration between Free RTC projects",
    "subtitle": "Sharing resources for mutual benefit",
    "track": "Real Time Communications",
    "abstract": "<p>This session will discuss how we can share infrastructure and resources between projects, with the goals of reducing administrative burdens, reducing duplication of effort and increasing interoperability between our solutions.  To satisfy user expectations, interoperability is more critical in the field of real-time communications than any other free software eco-system.  In particular, we will look at how to share management of an event calendar, Planet sites, repositories, CI for interop testing and various other tools.</p>",
    "description": "",
    "persons": [
      "Daniel Pocock"
    ]
  },
  {
    "start": 1577875800000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "XMPP: get your shoping cart ready!",
    "subtitle": "Your guide through the candy store of XMPP extensions",
    "track": "Real Time Communications",
    "abstract": "<p>The eXtensibility of XMPP makes it extremely powerful. But it is easy to get lost in the supermarket of extensions. In this talk I will guide you though the extensions: how to read the labels? How to cook some common recipes? I will also show some exotic but nice ingredients. And last but not least: we will be having some protocol-fun!</p>",
    "description": "<p>The eXtensible Messaging and Presence Protocol, XMPP, has extensibility at its core. It is because of the extensibility that there is a vivid XMPP ecosystem: it is easy to adapt XMPP to new developments and to new use cases. The XMPP Standards Foundation maintains a list of extensions to XMPP. In this talk I will dive into this list of extensions: what kind of extensions are there? What statuses can an extension have? What extensions to use in some common use cases? And I will touch some exotic extensions for use cases you may never have thought about. Oh, and what is the story about these ‘humorous’ extensions, can a protocol be humorous?</p>",
    "persons": [
      "Winfried Tilanus"
    ]
  },
  {
    "start": 1577877300000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Crossing the Bifröst - Bridging All The Things with Matrix",
    "subtitle": "In which we bridge together as many comms systems as possible via Matrix",
    "track": "Real Time Communications",
    "abstract": "<p>Matrix is an open source project developing an open protocol and network for decentralised end-to-end-encrypted communication, providing a viable open alternative to the proprietary communication silos of WhatsApp, Slack, Discord and friends.  One of Matrix's main goals is to provide a highest-common-denominator open network which can bridge together existing communication silos.  In this talk, we'll show off Bifröst, our new application framework for building bridges, and demonstrate high quality bridging with XMPP, Slack, Discord, WhatsApp, and more!</p>",
    "description": "<p>Matrix owes its name to the idea of binding together existing communication platforms into an open 'matrix' in which they can interoperate.  Over the last year the wider Matrix ecosystem has been focusing increasingly on bridging, with lots of exciting development happening around:</p>\n\n<ul>\n<li>Bifröst - a TypeScript application framework for building bridges, including high quality XMPP support via xmpp.js and freeform protocol support via libpurple</li>\n<li>Slack - matrix-appservice-bridge, including bridging edits, reactions, threads, and full DM support via \"puppetting\"</li>\n<li>WhatsApp - mautrix-whatsapp, a fully functional WhatsApp bridge (using the API from WhatsApp Web)</li>\n<li>Discord - matrix-appservice-discord, with experimental puppetting support for full</li>\n<li>IRC - matrix-appservice-irc</li>\n</ul>\n\n\n<p>We'll explain all the flavours of bridging available these days (from bot-based, virtual users, puppetted, gatewaying etc) and show off Bifröst providing full gatewayed bridging between Matrix &amp; XMPP (so that anyone in Matrix can reach anywhere in XMPP and vice versa), as well as double-bridging (e.g. IRC&lt;->Slack via Matrix) - and generally illustrate how Matrix can be used to heal fragmented communities which have ended up shattered between Slack, Discord, IRC and other platforms.</p>",
    "persons": [
      "Matthew Hodgson"
    ]
  },
  {
    "start": 1577878800000,
    "duration": 15,
    "room": "UD2.208 (Decroly)",
    "title": "High quality VoIP platforms with Kamailio",
    "subtitle": "test driven development and debugging",
    "track": "Real Time Communications",
    "abstract": "<p>Kamailio as widely-used Open Source SIP Server is used to implement large and complex real-time communication platforms. Ensuring a good user experience, performance and quality can be a challenge in these enviroments. The talk will present different ways how to use the power of Kamailio to support you with a modern testing and debugging workflow.</p>",
    "description": "<p>Kamailio as widely-used Open Source SIP Server is used to implement large VoIP and real-time communication platforms. Ensuring a good user experience, perfomance and quality can be a challenge in these complex enviroments. The talk will present different ways how to use the power of Kamailio to support you with a modern testing and debugging workflow.</p>\n\n<p>Kamailio contains different modules that support you to test and debug your configuration. Different usage examples based on real world problems are used to explain them in detail. A usual process to find problems in code is to use a debugger to step through your code. Due to the architecture of Kamailio this setup needs a bit more preparation. Additional the talk will show how to step through your configuration language execution process to debug complicated logic.</p>\n\n<p>Testing config logic that involves different modules together can be challenging due to the many involved moving parts. Kamailio can support you here with a dedicated and easy to use component testing infrastructure. This test infrastructure is based on docker container, is freely available and can help also in your setup.</p>\n\n<p>Ensuring a good code quality is especially important for a stable and secure VoIP server. Kamailio is using different static code analyzer and other testing methods to improve the quality of the server. The talk will give you an overview about this tools and highlight how to use them by yourself.</p>",
    "persons": [
      "Henning Westerholt"
    ]
  },
  {
    "start": 1577880000000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "WebRTC for Android",
    "subtitle": "Creating a serverless communication app",
    "track": "Real Time Communications",
    "abstract": "<p>Develop a serverless communication app Android by leveraging the WebRTC protocol</p>",
    "description": "<p>I will share my experience of developing a serverless communication app for android using the WebRTC protocol using the official WebRTC library.</p>",
    "persons": [
      "Mohammad Murad"
    ]
  },
  {
    "start": 1577881500000,
    "duration": 15,
    "room": "UD2.208 (Decroly)",
    "title": "Explore your VoIP Network with SIP3",
    "subtitle": "Explore your VoIP Network with SIP3",
    "track": "Real Time Communications",
    "abstract": "<p>It's most likely that you have already heard about the SIP3 platform which allows you to monitor and troubleshoot your VoIP infrastructure.</p>\n\n<p>For the last year SIP3 team has grown and added tons of nice features into platform.</p>\n\n<p>This talk will tell about past, present and upcoming future of SIP3 and will be useful for both newcomer users and people who've played with SIP3 in the past.</p>",
    "description": "",
    "persons": [
      "Oleg Agafonov"
    ]
  },
  {
    "start": 1577882700000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "WebRTC isn't just for (video) conference calls",
    "subtitle": "",
    "track": "Real Time Communications",
    "abstract": "<p>WebRTC is showing up in many places: security cameras, babymonitors, games streaming, autonomous cars etc\nI'll describe the advantages of WebRTC in these devices but also the challenges of non-mainstream usage.\nI'll bring a demo or two and some sample code.</p>",
    "description": "",
    "persons": [
      "Tim Panton"
    ]
  },
  {
    "start": 1577884200000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Wazo Platform",
    "subtitle": "An Open Source Project to build your own IP Telecom Platform",
    "track": "Real Time Communications",
    "abstract": "<p>Learn what is Wazo Platform. How it evolved from Wazo and Xivo. What it is the vision for this Open Source project. How we leverage Asterisk, Kamailio and RTPEngine in this vision. How to get involved.</p>",
    "description": "",
    "persons": [
      "Mathias Wolff"
    ]
  },
  {
    "start": 1577885700000,
    "duration": 15,
    "room": "UD2.208 (Decroly)",
    "title": "HOMER 2020",
    "subtitle": "Meet the latest HOMER and its ground breaking features, and learn about our project vision for the future of HEP and RTC monitoring",
    "track": "Real Time Communications",
    "abstract": "<p>HOMER 2020: The future of the HEP Stack</p>",
    "description": "<p>Meet the latest HOMER and its ground breaking features, and learn about our project vision for the future of HEP and OSS RTC monitoring tool and technologies</p>",
    "persons": [
      "Lorenzo Mangani"
    ]
  },
  {
    "start": 1577886900000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Nextcloud Talk",
    "subtitle": "A real-time communication platform for teams",
    "track": "Real Time Communications",
    "abstract": "<p>Real-Time communication happens often in teams these days: at work, in your sports club, your Free Software project and in many other places. For a long time this was, and in many areas often still is, dominated by centralized and proprietary tools. While there are many great Free Software projects out there which fill the gap, Nextcloud Talk is unique as it ingrates in a complete collaboration platform. You can have your files, calendar, contacts, project plan and any other data in the same place where the communication takes place, all nicely integrated. You can edit collaboratively office documents (text, spreadsheets, presentations,...) or markdown files while having a chat, video- or audio call for example. This talk will introduce you to some of the unique features Nextcloud Talk can offer to your teams.</p>",
    "description": "<p>Nextcloud Talk is part of the Nextcloud collaboration platform. A complete Free Software and on-premise cloud solution which allows you to manage all kind of data and work collaboratively on it, both in one Nextcloud instance and even across different Nextcloud instances. Nextcloud Talk is based on WebRTC and offers a nicely integrated tool for text-, video- and audio-chats. As Nextcloud Talk is a first-class citizen in the Nextcloud platform it is well integrated with all the other components. You can share documents from Nextcloud Files into a room and every user will directly be able to see, sync and edit the files. You can work on a document with your team while have a call or chat, and many more things are possible. While you can find for every single task great Free Software tools these days. The uniqueness of Nextcloud Talk is the integration in one collaboration platform so that all the different areas can work nicely together, integrated in a way which often even outperform proprietary competitors. This talk will introduce the audience to all the possibilities of Nextcloud Talk and will show them how they can reach a new level of productivity for their group with the collaboration platform Nextcloud.</p>",
    "persons": [
      "Björn Schießle"
    ]
  },
  {
    "start": 1577888400000,
    "duration": 15,
    "room": "UD2.208 (Decroly)",
    "title": "Modern VoIP in Modern Infrastructures",
    "subtitle": "Designing and implementing VoIP architectures in the cloud and container era",
    "track": "Real Time Communications",
    "abstract": "<p>In the last years we have seen huge changes in IT infrastructures and concepts. VoIP architectures too are evolving towards Software Defined Telecoms. In this talk we'll see how VoIP solutions are being shaped by the Cloud, the open points and share some thoughts about its future.</p>\n\n<p>This is co-authored by Giacomo Vacca and Federico Cabiddu.</p>",
    "description": "",
    "persons": [
      "Giacomo Vacca"
    ]
  },
  {
    "start": 1577889600000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Migrating reSIProcate to C99 stdint types",
    "subtitle": "",
    "track": "Real Time Communications",
    "abstract": "<p>reSIProcate was developed over many years using a custom set of fixed-width data types defined in one of the headers, compat.hxx. The C99 stdint header provides a more standard solution. I talk about how we experimented with replacing the types throughout the entire stack and the consequences for users.</p>",
    "description": "",
    "persons": [
      "Izabela Bakollari"
    ]
  },
  {
    "start": 1577891100000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Introducing Falconieri: Remote Provisioning Service as a Service",
    "subtitle": "A new, modern, open source and cloud native remote provisioning service gateway.",
    "track": "Real Time Communications",
    "abstract": "<p>Remote Provisioning Service is a service offered by phones vendors for easily and quickly provide a configuration for a phone.\nDespite the advantages of have a phone ready to use without any specific network configuration  (except for a internet connection), there are some drawbacks like different APIs for any vendors.\nFalconieri try to unify all the vendors specific APIs under a set of HTTP rest APIs.</p>",
    "description": "<p>In this talk will be shown the motivations beoynd the creation of Falconieri and the technical choises.\nWill be illustrated also the APIs of the four currently supported vendors:</p>\n\n<ul>\n<li>SNOM</li>\n<li>Gigaset</li>\n<li>Yealink</li>\n<li>Fanvil</li>\n</ul>",
    "persons": [
      "Matteo Valentini"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 15,
    "room": "UD2.208 (Decroly)",
    "title": "Asterisk: A Project Update",
    "subtitle": "",
    "track": "Real Time Communications",
    "abstract": "<p>This talk will be about what's happened this last year in the world of Asterisk, including what's happened in the latest major release of Asterisk (Asterisk 17) as well as a discussion of some of the more recent developments that have happened since 17's release.</p>",
    "description": "<p>This talk will be about what's happened this last year in the world of Asterisk, including what's happened in the latest major release of Asterisk (Asterisk 17) as well as a discussion of some of the more recent developments that have happened since 17's release.</p>",
    "persons": [
      "Matthew Fredrickson"
    ]
  },
  {
    "start": 1577893800000,
    "duration": 15,
    "room": "UD2.208 (Decroly)",
    "title": "Chat Over IMAP (COI): State of the Union",
    "subtitle": "When will messaging via e-mail crash the monopolies?",
    "track": "Real Time Communications",
    "abstract": "<p>With the free &amp; open COI standard we enable every mail user to chat via email. We presented this idea last year, have in the meantime launched the COI plugin of the Dovecot IMAP server and the OX COI Messenger app. In this talk you will learn how the basic idea evolved over time, what we have learned during the journey, where we are heading to and: Why and how you should join us on that trip.</p>",
    "description": "<p>We presented the idea on last year's FOSDEM: Instead of trusting companies like Facebook, Tencent and rely on their infrastructure for personal communication, we wanted to create a new chat ecosystem based on open standards and federated infrastructure. As much as we like privacy-entered messengers like Signal, it's just another silo and you have to trust the provider.\nIn the end all popular messaging services today are proprietary, closed and operated by single providers.\nThis has unwilling consequences we are not willing to accept anymore:\nYour provider knows all about your social network: when you communicate with whom, the frequency of your communication and the number, type and length of your messages – even with end to end encryption in place.\nYou are locked in as a user: You cannot simply change to another provider, because your friends and peers are also using your current provider. If you want to switch you would need to convince everyone to follow. And of course you would lose all your conversation history if you dare to move away.\nThe network is only controlled by a single party, you cannot start hosting your own WhatsApp, WeChat or FB Messenger service. They set the rules and usually do not give you access to the API for creating your own software.\nSo why not taking an existing network based on open protocols which is already used by almost everyone? Why not using.... email? It already provides a federated infrastructure and is based on open standards like SMTP and IMAP. Why not building build a chat ecosystem on top of it? We called it COI - Chat Over Imap.\nThis was the initial idea and some key players joined the initiative around this idea right from the beginning: Dovecot (the most popular IMAP server), DeltaChat (an open source email based messenger) and Open-Xchange (an open source email technology and service provider) joined forces and kicked of COI - Chat Over IMAP.\nThe Dovecot team started working on extending the existing IMAP protocol and build some services on top to allow encrypted push notifications, to reduce latency etc. The DeltaChat developers worked on improving the client's core and adjust it to the needs of the Open-Xchange team who worked on compatible Flutter-based clients for Android and iOS. Of course everything was a little more complicated than most of us thought, but in the end we made it: In October 2019 we presented beta versions of iOS and Android COI Messenger clients and we introduced the COI plugin for Dovecot.\nIn this talk we will share the main challenges we have been facing while adding real-time messaging features to IMAP and how we solved them. And we will give insights into the problems we ran into in client development where we had to combine a Rust-based DeltaChat-Core code base with Flutter based mobile UIs and platform-specific native features.<br/>\nWe would also like to share with you what the main outstanding challenges are that might still stop us from being the WhatsApp killer we'd love to be.\nAnd of course we would love to encourage you to join the party.</p>",
    "persons": [
      "Robert Virkus"
    ]
  },
  {
    "start": 1577895000000,
    "duration": 20,
    "room": "UD2.208 (Decroly)",
    "title": "Reach for the Clouds With OpenSIPS 3.0",
    "subtitle": "A major release focused on the DevOps mindset",
    "track": "Real Time Communications",
    "abstract": "<p>Tune in and get up-to-date with the philosophy and features behind the major\nversion shift in the latest OpenSIPS 3.0 release.</p>",
    "description": "<p>OpenSIPS 3.0 embraces the cloud movement with open arms and aims to be easier to deploy and\na lot more enjoyable to maintain.  You can now automate routing logic updates to hundreds of\ninstances with a single click, thanks to the long-awaited ability to reload the routing script which\nis finally here!</p>\n\n<p>Individual running instances of OpenSIPS 3.0 are now capable of automatically upscaling or downscaling\nthemselves according to the volume of traffic that is running through them.  By auto-forking more SIP\nworker processes during peak day hours and un-forking them at night, OpenSIPS 3.0 maintains steady\nperformance while minimizing the costs of the cloud instances which host it.</p>\n\n<p>Say goodbye to \"opensipsctl\" and meet the new Python3-based tool for managing 3.x instances:\n\"opensips-cli\".  While retaining the majority of features of its predecessor, opensips-cli builds\nupon them, adding an intuitive way of interacting with multiple instances, the ability to filter, trace\nand troubleshoot specific calls which take place on any of them or to ask for an instance\ndiagnosis, which will instantly pinpoint issues such as insufficient memory, slow SQL/NoSQL service,\nslow DNS service, overflowing UDP queues and many more!</p>\n\n<p>Presented by Liviu Chircu and <a href=\"https://archive.fosdem.org/2019/schedule/speaker/razvan_crainea/\">Răzvan Crainea</a></p>",
    "persons": [
      "Liviu Chircu"
    ]
  },
  {
    "start": 1577869200000,
    "duration": 10,
    "room": "UD2.218A",
    "title": "How many engineers does it take to change an IOT light bulb?",
    "subtitle": "Welcome to the IOT devroom 2020",
    "track": "Internet of Things",
    "abstract": "<p>Welcome to the IOT devroom</p>",
    "description": "<p>How many engineers does it take to change an IOT light bulb?</p>\n\n<p>Let's discuss the Philips Hue architecture.</p>\n\n<p>\"Our new cloud \"maintains a permanent, open socket connection into every Hue home, 24x7, everywhere in the world to obtain real-time performance\"\n\"- Holy fcking what?\"</p>",
    "persons": []
  },
  {
    "start": 1577869800000,
    "duration": 20,
    "room": "UD2.218A",
    "title": "Checkpointing in a real time OS for transiently-powered embedded systems",
    "subtitle": "Checkpointing in a real time OS for transiently-powered embedded systems (MSP430)",
    "track": "Internet of Things",
    "abstract": "<p>Some constrained embedded systems cannot use batteries, those are called transiently-powered embedded systems.\nThey can be equipped with a non volatile RAM (NVRAM) and a super capacitor for gathering energy when available.\nDeveloping in such an environment is not straight.\nDuring this talk, we will expose our work on a constrained real time OS tolerant with power loss on a MSP430 based platform.\nThis platform is a MSP430FR5994 Launchpad equipped with FRAM and a super capacitor.</p>",
    "description": "<p>This talk deals with constrained transiently-powered embedded systems equipped with non volatile RAM (NVRAM).\nTransiently-powered systems may be autonomous sensors, sensor networks, mobile devices without batteries, systems where the use of batteries is infeasible.\nIn our case, such systems are equipped with a super capacitor that gathers energy and restitutes that energy to the system for a short time (from a few tens of milliseconds to a few minutes).\nProgramming on such a device is difficult because it operates only intermittently, as energy is available.\nSuch devices must not reboot but continue their computations all along successive powered periods, meaning that they must keep their states and values even when loosing power.\nUsing NVRAM may seem to be an easy solution, but that is not the case. Indeed, using NVRAM as a kind of RAM is likely to lead the system to an altered behaviour, an inconsistent state.</p>\n\n<p>Our motivation is to provide a transiently-powered computing platform to accommodate both usual sensing and transmission functions as well as as heavy as possible computations aka edge computing.\nThis platform shall abstract the use of NVRAM or at least assist the user in the design of its application.</p>\n\n<p>Our based test platform is an Autosar compliant open source real time OS, Trampoline (https://github.com/TrampolineRTOS/trampoline), already used inside high-end vehicles.\nTrampoline is very light, configurable, suitable for constrained devices.\nWe have made some experimentations on a MSP430FR5994 platform, it is equipped with a MSP430 and a 0.22F super capacitor.\nOur device shall start just once, and then restart from the last checkpoint on each power recovery.</p>\n\n<p>During this talk we will present the current state of our experimentations.\nThat is to say :\n- Real-time and estimation of remaining energy;\n- Task scheduling model for such devices;\n- Platform energy consumption models: start and restart;\n- Peripherals states and initializations.</p>",
    "persons": [
      "David Garriou"
    ]
  },
  {
    "start": 1577871000000,
    "duration": 20,
    "room": "UD2.218A",
    "title": "Building composable IOT toolsets with Docker, Node-Red and OpenOCD",
    "subtitle": "Building composable IOT toolsets with Docker, Node-Red and OpenOCD",
    "track": "Internet of Things",
    "abstract": "<p>We will demonstrate how to quickly develop simple tools to: check for a GPIO state, communicate with an I2C OLED screen or gdb into a running firmware.</p>",
    "description": "<p>Thanks to <a href=\"https://hub.docker.com/r/multiarch/qemu-user-static/\">multiarch/qemu-user-static</a>, it is now trivially easy to build arm32 docker images from any workstation or server. These images can be shared on public repositories like any other x86 image.</p>\n\n<p>Single board computers such as the Raspberrry Pis have built in Linux primitives for basic manipulations of GPIOs and ports (I2C, UART, SPI) such as <code>/sys/class/gpio</code> or <code>/dev/i2c*</code>.</p>\n\n<p>Node-RED allows to visually develop simple workflows that can leverage the kernel primitives but also other packages like OpenOCD.</p>\n\n<p>Most of the presentation will be live with just a few stops on GitHub or the Docker Hub. No slides should be necessary.</p>",
    "persons": [
      "Dimitri del Marmol (ddm)"
    ]
  },
  {
    "start": 1577872200000,
    "duration": 20,
    "room": "UD2.218A",
    "title": "Making a robot controller from scratch",
    "subtitle": "With NuttX, IoT.js, WebThing and more",
    "track": "Internet of Things",
    "abstract": "<p>Technical barrier to target low cost micro controllers can be too high for many developers already used to high level API. But did you know that those devices can support many operating systems like NuttX inspired by POSIX (same for Linux), but it goes even behind than C APIs, even JavaScript runtimes like IoT.js can be supported too. IoT.js can also support JS community modules such as Generic-sensor-lite to support sensors and actuators or webthing-iotjs to enable REST API for embedded applications.</p>",
    "description": "<p>A Robot demonstration running on IoT.js will be explained from porting task to support new STM32F7 board, generate PWM signal to handle servo motors and also providing REST API for \"Web Of Things (WoT)\" applications. To illustrate \"Digital Twins\" concept the robot can interact with a WebVR application using A-Frame framework bridged to Mozilla webthing protocol.</p>",
    "persons": [
      "Philippe Coval"
    ]
  },
  {
    "start": 1577873400000,
    "duration": 20,
    "room": "UD2.218A",
    "title": "AI at the edge with Tensorflow Lite to Design the Future of Vertical Farming",
    "subtitle": "",
    "track": "Internet of Things",
    "abstract": "<p>While Machine Learning is usually deployed in the cloud, lightweight versions of these algorithms that fit for constrained IoT systems such as microcontrollers are appearing.\nUsing Machine Learning « at-the-edge » has indeed several advantages such as the reduction of network latency, it provides better privacy, and are working offline.\nIn this presentation, we will demonstrate how to deploy Deep Learning algorithms on IoT devices thanks to TensorFlow Lite. We will see how to use it to design a smart vertical farming system able to predict and optimize the plant growth, at home or in developing countries where a reliable Internet connection still is missing.</p>",
    "description": "<p>In this talk I will show how trending technologies like IoT, Machine Learning and Tensorflow can make the world better :)\nI will discuss how we can use Tensorflow Lite on IoT and evaluate its performances and limits.\nI will explain our use case in vertical farming, show code snippets and make some short demo.</p>\n\n<p>Summary:</p>\n\n<ul>\n<li>Vertical farming use case, how to use technologies to try stopping hunger in poor countries</li>\n<li>Our IoT system for data collection: data collection, electronics and embedded system</li>\n<li>Tensorflow Lite</li>\n<li>How we use Tensorflow Lite on RPI</li>\n<li>Short Demo</li>\n<li>Performance evaluation, limits, further work</li>\n</ul>",
    "persons": [
      "Alexis DUQUE"
    ]
  },
  {
    "start": 1577874600000,
    "duration": 20,
    "room": "UD2.218A",
    "title": "Sphactor: actor model concurrency for creatives",
    "subtitle": "Sphactor: actor model concurrency for creatives",
    "track": "Internet of Things",
    "abstract": "<p>We propose a combined visual and text-based programming environment based on the actor model suitable for novice to expert programmers. This model encompasses simple communicating entities which easily scale from utilizing threads inside the computer to massive distributed computer systems. Our proposal is very suitable for IOT scenarios, creative coding practices and rapid prototyping. The prototype utilizes zeromq transports and embeds python for easy creation of actors.</p>",
    "description": "<p>Sphactor is currently a research project for a framework for concurrent programming suitable for novice users while maintaining features needed by expert programmers. The library features an actor model at its core and features a GUI application to manage actor dependencies visually and also program individual actors using a classical text based approach.</p>\n\n<p>One of the initial questions for Sphactor was the fact that when students want to access new technologies they often need to be experienced programmers. However this is hardly ever the case. As an example; students need to access motion capture sensors however only an SDK is provided. We can overcome this hurdle by adding some software to make this more accessible. We found that most tools used by students in our academy can utilize the OSC (Open Sound Control) out of the box. Therefore we started transmitting sensor data through OSC. This has proven to be very comfortable for students. We then ran into the situation that for every technology we needed to develop a piece of software to translate its features to OSC. To prevent creating a jungle of tools we started researching how we could create a general intermediate layer between technologies and use OSC as a transport. This is a common question in the IOT world.</p>\n\n<p>Continued research showed us that students, using existing tools and frameworks were hardly ever utilizing all processors in their machines. This is due to the fact that tools they operate are only designed for single threaded situations. Tools utilizing all processors are very rare, especially for novice users. Message passing is one of the fundamental models for concurrent programming and is actually very similar to what we were already doing in our intermediate software layer and in common IOT scenarios when we are processing all our sensor data.</p>\n\n<p>These situation are driving the development of Sphactor. We currently have a prototype ready for testing which we will demonstrate and talk about. Sphactor is being researched by the HKU University of the Arts Utrecht in the Netherlands for use in creative processes and as a educational environment for programming and interacting with new technologies. Libsphactor is developed in C using Zeromq's czmq framework. The gui is done is C++ using an Immediate Mode UI with minimal dependencies.</p>\n\n<p>This research project is a continuation of research which was presented in the FOSDEM IOT devroom in 2015 and 2016.</p>",
    "persons": [
      "Arnaud Loonstra"
    ]
  },
  {
    "start": 1577875800000,
    "duration": 20,
    "room": "UD2.218A",
    "title": "Zyre: p2p messaging to fuck the cloud",
    "subtitle": "Pieter Hintjens last IOT project",
    "track": "Internet of Things",
    "abstract": "<p>Pieter Hintjens last IOT project, running OpenWRT and the Zyre p2p library.</p>",
    "description": "<p>Zyre has the potential to change the cloud paradigm, with auto-discovery inside the LAN, and without any requirement of an internet connectivity.</p>\n\n<p>Your TV can then discover your fridge.</p>\n\n<p>3 years ago, I was working with Pieter Hintjens, main author of ZeroMQ and organiser of the IOT devroom, on a demo setup for his last conference at IOT Munich.</p>\n\n<p>When he came back from Munich, he was diagnosed with terminal lung cancer.</p>\n\n<p>We demonstrated a pile of OpenWRT routers blinking LED lamps in an orchestrated way.</p>\n\n<p>Most of my work included assembling the hardware, writing OpenWRT packages, making a CI/CD for the devices.</p>\n\n<p>I will show how to build OpenWRT packages with new cluster technologies, like Kubernetes.</p>",
    "persons": [
      "Benjamin Henrion (zoobab)"
    ]
  },
  {
    "start": 1577877000000,
    "duration": 20,
    "room": "UD2.218A",
    "title": "How to build Webthings?",
    "subtitle": "Interact with Mozilla IoT gateway",
    "track": "Internet of Things",
    "abstract": "<p>Mozilla Webthing is a smart home platform built with Privacy by Design. It is an implementation of Web of Things concepts specified by W3C. The presentation explains how to create new things and interact with gateway using addon adapters.</p>",
    "description": "<p>Mozilla's WebThing schemas specify many IoT devices. They are also flexible enough to describe any devices using generic types. Standalone devices can be interacted with using WebThings REST API or using the Mozilla gateway. The gateway is designed to be extensible using addon adapters. A couple of my contributions involving sensors and virtual things will be demonstrated. Recipes will be shared to build your own adapters using your favorite language (Js, Python, etc). The addons can easily be deployed on your RaspberryPi for self-hosted home automation.</p>",
    "persons": [
      "Christian Paul"
    ]
  },
  {
    "start": 1577878200000,
    "duration": 20,
    "room": "UD2.218A",
    "title": "Astarte: A Data-First approach to IoT",
    "subtitle": "Astarte: A Data-First approach to IoT",
    "track": "Internet of Things",
    "abstract": "<p>Even though the IoT buzz has been around for years, ecosystems are still scattered and developers must usually patch together a number of solutions to achieve their goals. Astarte is a free software, opinionated \"blackbox\" solution which aims at empowering developers with a platform which puts Data as the first-class citizen rather than focusing on Device-to-Cloud communication, and can scale to production-tier deployments easily.</p>",
    "description": "<p>Astarte enables developers to skip the details of all the plumbing in IoT data collection, and skip straight to easy-to-use mechanisms for harnessing data produced by IoT devices through analytics, AI, or simply visualisation.\nThe talk will go over Astarte's design and architecture, both from a plumbing perspective and from the daily developer usage. The live demo will show how to install Astarte in a Kubernetes Cluster, set up a Device (real or a simulator) to interact with Astarte, and build a minimal web application to interact with it - all in the timespan of the talk.</p>",
    "persons": [
      "Dario Freddi"
    ]
  },
  {
    "start": 1577879400000,
    "duration": 20,
    "room": "UD2.218A",
    "title": "Building IoT solutions with Eclipse IoT technology",
    "subtitle": "Building IoT solutions with Eclipse IoT technology",
    "track": "Internet of Things",
    "abstract": "<p>The  IoT working group within the Eclipse Foundation is a joint effort to develop generic building blocks for creating IoT solutions. As of now, they host over 30 projects, which address different aspects of the realization of IoT use cases. The vast number of projects allow the design of tailored IoT solutions but bear the risk that people get lost in the wide range of projects. The recently introduced project Eclipse IoT packages will help here. It aims to provide pre-bundled software packages for the IoT.</p>\n\n<p>In this talk, we are going to introduce the work of the Eclipse IoT working group and showcase selected projects with the focus on how one can use those technologies to build custom domain specific IoT solutions. Specifically, we will focus on the message hub Eclipse Hono,, the digital twin solution Eclipse Ditto, the update manager Eclipse hawkBit and Eclipse Vorto a description language for IoT devices. We further plan to show how one can combine and use the projects in a sensible way within the Eclipse IoT packages project.</p>",
    "description": "<p>Building your own backend for an IoT based solution can be difficult, as one needs to solve a number of challenges like among others connecting large number of devices in a scalable way, abstracting the access to the device and managing the software on the device. Within the Eclipse Foundation, developers contributes to over 30 projects to tackle the aforementioned and further issues. Various partners drive this working group from academia and industry like the strategic partners RedHat, Eurotech and Bosch. In this talk we intend to give an overview of the ongoing work in the working group and want to showcase some of the projects that are developed in that context. Namely, we plan to focus on the following projects:</p>\n\n<p>Eclipse Hono – A scalable message hub for connecting a larger number of IoT devices to the cloud supporting various protocols</p>\n\n<p>Eclipse Ditto – Abstracting the access to the state and actions of the physical IoT device in the digital world. This is done, by the provision of HTTP and WebSocket endpoints that can be used by other applications in order to access the data of the device or send commands to the device.</p>\n\n<p>Eclipse hawkBit – Manage the software version of an IoT device and manage campaigns for rolling out new software updates.</p>\n\n<p>Eclipse Vorto – Description language for stating the capabilities and features of an IoT device. The Eclipse Vorto models can then be used by other tools e.g. to provide an API for the features or generate implementations that are already integrated with the backend.</p>\n\n<p>Based on those projects it is possible to build your own IoT solution for which we will give examples during this talk. Moreover, we will introduce the new Eclipse IoT packages project, which aims to provide pre-bundled packages of Eclipse IoT projects. The aim of creating those packages is to support developers who want to leverage Eclipse IoT technology in their specific context or domain.</p>",
    "persons": [
      "Sven Erik Jeroschewski"
    ]
  },
  {
    "start": 1577880600000,
    "duration": 20,
    "room": "UD2.218A",
    "title": "IoT Updates with IPv6 Multicast",
    "subtitle": "Updating a Billion Nodes from One Tiny Server",
    "track": "Internet of Things",
    "abstract": "<p>Could we update a billion IoT nodes from just one tiny virtual server?</p>\n\n<p>Could a server that is behind a completely closed inbound firewall, using no caching, no CDNs and which never accepts any inbound traffic communicate at massive scale?</p>\n\n<p>How can we handle flow control, with no feedback mechanism?</p>",
    "description": "<p>Lets find out!</p>",
    "persons": [
      "Brett Sheffield"
    ]
  },
  {
    "start": 1577881800000,
    "duration": 20,
    "room": "UD2.218A",
    "title": "IoT with CircuitPython",
    "subtitle": "Look mam, no development environment.",
    "track": "Internet of Things",
    "abstract": "<p>Introduction to CircuitPython and how to make basic IoT without a development environment.</p>",
    "description": "<p>A brief history of CircuitPython\nCircuitPython vs MicroPython</p>\n\n<p>Hello World demo:\n1. Hello World in REPL\n2. Hello World in a Python script\n3. Blink (the electronic Hello World)\n4. Cheerlights (the internet connectivity Hello World)\n5. Hide and Seek (a BLE Hello World?)</p>\n\n<p>Circuit Python supported hardware used for the IoT demo:\n* nRF52840 (Nordic Semiconductor) with build-in BLE\n* ATSAMD51 (Microchip) M4 with Airlift (ESP32 used as a Wifi Co-Processor)</p>",
    "persons": [
      "David Glaude"
    ]
  },
  {
    "start": 1577883000000,
    "duration": 10,
    "room": "UD2.218A",
    "title": "PSLab.io",
    "subtitle": "Pocket Science Lab",
    "track": "Internet of Things",
    "abstract": "<p>PSLab is a small USB powered iOT board to do measurements. It comes with slots for ESP WiFi chips and Bluetooth and can be used as hardware extension for Android phones or PCs. PSLab has a built-in Oscilloscope, Multimeter, Wave Generator, Logic Analyzer, Power Source, and we are constantly adding more digital instruments. To start measuring, connect two wires to the relevant pins and use the Android or desktop apps to view and collect the data. You can also plug in hundreds of compatible I²C standard sensors to the PSLab pin slots or even control robots with the robotic arm tool.</p>",
    "description": "<p>PSLab is a small USB powered iOT board to do measurements. It comes with slots for ESP WiFi chips and Bluetooth and can be used as hardware extension for Android phones or PCs. PSLab has a built-in Oscilloscope, Multimeter, Wave Generator, Logic Analyzer, Power Source, and we are constantly adding more digital instruments. To start measuring, connect two wires to the relevant pins and use the Android or desktop apps to view and collect the data. You can also plug in hundreds of compatible I²C standard sensors to the PSLab pin slots or even control robots with the robotic arm tool.</p>",
    "persons": [
      "Mario Behling"
    ]
  },
  {
    "start": 1577883600000,
    "duration": 10,
    "room": "UD2.218A",
    "title": "Erlang and Elixir on IoT devices using AtomVM",
    "subtitle": "Boost your IoT project with functional languages",
    "track": "Internet of Things",
    "abstract": "<p>This talk will present AtomVM, a tiny portable virtual machine that allows Elixir and Erlang code to run on microcontrollers with less than 500KB of RAM such as the ESP32 or several STM32.</p>",
    "description": "<p>Erlang and Elixir are really good at handling network packets, running concurrent processes, dealing with faults, writing testable software and enabling rapid development.\nAll this features are relevant for any language or framework that aims to IoT devices, therefore Erlang and Elixir might be good choice for IoT devices. However the BEAM, which is the standard Erlang and Elixir VM, cannot fit on those devices. AtomVM aims to overcome this limitation, by implementing a tiny virtual machine from scratch.</p>\n\n<p>This talk aims to present AtomVM and to show how a functional language such as Elixir might boost development of IoT projects.</p>",
    "persons": [
      "Davide Bettio"
    ]
  },
  {
    "start": 1577884200000,
    "duration": 20,
    "room": "UD2.218A",
    "title": "IOT Lightning Talks",
    "subtitle": "Show us your IOT pet project, 5mins each, don't be shy",
    "track": "Internet of Things",
    "abstract": "<p>Show us your IOT pet project, 5mins each, don't be shy</p>",
    "description": "<p>Show us your IOT pet project, 5mins each, no need to register, if you have a proposal, send a quick email to zoobabATgmail.com.</p>",
    "persons": []
  },
  {
    "start": 1577885400000,
    "duration": 30,
    "room": "UD2.218A",
    "title": "IoT Projects in FLOSS Foundations",
    "subtitle": "A report based on communities data",
    "track": "Internet of Things",
    "abstract": "<p>A data based analysis of IoT Projects in FLOSS Foundations</p>",
    "description": "<p>In the last decade, Industry 4.0 has emerged as a revolution for the traditional technology, and the Internet of Things (IoT) is at the core of it. Apache, Eclipse and Linux foundations, three of the main actors in Open Source, have put in place their own IoT architectures powered by different Open Source projects. In this talk, these architectures are compared and a common architecture is identified based on emerging standards, with a special focus in the Edge. Then, the common architecture is used to classify the different Open Source projects.</p>\n\n<p>For each project, activity and community analysis based on the data extracted from Git and Github issue trackers is achieved using the GrimoireLab platform, a powerful Open Source tool for software analytics. Finally, the data obtained is used to understand the Open Source IoT landscape in terms of companies involved, leading projects, technologies adopted and communities.</p>\n\n<p>A total of 55 projects have been analyzed and all of them are classified in the categories: Edge, Cloud, Enterprise, Tools. And inside the Edge category, five subcategories are defined: OS and virtualization in devices, communication protocols, data processing, platforms for interoperability and applications. For all the projects the data for the activity (commits) and community size (people doing commits) are extracted and analyzed in time series.</p>\n\n<p>The data will be presented as dashboards that all attendees can consult online and all the data could be shared with interested people for further analysis.</p>\n\n<ul>\n<li><a href=\"https://www.linkedin.com/in/acslinkedin/\">Alvaro del Castillo</a></li>\n<li><a href=\"https://www.linkedin.com/in/valerio-cosentino-29980121\">Valerio Cosentino</a></li>\n</ul>",
    "persons": [
      "Alvaro del Castillo"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 30,
    "room": "UD2.218A",
    "title": "Kubernetes of Things",
    "subtitle": "Case-study building sensors and actors as CRDs",
    "track": "Internet of Things",
    "abstract": "<p>Kubernetes allows to extend its API surface with custom objects called CustomResources (CR) whose JSON contents is described via OpenAPI schemas. The REST API allows realtime notification of changes sent out to multiple listeners. This sounds like the ingredients necessary to build an open IoT platform. This talk is about using CustomResources for Kubernetes as Things, i.e. namespaced objects representing sensors and actors. It is based on a case-study implementing this idea following an example of a deep sea station with valves and pumps, a controller controlling the air pressure in the station.</p>",
    "description": "",
    "persons": [
      "Stefan Schimanski"
    ]
  },
  {
    "start": 1577889000000,
    "duration": 30,
    "room": "UD2.218A",
    "title": "Insight Fieldtracks",
    "subtitle": "Tracking firefighters, medics & actors during field exercises",
    "track": "Internet of Things",
    "abstract": "<p>Understanding the course of a drill is important for both steering and evaluation. Fieldtracks implements localization and tracking based on BLE, ESP32 and mesh networking for indoor and outdoor environments. This talk will introduce you into fieldtracks and its challenges.</p>",
    "description": "<p>Back in 2018, we started implementing a small prototype for indoor tracking using inexpensive and compact Espressif ESP32 devices. Beaconing and scanning at the same time, results are transmitted using MQTT for real time analysis and recording.</p>\n\n<p>The first results looked promising and shifted the focus towards user-experience (UX) and visualization. An Angular client allows accessing data in real-time, utilizing a distributed on-site network integrated into the cloud. On-site, UX also concerns the rapid tactical deployment of mesh networks utilizing various links (IEEE 802.3 / 802.11, ITU G.992.5 aka DSL).</p>\n\n<p>The initial deployment happened in September 2019. It has demonstrated the general feasibility and outlined the room for a lot of UI an UX improvements, from which the need for expertise and new contributors has arisen.</p>\n\n<p>Let's have a look at the challenges we encountered building the network and see, why it's exciting to build an IoT-network to be used in the field. We would like to invite you to work with us and welcome some networking later on.</p>",
    "persons": [
      "yanosz"
    ]
  },
  {
    "start": 1577890800000,
    "duration": 30,
    "room": "UD2.218A",
    "title": "WolfBoot",
    "subtitle": "Secure boot and remote updates",
    "track": "Internet of Things",
    "abstract": "<p>Firmware updates in IoT pose a new set of security risks. Secure bootloaders can be handy to deploy new versions of the firmware on those devices that are only reachable through a remote connection.</p>\n\n<p>wolfBoot is a portable, GPL, OS-agnostic, secure bootloader solution for 32-bit microcontrollers, relying on wolfCrypt for firmware authentication, providing secure firmware update mechanisms.</p>\n\n<p>Due to the minimalist design of the bootloader and the tiny HAL API, wolfBoot is completely independent from any OS or bare-metal application, and can be easily ported and integrated in existing embedded software projects to provide a secure firmware update mechanism.</p>\n\n<p>This presentation will focus on the implementation details and the design choices of the project, and the porting done to ARM Cortex-M and RISCV32 microcontrollers.</p>",
    "description": "",
    "persons": [
      "Daniele Lacamera"
    ]
  },
  {
    "start": 1577892600000,
    "duration": 30,
    "room": "UD2.218A",
    "title": "Using Micropython to develop an IoT sensor platform with an Augmented Reality UI",
    "subtitle": "How to marry the physical world and IoT with the virtual",
    "track": "Internet of Things",
    "abstract": "<h1>IoT with Augmented Reality</h1>\n\n<p>Anyone with a curious mind and interested in how to marry the physical world and IoT with the virtual. This talk is pitched at intermediate, but for beginners extensive documentation and a github repo is available to read and learn from. It shows the tiny <a href=\"https://pybd.io/\">Micorpython Pyboard 'D'</a> being used as a tiny web server serving up an Augmented Reality display. The display shows sensor data overlay on a marker used to identify the IoT device. The work is open sourced from a project being run at the Samsung Research Institute in the UK.</p>",
    "description": "<h1>Using Micropython to develop an IoT multimode sensor platform with an Augmented Reality UI</h1>\n\n<h2>Who is this for</h2>\n\n<p>Anyone with a curious mind and interested in how to marry the physical world and IoT with the virtual. This talk is pitched at intermediate, but for beginners extensive documentation and a <a href=\"https://github.com/SamsungResearchUK-IoT-Meetup/multimode_sensor_platform/wiki\">github repo</a> is available to read and learn from.</p>\n\n<h2>What is it about</h2>\n\n<p>The story of a project at Samsung to capture data for the purpose of using in AI projects, which then became an OS platform with innovative ideas more important that the original concept. A multimode sensor platform using Augmented Reality for visualization.</p>\n\n<h2>Will I See The Real Prototype</h2>\n\n<p>Yes there will be a demonstration of the prototype and this will be used as part of the talk to explain the concepts.</p>\n\n<h2>What Technologies Will It Cover</h2>\n\n<p>This talk will cover a microcontroller with plug-able sensors using <a href=\"http://www.micropython.org/\">micropython</a> for the main software development platform. For the serve side components the talk will be using a micro web server used to serve sensor data and a simple AR display. For Augmented Reality the talk will switch to using HTML and JavaScript needed to render objects in the browser for the demo.</p>\n\n<h2>What Will I Learn</h2>\n\n<p>Hopefully you will learn an introduction to IoT concepts to developers who are interested in this tech, but we will use <a href=\"http://micropython.org/\">micropython</a> for embedded systems and show how to used. You will see in this talk:\n1. How to use a tiny microcontroller with real sensors using <a href=\"http://www.micropython.org/\">micropython</a>.\n2. How to connect 'things' to the platform.\n3. A micro web server to serve content.\n4. The web side, using web technologies like A-Frame, AR.js for the purpose of having AR capabilities.</p>\n\n<h2>In Summary</h2>\n\n<p>This talk will cover going from the small embedded world of IoT and sensors using micropython. Some time will be given to go over the general micro server. Then the last part of the talk will go over the concepts of Augmented Reality and using a mobile browser to view data from the sensors.</p>\n\n<h2>Resources</h2>\n\n<p>The main github repo for the <a href=\"https://github.com/SamsungResearchUK-IoT-Meetup/multimode_sensor_platform\">project is here</a>. It's open source and still changing. Be sure to keep track on it! :-)</p>",
    "persons": [
      "Nicholas Herriot"
    ]
  },
  {
    "start": 1577894400000,
    "duration": 30,
    "room": "UD2.218A",
    "title": "TinyGo: Fast, Small, Concurrent: Choose Three",
    "subtitle": "TinyGo on microcontrollers",
    "track": "Internet of Things",
    "abstract": "<p>This talk will show using live demos why TinyGo (http://tinygo.org) the Go compiler for \"small places\" is a excellent choice for next generation IoT and embedded devices.</p>",
    "description": "<p>This talk will show using live demos why TinyGo (http://tinygo.org) the Go compiler for \"small places\" is a excellent choice for next generation IoT and embedded devices.</p>",
    "persons": [
      "Ron Evans"
    ]
  },
  {
    "start": 1577896200000,
    "duration": 30,
    "room": "UD2.218A",
    "title": "Tarantool Cartridge: Framework for Distributed Apps",
    "subtitle": "Tarantool Cartridge: Framework for Distributed Apps",
    "track": "Internet of Things",
    "abstract": "<p>We will have to talk about Tarantool Cartridge: it's framework for creating distributed applications. The application will be based on Tarantool – blazing fast in-memory database and Lua application server in one.\nInstances interconnect with each other by SWIM protocol – UDP Gossip protocol. We believe that our platform can bring brand new opportunities for the IoT world: we have the aggregated solution for many potential cases in IoT.</p>",
    "description": "<p>We will have to talk about Tarantool Cartridge: it's framework for creating distributed applications. The application will be based on Tarantool – blazing fast in-memory database and Lua application server in one.\nInstances interconnect with each other by SWIM protocol – UDP Gossip protocol. We believe that our platform can bring brand new opportunities for the IoT world: we have the aggregated solution for many potential cases in IoT.</p>",
    "persons": [
      "Artur Barsegyan"
    ]
  },
  {
    "start": 1577887200000,
    "duration": 120,
    "room": "UD2.Corridor",
    "title": "PGP Keysigning",
    "subtitle": "",
    "track": "Keysigning",
    "abstract": "<p>The FOSDEM 2020 PGP Keysigning</p>",
    "description": "<p>The keysigning event takes place in the corridor on the second level of the U building. There is no fixed end time. Previous editions last for approximately one hour per 100 keys on the list. You must register before the conference to take part. Please bring the printed list, a pen and appropriate form of identification with you the event.</p>\n\n<p>Please note that you must register your key at least a week in advance of the conference.</p>",
    "persons": [
      "FOSDEM Staff"
    ]
  }
]